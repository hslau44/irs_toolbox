{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for heatmaps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive_learning_for_unseen_environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(1024)\n",
    "torch.manual_seed(1024)\n",
    "\n",
    "# gpu setting\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = DEVICE\n",
    "torch.cuda.set_device(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.spectrogram import import_data, import_pair_data\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.baseline import *\n",
    "from models.cnn import *\n",
    "from models.self_supervised import *\n",
    "from models.utils import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import NT_Xent\n",
    "from contrastive_learning import pretrain\n",
    "from train import record_log,evaluation,train,load_checkpoint,save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder(network, model_fp):\n",
    "    encoder, outsize = create_encoder(network,'nuc2')\n",
    "    encoder.load_state_dict(torch.load(model_fp))\n",
    "    return encoder, outsize\n",
    "\n",
    "def load_model(network, freeze, lb, model_fp):\n",
    "    encoder, outsize = create_encoder(network,'nuc2')\n",
    "    model = add_classifier(encoder,in_size=outsize,out_size=len(lb.classes_),freeze=freeze)\n",
    "    model.load_state_dict(torch.load(model_fp))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINMODE = 'normal' #  'simclr' #        \n",
    "NETWORK =  'shallow2v2' #  'alexnet' # 'vgg16' # 'resnet' # 'shallow' #      \n",
    "PAIRING =  'nuc2' # 'pwr' #  \n",
    "TEMPERATURE = 'L' \n",
    "\n",
    "REGULARIZE = None\n",
    "JOINT = 'first'\n",
    "\n",
    "PRETRAIN_SKIP = ['waving'] # 'standff'\n",
    "FINETUNE_SKIP = ['waving'] # 'waving','waving'\n",
    "\n",
    "EXTRA = 'p6bf6b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp2 = './data/experiment_data/exp_2/spectrogram' \n",
    "fp3 = './data/experiment_data/exp_3/spectrogram' \n",
    "fp4 = './data/experiment_data/exp_4/spectrogram_multi'\n",
    "OUT_PATH = './laboratory/experiment_b'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "PRETRAIN_EPOCHS = 750\n",
    "LAB_FINETUNE_EPOCHS = 200\n",
    "FIELD_FINETUNE_EPOCHS = 200\n",
    "Y_SAMPLING = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainmode-normal-_Network-shallow2v2_Data-exp4csinuc2-Extra-p6bf6b\n"
     ]
    }
   ],
   "source": [
    "trainmode = TRAINMODE\n",
    "pairing = PAIRING\n",
    "network = NETWORK\n",
    "temperature = TEMPERATURE\n",
    "joint = JOINT\n",
    "pretrain_skip = PRETRAIN_SKIP\n",
    "activities = FINETUNE_SKIP\n",
    "batch_size = BATCH_SIZE\n",
    "num_workers = NUM_WORKERS\n",
    "pretrain_epochs = PRETRAIN_EPOCHS \n",
    "lab_finetune_epochs = LAB_FINETUNE_EPOCHS \n",
    "field_finetune_epochs = FIELD_FINETUNE_EPOCHS\n",
    "regularize = REGULARIZE \n",
    "t = TEMPERATURE\n",
    "y_sampling = Y_SAMPLING\n",
    "\n",
    "temperature_dict = {'L':0.1,'M':0.5,'H':1}    \n",
    "temperature = temperature_dict[t]\n",
    "\n",
    "if TRAINMODE == 'pwr':\n",
    "    joint = 'first'\n",
    "\n",
    "if TRAINMODE == 'normal':\n",
    "    if regularize == True:\n",
    "        t = 'r'\n",
    "    else:\n",
    "        t = ''\n",
    "    pairing = 'nuc2'\n",
    "    freeze=False\n",
    "else:\n",
    "    freeze=True\n",
    "\n",
    "if JOINT == 'joint':\n",
    "    j='-j'\n",
    "else:\n",
    "    j=''\n",
    "    \n",
    "if EXTRA == None:\n",
    "    extra = ''\n",
    "else:\n",
    "    extra = '-Extra-'+str(EXTRA)\n",
    "\n",
    "\n",
    "exp_name = f'Trainmode-{trainmode}-{t}_Network-{network}_Data-exp4csi{pairing}{j}{extra}'\n",
    "model_outpath  = OUT_PATH+'/'+'saved_models'\n",
    "record_outpath = OUT_PATH+'/'+'records'\n",
    "\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Data >>>>>>> Complete\n"
     ]
    }
   ],
   "source": [
    "####################################################### Main #############################################################\n",
    "\n",
    "datas = import_pair_data(fp4,modal=['csi',pairing])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1,X2,y = initial_filtering_activities(datas,activities=pretrain_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, X2_train, X2_test, y_train_, y_test_ = split_datasets(X1,X2,y,split=0.8,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_train:  (477, 1, 65, 501) \tX2_train:  (477, 1, 65, 501)\n"
     ]
    }
   ],
   "source": [
    "################################ Lab-Pretrain-phase ################################\n",
    "\n",
    "### data\n",
    "pretrain_loader = create_dataloader(X1_train,X2_train,batch_size=batch_size,num_workers=num_workers)\n",
    "print('X1_train: ',X1_train.shape,'\\tX2_train: ',X2_train.shape)\n",
    "\n",
    "### network\n",
    "encoder, outsize = create_encoder(network,'nuc2')\n",
    "encoder2, outsize2 = None,None\n",
    "\n",
    "if joint == 'joint':\n",
    "    simclr = add_SimCLR(encoder, outsize)\n",
    "else:\n",
    "    encoder2, outsize2 = create_encoder(network,pairing)\n",
    "    simclr = add_SimCLR_multi(enc1=encoder,enc2=encoder2,out_size1=outsize,out_size2=outsize2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "### pretraining\n",
    "phase = 'pretrain'\n",
    "if trainmode == 'simclr':\n",
    "    criterion = NT_Xent(batch_size, temperature, world_size=1)\n",
    "    optimizer = torch.optim.SGD(list(simclr.parameters()), lr=0.0005)\n",
    "    simclr, record = pretrain(model=simclr,\n",
    "                              train_loader=pretrain_loader,\n",
    "                              criterion=criterion,\n",
    "                              optimizer=optimizer,\n",
    "                              end=pretrain_epochs,\n",
    "                              device=device)\n",
    "    record_log(record_outpath,exp_name,phase,record=record)\n",
    "    \n",
    "# save\n",
    "encoder_fp = save_model(model_outpath,exp_name,phase,simclr.encoder)\n",
    "del simclr,encoder,outsize, encoder2,outsize2\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sampling:  1 \n",
      "\n",
      "X_train:  (6, 1, 65, 501) \ty_train:  (6,) \tX_test:  (120, 1, 65, 501) \ty_test:  (120,)\n",
      "class:  ['lay' 'pickup' 'sit' 'stand' 'standff' 'walk']\n",
      "class_size:  tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n",
      "             0    1    2     3         4    5  accuracy  macro avg  \\\n",
      "f1-score   0.0  0.0  0.0  0.40  0.666667  0.0  0.333333   0.177778   \n",
      "precision  0.0  0.0  0.0  0.25  0.500000  0.0  0.333333   0.125000   \n",
      "recall     0.0  0.0  0.0  1.00  1.000000  0.0  0.333333   0.333333   \n",
      "support    1.0  1.0  1.0  1.00  1.000000  1.0  0.333333   6.000000   \n",
      "\n",
      "           weighted avg  \n",
      "f1-score       0.177778  \n",
      "precision      0.125000  \n",
      "recall         0.333333  \n",
      "support        6.000000  \n",
      "Start Training\n",
      "Epoch 1: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> loss: 1.7916284799575806 \n",
      "Epoch 2: > loss: 1.7782055139541626 \n",
      "Epoch 3: > loss: 1.9418363571166992 \n",
      "Epoch 4: > loss: 1.8186302185058594 \n",
      "Epoch 5: > loss: 1.7999969720840454 \n",
      "Epoch 6: > loss: 1.8294713497161865 \n",
      "Epoch 7: > loss: 1.78205144405365 \n",
      "Epoch 8: > loss: 1.7781826257705688 \n",
      "Epoch 9: > loss: 1.7797338962554932 \n",
      "Epoch 10: > loss: 1.767246961593628  accuracy: 0.25\n",
      "Epoch 11: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> loss: 1.7204627990722656 \n",
      "Epoch 12: > loss: 1.6936477422714233 \n",
      "Epoch 13: > loss: 1.5615367889404297 \n",
      "Epoch 14: > loss: 1.3416565656661987 \n",
      "Epoch 15: > loss: 1.2088096141815186 \n",
      "Epoch 16: > loss: 1.474308729171753 \n",
      "Epoch 17: > loss: 0.8448276519775391 \n",
      "Epoch 18: > loss: 0.8890425562858582 \n",
      "Epoch 19: > loss: 0.8391897082328796 \n",
      "Epoch 20: > loss: 0.36191755533218384  accuracy: 0.4666666666666667\n",
      "Epoch 21: > loss: 0.4361000657081604 \n",
      "Epoch 22: > loss: 0.40980440378189087 \n",
      "Epoch 23: > loss: 0.25358840823173523 \n",
      "Epoch 24: > loss: 0.2802278697490692 \n",
      "Epoch 25: > loss: 0.10860762745141983 \n",
      "Epoch 26: > loss: 0.04200801998376846 \n",
      "Epoch 27: > loss: 0.1735728532075882 \n",
      "Epoch 28: > loss: 0.008719624020159245 \n",
      "Epoch 29: > loss: 0.010665475390851498 \n",
      "Epoch 30: > loss: 0.008904831483960152  accuracy: 0.45\n",
      "Epoch 31: > loss: 0.011554531753063202 \n",
      "Epoch 32: > loss: 0.016103357076644897 \n",
      "Epoch 33: > loss: 0.004962273873388767 \n",
      "Epoch 34: > loss: 0.003800180973485112 \n",
      "Epoch 35: > loss: 0.004470103420317173 \n",
      "Epoch 36: > loss: 0.005072232335805893 \n",
      "Epoch 37: > loss: 9.628139378037304e-05 \n",
      "Epoch 38: > loss: 0.0009841357823461294 \n",
      "Epoch 39: > loss: 0.007119068410247564 \n",
      "Epoch 40: > loss: 0.001624896889552474  accuracy: 0.48333333333333334\n",
      "Epoch 41: > loss: 6.539666355820373e-05 \n",
      "Epoch 42: > loss: 7.549784641014412e-06 \n",
      "Epoch 43: > loss: 0.03145391494035721 \n",
      "Epoch 44: > loss: 7.94728478581419e-08 \n",
      "Epoch 45: > loss: 7.450414159393404e-06 \n",
      "Epoch 46: > loss: 0.0016598992515355349 \n",
      "Epoch 47: > loss: 0.35742276906967163 \n",
      "Epoch 48: > loss: 0.37728530168533325 \n",
      "Epoch 49: > loss: 0.15125784277915955 \n",
      "Epoch 50: > loss: 0.00940287858247757  accuracy: 0.36666666666666664\n",
      "Epoch 51: > loss: 1.49010941186134e-06 \n",
      "Epoch 52: > loss: 0.000277030689176172 \n",
      "Epoch 53: > loss: 5.7219476730097085e-06 \n",
      "Epoch 54: > loss: 0.0005652677500620484 \n",
      "Epoch 55: > loss: 0.004832613281905651 \n",
      "Epoch 56: > loss: 1.374068260192871 \n",
      "Epoch 57: > loss: 0.0002161850279662758 \n",
      "Epoch 58: > loss: 3.4866818168666214e-05 \n",
      "Epoch 59: > loss: 0.26894158124923706 \n",
      "Epoch 60: > loss: 0.0003734103520400822  accuracy: 0.4166666666666667\n",
      "Epoch 61: > loss: 4.7683647608209867e-07 \n",
      "Epoch 62: > loss: 0.11344581842422485 \n",
      "Epoch 63: > loss: 3.6178022128297016e-05 \n",
      "Epoch 64: > loss: 0.0007397752488031983 \n",
      "Epoch 65: > loss: 0.5822461843490601 \n",
      "Epoch 66: > loss: 5.761772285950428e-07 \n",
      "Epoch 67: > loss: 0.1021968275308609 \n",
      "Epoch 68: > loss: 1.986820308275128e-07 \n",
      "Epoch 69: > loss: 1.9868095932906726e-06 \n",
      "Epoch 70: > loss: 0.014627610333263874  accuracy: 0.35833333333333334\n",
      "Epoch 71: > loss: 0.0002193596155848354 \n",
      "Epoch 72: > loss: 0.0014214325929060578 \n",
      "Epoch 73: > loss: 0.0002516939421184361 \n",
      "Epoch 74: > loss: 0.0002810435544233769 \n",
      "Epoch 75: > loss: 0.15721604228019714 \n",
      "Epoch 76: > loss: 0.300750195980072 \n",
      "Epoch 77: > loss: 0.0003141464840155095 \n",
      "Epoch 78: > loss: 0.0027096348349004984 \n",
      "Epoch 79: > loss: 0.7363366484642029 \n",
      "Epoch 80: > loss: 7.1921558628673665e-06  accuracy: 0.4\n",
      "Epoch 81: > loss: 1.4671231508255005 \n",
      "Epoch 82: > loss: 8.344640036739293e-07 \n",
      "Epoch 83: > loss: 1.4894587993621826 \n",
      "Epoch 84: > loss: 9.902122110361233e-05 \n",
      "Epoch 85: > loss: 0.02228691801428795 \n",
      "Epoch 86: > loss: 6.833718362031505e-05 \n",
      "Epoch 87: > loss: 0.0018168212845921516 \n",
      "Epoch 88: > loss: 1.660907219047658e-05 \n",
      "Epoch 89: > loss: 1.0872137546539307 \n",
      "Epoch 90: > loss: 0.005401368252933025  accuracy: 0.4\n",
      "Epoch 91: > loss: 5.642485575663159e-06 \n",
      "Epoch 92: > loss: 1.0927499261015328e-06 \n",
      "Epoch 93: > loss: 3.3775930319279723e-07 \n",
      "Epoch 94: > loss: 2.384183943604512e-07 \n",
      "Epoch 95: > loss: 1.788138632718983e-07 \n",
      "Epoch 96: > loss: 9.079527444555424e-06 \n",
      "Epoch 97: > loss: 1.5238225387292914e-05 \n",
      "Epoch 98: > loss: 2.227092772955075e-05 \n",
      "Epoch 99: > loss: 0.0001463256194256246 \n",
      "Epoch 100: > loss: 0.0022064042277634144  accuracy: 0.44166666666666665\n",
      "Epoch 101: > loss: 0.23843352496623993 \n",
      "Epoch 102: > loss: 0.006414984352886677 \n",
      "Epoch 103: > loss: 0.010384934954345226 \n",
      "Epoch 104: > loss: 0.8838435411453247 \n",
      "Epoch 105: > loss: 0.003726507071405649 \n",
      "Epoch 106: > loss: 7.271618414961267e-06 \n",
      "Epoch 107: > loss: 5.006734681956004e-06 \n",
      "Epoch 108: > loss: 1.3073023183096666e-05 \n",
      "Epoch 109: > loss: 5.900773885514354e-06 \n",
      "Epoch 110: > loss: 0.008602983318269253  accuracy: 0.4166666666666667\n",
      "Epoch 111: > loss: 0.003080998081713915 \n",
      "Epoch 112: > loss: 0.022802570834755898 \n",
      "Epoch 113: > loss: 0.3190429210662842 \n",
      "Epoch 114: > loss: 0.0003621979267336428 \n",
      "Epoch 115: > loss: 0.14599399268627167 \n",
      "Epoch 116: > loss: 0.0007403044728562236 \n",
      "Epoch 117: > loss: 0.0011304941726848483 \n",
      "Epoch 118: > loss: 0.09376184642314911 \n",
      "Epoch 119: > loss: 0.08517332375049591 \n",
      "Epoch 120: > loss: 0.013338090851902962  accuracy: 0.36666666666666664\n",
      "Epoch 121: > loss: 0.0005813211901113391 \n",
      "Epoch 122: > loss: 0.0008928587776608765 \n",
      "Epoch 123: > loss: 8.869705197867006e-05 \n",
      "Epoch 124: > loss: 0.00012863877054769546 \n",
      "Epoch 125: > loss: 1.718515341053717e-05 \n",
      "Epoch 126: > loss: 2.944210973510053e-05 \n",
      "Epoch 127: > loss: 5.523280833585886e-06 \n",
      "Epoch 128: > loss: 2.384184938364342e-07 \n",
      "Epoch 129: > loss: 3.973642392907095e-08 \n",
      "Epoch 130: > loss: 6.457092240452766e-06  accuracy: 0.36666666666666664\n",
      "Epoch 131: > loss: 6.973636573093245e-06 \n",
      "Epoch 132: > loss: 4.80804101243848e-06 \n",
      "Epoch 133: > loss: 6.357821575875278e-07 \n",
      "Epoch 134: > loss: 8.682274710736237e-06 \n",
      "Epoch 135: > loss: 0.0014303781790658832 \n",
      "Epoch 136: > loss: 1.529846144876501e-06 \n",
      "Epoch 137: > loss: 3.397436785235186e-06 \n",
      "Epoch 138: > loss: 1.1523532066348707e-06 \n",
      "Epoch 139: > loss: 0.0006460790173150599 \n",
      "Epoch 140: > loss: 1.3907747131725046e-07  accuracy: 0.35833333333333334\n",
      "Epoch 141: > loss: 1.9868213740892315e-08 \n",
      "Epoch 142: > loss: 4.7683653292551753e-07 \n",
      "Epoch 143: > loss: 1.9868213740892315e-08 \n",
      "Epoch 144: > loss: 2.1855028364825557e-07 \n",
      "Epoch 145: > loss: 9.934105804632054e-08 \n",
      "Epoch 146: > loss: 8.344452908204403e-06 \n",
      "Epoch 147: > loss: 1.7483944247942418e-06 \n",
      "Epoch 148: > loss: 1.192092469182171e-07 \n",
      "Epoch 149: > loss: 1.986820450383675e-07 \n",
      "Epoch 150: > loss: 1.1722211183951003e-06  accuracy: 0.31666666666666665\n",
      "Epoch 151: > loss: 3.973642748178463e-08 \n",
      "Epoch 152: > loss: 9.934105804632054e-08 \n",
      "Epoch 153: > loss: 1.0271570317854639e-05 \n",
      "Epoch 154: > loss: 2.0265456441848073e-06 \n",
      "Epoch 155: > loss: 1.3907744289554103e-07 \n",
      "Epoch 156: > loss: 4.490181709115859e-06 \n",
      "Epoch 157: > loss: 7.947284075271455e-08 \n",
      "Epoch 158: > loss: 9.934105804632054e-08 \n",
      "Epoch 159: > loss: 5.165678430785192e-06 \n",
      "Epoch 160: > loss: 3.7467390939127654e-05  accuracy: 0.31666666666666665\n",
      "Epoch 161: > loss: 1.6689227777533233e-06 \n",
      "Epoch 162: > loss: 5.960463411724959e-08 \n",
      "Epoch 163: > loss: 1.589456815054291e-07 \n",
      "Epoch 164: > loss: 1.5894563887286495e-07 \n",
      "Epoch 165: > loss: 1.072880422725575e-06 \n",
      "Epoch 166: > loss: 5.960463411724959e-08 \n",
      "Epoch 167: > loss: 4.030773197882809e-05 \n",
      "Epoch 168: > loss: 4.5696828010477475e-07 \n",
      "Epoch 169: > loss: 2.1855025522654614e-07 \n",
      "Epoch 170: > loss: 2.3284039343707263e-05  accuracy: 0.35833333333333334\n",
      "Epoch 171: > loss: 1.5794483260833658e-05 \n",
      "Epoch 172: > loss: 1.9868213740892315e-08 \n",
      "Epoch 173: > loss: 1.7105658116634004e-05 \n",
      "Epoch 174: > loss: 2.7614540158538148e-05 \n",
      "Epoch 175: > loss: 2.5828660454862984e-07 \n",
      "Epoch 176: > loss: 7.94728478581419e-08 \n",
      "Epoch 177: > loss: 0.0 \n",
      "Epoch 178: > loss: 0.0 \n",
      "Epoch 179: > loss: 0.0 \n",
      "Epoch 180: > loss: 4.7683647608209867e-07  accuracy: 0.3416666666666667\n",
      "Epoch 181: > loss: 0.0 \n",
      "Epoch 182: > loss: 1.9868213740892315e-08 \n",
      "Epoch 183: > loss: 1.9868213740892315e-08 \n",
      "Epoch 184: > loss: 0.0 \n",
      "Epoch 185: > loss: 1.7881383485018887e-07 \n",
      "Epoch 186: > loss: 0.0 \n",
      "Epoch 187: > loss: 1.7285277635892271e-06 \n",
      "Epoch 188: > loss: 1.9868213740892315e-08 \n",
      "Epoch 189: > loss: 0.0 \n",
      "Epoch 190: > loss: 1.1523524108270067e-06  accuracy: 0.35\n",
      "Epoch 191: > loss: 1.9868213740892315e-08 \n",
      "Epoch 192: > loss: 1.1920926823449918e-07 \n",
      "Epoch 193: > loss: 3.576274707484117e-07 \n",
      "Epoch 194: > loss: 1.3907745710639574e-07 \n",
      "Epoch 195: > loss: 1.9868213740892315e-08 \n",
      "Epoch 196: > loss: 0.0 \n",
      "Epoch 197: > loss: 5.563090894611378e-07 \n",
      "Epoch 198: > loss: 2.284830316057196e-06 \n",
      "Epoch 199: > loss: 1.192092469182171e-07 \n",
      "Epoch 200: > loss: 1.1920926823449918e-07  accuracy: 0.35\n",
      "                  0          1     2          3         4          5  \\\n",
      "f1-score   0.193548   0.357143   0.0   0.250000  0.190476   0.631579   \n",
      "precision  0.136364   0.294118   0.0   0.235294  0.153846   0.923077   \n",
      "recall     0.333333   0.454545   0.0   0.266667  0.250000   0.480000   \n",
      "support    9.000000  22.000000  16.0  15.000000  8.000000  50.000000   \n",
      "\n",
      "           accuracy   macro avg  weighted avg  \n",
      "f1-score   0.358333    0.270458      0.387099  \n",
      "precision  0.358333    0.290450      0.488432  \n",
      "recall     0.358333    0.297424      0.358333  \n",
      "support    0.358333  120.000000    120.000000  \n",
      "\n",
      "\n",
      "sampling:  5 \n",
      "\n",
      "X_train:  (30, 1, 65, 501) \ty_train:  (30,) \tX_test:  (120, 1, 65, 501) \ty_test:  (120,)\n",
      "class:  ['lay' 'pickup' 'sit' 'stand' 'standff' 'walk']\n",
      "class_size:  tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch 1: > loss: 1.7931773662567139 \n",
      "Epoch 2: > loss: 1.78843355178833 \n",
      "Epoch 3: > loss: 1.8466020822525024 \n",
      "Epoch 4: > loss: 1.7666314840316772 \n",
      "Epoch 5: > loss: 1.6893424987792969 \n",
      "Epoch 6: > loss: 1.6157251596450806 \n",
      "Epoch 7: > loss: 1.5035185813903809 \n",
      "Epoch 8: > loss: 1.5522972345352173 \n",
      "Epoch 9: > loss: 1.7579237222671509 \n",
      "Epoch 10: > loss: 1.3998541831970215  accuracy: 0.275\n",
      "Epoch 11: > loss: 1.3191077709197998 \n",
      "Epoch 12: > loss: 1.391413688659668 \n",
      "Epoch 13: > loss: 1.2156448364257812 \n",
      "Epoch 14: > loss: 1.1763155460357666 \n",
      "Epoch 15: > loss: 1.1570161581039429 \n",
      "Epoch 16: > loss: 0.9813858866691589 \n",
      "Epoch 17: > loss: 0.9592292904853821 \n",
      "Epoch 18: > loss: 0.9403376579284668 \n",
      "Epoch 19: > loss: 0.8586141467094421 \n",
      "Epoch 20: > loss: 0.8373139500617981  accuracy: 0.5333333333333333\n",
      "Epoch 21: > loss: 0.7186429500579834 \n",
      "Epoch 22: > loss: 0.7130441665649414 \n",
      "Epoch 23: > loss: 0.6686277985572815 \n",
      "Epoch 24: > loss: 0.7452587485313416 \n",
      "Epoch 25: > loss: 0.6344105005264282 \n",
      "Epoch 26: > loss: 0.4950486123561859 \n",
      "Epoch 27: > loss: 0.5822346210479736 \n",
      "Epoch 28: > loss: 0.5474879741668701 \n",
      "Epoch 29: > loss: 0.41166776418685913 \n",
      "Epoch 30: > loss: 0.49608758091926575  accuracy: 0.45\n",
      "Epoch 31: > loss: 0.39046311378479004 \n",
      "Epoch 32: > loss: 0.5011112093925476 \n",
      "Epoch 33: > loss: 0.27350491285324097 \n",
      "Epoch 34: > loss: 0.7870861887931824 \n",
      "Epoch 35: > loss: 0.44187313318252563 \n",
      "Epoch 36: > loss: 0.31328141689300537 \n",
      "Epoch 37: > loss: 0.4881581664085388 \n",
      "Epoch 38: > loss: 0.24285562336444855 \n",
      "Epoch 39: > loss: 0.2502640187740326 \n",
      "Epoch 40: > loss: 0.45447075366973877  accuracy: 0.5666666666666667\n",
      "Epoch 41: > loss: 0.3434356153011322 \n",
      "Epoch 42: > loss: 0.34956419467926025 \n",
      "Epoch 43: > loss: 0.19770604372024536 \n",
      "Epoch 44: > loss: 0.26684364676475525 \n",
      "Epoch 45: > loss: 0.2867518961429596 \n",
      "Epoch 46: > loss: 0.21692968904972076 \n",
      "Epoch 47: > loss: 0.29789066314697266 \n",
      "Epoch 48: > loss: 0.1925702691078186 \n",
      "Epoch 49: > loss: 0.20605316758155823 \n",
      "Epoch 50: > loss: 0.13537393510341644  accuracy: 0.49166666666666664\n",
      "Epoch 51: > loss: 0.12345419079065323 \n",
      "Epoch 52: > loss: 0.11283250898122787 \n",
      "Epoch 53: > loss: 0.15103374421596527 \n",
      "Epoch 54: > loss: 0.09259076416492462 \n",
      "Epoch 55: > loss: 0.06203330680727959 \n",
      "Epoch 56: > loss: 0.056623030453920364 \n",
      "Epoch 57: > loss: 0.25060826539993286 \n",
      "Epoch 58: > loss: 0.06611637771129608 \n",
      "Epoch 59: > loss: 0.05565618723630905 \n",
      "Epoch 60: > loss: 0.0564076229929924  accuracy: 0.425\n",
      "Epoch 61: > loss: 0.10552946478128433 \n",
      "Epoch 62: > loss: 0.05951067805290222 \n",
      "Epoch 63: > loss: 0.05067429691553116 \n",
      "Epoch 64: > loss: 0.037241410464048386 \n",
      "Epoch 65: > loss: 0.02443048171699047 \n",
      "Epoch 66: > loss: 0.053777895867824554 \n",
      "Epoch 67: > loss: 0.007896343246102333 \n",
      "Epoch 68: > loss: 0.00856723077595234 \n",
      "Epoch 69: > loss: 0.018213974311947823 \n",
      "Epoch 70: > loss: 0.005081682465970516  accuracy: 0.49166666666666664\n",
      "Epoch 71: > loss: 0.006391055416315794 \n",
      "Epoch 72: > loss: 0.00259499647654593 \n",
      "Epoch 73: > loss: 0.004088531248271465 \n",
      "Epoch 74: > loss: 0.006023464258760214 \n",
      "Epoch 75: > loss: 0.05995282530784607 \n",
      "Epoch 76: > loss: 0.001844225451350212 \n",
      "Epoch 77: > loss: 0.029886553063988686 \n",
      "Epoch 78: > loss: 0.11984843015670776 \n",
      "Epoch 79: > loss: 0.06268296390771866 \n",
      "Epoch 80: > loss: 0.1526990383863449  accuracy: 0.35\n",
      "Epoch 81: > loss: 0.036889221519231796 \n",
      "Epoch 82: > loss: 0.89333575963974 \n",
      "Epoch 83: > loss: 0.5091308355331421 \n",
      "Epoch 84: > loss: 1.633026361465454 \n",
      "Epoch 85: > loss: 0.2694692611694336 \n",
      "Epoch 86: > loss: 0.4059668481349945 \n",
      "Epoch 87: > loss: 2.103623390197754 \n",
      "Epoch 88: > loss: 0.12441082298755646 \n",
      "Epoch 89: > loss: 0.5022695660591125 \n",
      "Epoch 90: > loss: 1.264115333557129  accuracy: 0.49166666666666664\n",
      "Epoch 91: > loss: 0.8345986604690552 \n",
      "Epoch 92: > loss: 0.5753640532493591 \n",
      "Epoch 93: > loss: 0.4187205135822296 \n",
      "Epoch 94: > loss: 0.4814295172691345 \n",
      "Epoch 95: > loss: 0.43337541818618774 \n",
      "Epoch 96: > loss: 0.3194914162158966 \n",
      "Epoch 97: > loss: 0.24165041744709015 \n",
      "Epoch 98: > loss: 0.28144562244415283 \n",
      "Epoch 99: > loss: 0.3202432096004486 \n",
      "Epoch 100: > loss: 0.31365135312080383  accuracy: 0.5\n",
      "Epoch 101: > loss: 0.19890974462032318 \n",
      "Epoch 102: > loss: 0.14036132395267487 \n",
      "Epoch 103: > loss: 0.07750366628170013 \n",
      "Epoch 104: > loss: 0.22209694981575012 \n",
      "Epoch 105: > loss: 0.052012160420417786 \n",
      "Epoch 106: > loss: 0.09285222738981247 \n",
      "Epoch 107: > loss: 0.04979996383190155 \n",
      "Epoch 108: > loss: 0.06543009728193283 \n",
      "Epoch 109: > loss: 0.09558509290218353 \n",
      "Epoch 110: > loss: 0.046303294599056244  accuracy: 0.43333333333333335\n",
      "Epoch 111: > loss: 0.04944490268826485 \n",
      "Epoch 112: > loss: 0.04640943184494972 \n",
      "Epoch 113: > loss: 0.1270926147699356 \n",
      "Epoch 114: > loss: 0.03655323013663292 \n",
      "Epoch 115: > loss: 0.03838269039988518 \n",
      "Epoch 116: > loss: 0.0032021119259297848 \n",
      "Epoch 117: > loss: 0.006607220973819494 \n",
      "Epoch 118: > loss: 0.0073451572097837925 \n",
      "Epoch 119: > loss: 0.006626452319324017 \n",
      "Epoch 120: > loss: 0.03782530874013901  accuracy: 0.4166666666666667\n",
      "Epoch 121: > loss: 0.04702148213982582 \n",
      "Epoch 122: > loss: 0.012624083086848259 \n",
      "Epoch 123: > loss: 0.0034103370271623135 \n",
      "Epoch 124: > loss: 0.005655554123222828 \n",
      "Epoch 125: > loss: 0.014492894522845745 \n",
      "Epoch 126: > loss: 0.004306827671825886 \n",
      "Epoch 127: > loss: 0.004671335220336914 \n",
      "Epoch 128: > loss: 0.0017528109019622207 \n",
      "Epoch 129: > loss: 0.0005916666123084724 \n",
      "Epoch 130: > loss: 0.0005533290677703917  accuracy: 0.5\n",
      "Epoch 131: > loss: 0.0006782496930100024 \n",
      "Epoch 132: > loss: 0.0012473980896174908 \n",
      "Epoch 133: > loss: 0.0029185728635638952 \n",
      "Epoch 134: > loss: 0.0003325000579934567 \n",
      "Epoch 135: > loss: 0.0005537837860174477 \n",
      "Epoch 136: > loss: 0.00024639573530294 \n",
      "Epoch 137: > loss: 0.0012171214912086725 \n",
      "Epoch 138: > loss: 0.0005107331671752036 \n",
      "Epoch 139: > loss: 0.0185695830732584 \n",
      "Epoch 140: > loss: 0.00039376382483169436  accuracy: 0.45\n",
      "Epoch 141: > loss: 0.00045278933248482645 \n",
      "Epoch 142: > loss: 0.007585597224533558 \n",
      "Epoch 143: > loss: 0.0002776919864118099 \n",
      "Epoch 144: > loss: 0.0009447467164136469 \n",
      "Epoch 145: > loss: 0.00043721511610783637 \n",
      "Epoch 146: > loss: 0.0011614655377343297 \n",
      "Epoch 147: > loss: 0.006854744162410498 \n",
      "Epoch 148: > loss: 0.020062848925590515 \n",
      "Epoch 149: > loss: 0.01136128231883049 \n",
      "Epoch 150: > loss: 0.009602682664990425  accuracy: 0.5\n",
      "Epoch 151: > loss: 0.0016109788557514548 \n",
      "Epoch 152: > loss: 0.0008620527805760503 \n",
      "Epoch 153: > loss: 0.001303893863223493 \n",
      "Epoch 154: > loss: 0.0012986564543098211 \n",
      "Epoch 155: > loss: 0.000961722747888416 \n",
      "Epoch 156: > loss: 0.0020074716303497553 \n",
      "Epoch 157: > loss: 0.0027575648855417967 \n",
      "Epoch 158: > loss: 0.0049307579174637794 \n",
      "Epoch 159: > loss: 0.0017954803770408034 \n",
      "Epoch 160: > loss: 0.00043537732562981546  accuracy: 0.5083333333333333\n",
      "Epoch 161: > loss: 0.0010069156996905804 \n",
      "Epoch 162: > loss: 0.002154330722987652 \n",
      "Epoch 163: > loss: 0.0013348991051316261 \n",
      "Epoch 164: > loss: 0.16405196487903595 \n",
      "Epoch 165: > loss: 0.023172223940491676 \n",
      "Epoch 166: > loss: 0.020784983411431313 \n",
      "Epoch 167: > loss: 0.02643566206097603 \n",
      "Epoch 168: > loss: 0.011618663556873798 \n",
      "Epoch 169: > loss: 0.005053894128650427 \n",
      "Epoch 170: > loss: 0.11225955188274384  accuracy: 0.48333333333333334\n",
      "Epoch 171: > loss: 0.007225391920655966 \n",
      "Epoch 172: > loss: 0.019421832635998726 \n",
      "Epoch 173: > loss: 0.04521443694829941 \n",
      "Epoch 174: > loss: 0.011995688080787659 \n",
      "Epoch 175: > loss: 0.0021943028550595045 \n",
      "Epoch 176: > loss: 0.02161506377160549 \n",
      "Epoch 177: > loss: 0.002466350793838501 \n",
      "Epoch 178: > loss: 0.08211874961853027 \n",
      "Epoch 179: > loss: 0.0135114137083292 \n",
      "Epoch 180: > loss: 0.00939884688705206  accuracy: 0.425\n",
      "Epoch 181: > loss: 0.009479797445237637 \n",
      "Epoch 182: > loss: 0.004845008719712496 \n",
      "Epoch 183: > loss: 0.0024339589290320873 \n",
      "Epoch 184: > loss: 0.0010188330197706819 \n",
      "Epoch 185: > loss: 0.0022991925943642855 \n",
      "Epoch 186: > loss: 0.0015428137267008424 \n",
      "Epoch 187: > loss: 0.00033482129219919443 \n",
      "Epoch 188: > loss: 0.0008149345521815121 \n",
      "Epoch 189: > loss: 0.00055701844394207 \n",
      "Epoch 190: > loss: 0.0029315308202058077  accuracy: 0.4583333333333333\n",
      "Epoch 191: > loss: 0.0007351457024924457 \n",
      "Epoch 192: > loss: 8.851540042087436e-05 \n",
      "Epoch 193: > loss: 0.002363931154832244 \n",
      "Epoch 194: > loss: 0.00017254870908800513 \n",
      "Epoch 195: > loss: 0.0004546834679786116 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 196: > loss: 0.00016624564887024462 \n",
      "Epoch 197: > loss: 0.0021340607199817896 \n",
      "Epoch 198: > loss: 0.0007504162495024502 \n",
      "Epoch 199: > loss: 4.0151018765755e-05 \n",
      "Epoch 200: > loss: 0.0015516772400587797  accuracy: 0.4583333333333333\n",
      "                  0          1          2          3         4          5  \\\n",
      "f1-score   0.125000   0.146341   0.416667   0.279070  0.518519   0.719101   \n",
      "precision  0.142857   0.157895   0.625000   0.214286  0.368421   0.820513   \n",
      "recall     0.111111   0.136364   0.312500   0.400000  0.875000   0.640000   \n",
      "support    9.000000  22.000000  16.000000  15.000000  8.000000  50.000000   \n",
      "\n",
      "           accuracy   macro avg  weighted avg  \n",
      "f1-score       0.45    0.367450      0.460837  \n",
      "precision      0.45    0.388162      0.516222  \n",
      "recall         0.45    0.412496      0.450000  \n",
      "support        0.45  120.000000    120.000000  \n",
      "\n",
      "\n",
      "sampling:  10 \n",
      "\n",
      "X_train:  (60, 1, 65, 501) \ty_train:  (60,) \tX_test:  (120, 1, 65, 501) \ty_test:  (120,)\n",
      "class:  ['lay' 'pickup' 'sit' 'stand' 'standff' 'walk']\n",
      "class_size:  tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n",
      "Start Training\n",
      "Epoch 1: > loss: 1.7950166463851929 \n",
      "Epoch 2: > loss: 1.847744107246399 \n",
      "Epoch 3: > loss: 1.798213243484497 \n",
      "Epoch 4: > loss: 1.7877427339553833 \n",
      "Epoch 5: > loss: 1.7608147859573364 \n",
      "Epoch 6: > loss: 1.715681552886963 \n",
      "Epoch 7: > loss: 1.6482696533203125 \n",
      "Epoch 8: > loss: 1.5433839559555054 \n",
      "Epoch 9: > loss: 1.9823782444000244 \n",
      "Epoch 10: > loss: 1.625553011894226  accuracy: 0.38333333333333336\n",
      "Epoch 11: > loss: 1.6883766651153564 \n",
      "Epoch 12: > loss: 1.5129649639129639 \n",
      "Epoch 13: > loss: 1.5999963283538818 \n",
      "Epoch 14: > loss: 1.6204845905303955 \n",
      "Epoch 15: > loss: 1.5380783081054688 \n",
      "Epoch 16: > loss: 1.4656332731246948 \n",
      "Epoch 17: > loss: 1.5020484924316406 \n",
      "Epoch 18: > loss: 1.4755901098251343 \n",
      "Epoch 19: > loss: 1.4115530252456665 \n",
      "Epoch 20: > loss: 1.3887829780578613  accuracy: 0.31666666666666665\n",
      "Epoch 21: > loss: 1.3443603515625 \n",
      "Epoch 22: > loss: 1.3231747150421143 \n",
      "Epoch 23: > loss: 1.3409420251846313 \n",
      "Epoch 24: > loss: 1.219295620918274 \n",
      "Epoch 25: > loss: 1.2920809984207153 \n",
      "Epoch 26: > loss: 1.1711326837539673 \n",
      "Epoch 27: > loss: 1.184767723083496 \n",
      "Epoch 28: > loss: 1.2369811534881592 \n",
      "Epoch 29: > loss: 1.1916602849960327 \n",
      "Epoch 30: > loss: 1.081138253211975  accuracy: 0.4\n",
      "Epoch 31: > loss: 1.0968468189239502 \n",
      "Epoch 32: > loss: 1.0560929775238037 \n",
      "Epoch 33: > loss: 1.0819183588027954 \n",
      "Epoch 34: > loss: 0.9927338361740112 \n",
      "Epoch 35: > loss: 0.9109947085380554 \n",
      "Epoch 36: > loss: 0.9310866594314575 \n",
      "Epoch 37: > loss: 0.7837761044502258 \n",
      "Epoch 38: > loss: 0.9142043590545654 \n",
      "Epoch 39: > loss: 0.8030330538749695 \n",
      "Epoch 40: > loss: 0.8249013423919678  accuracy: 0.3333333333333333\n",
      "Epoch 41: > loss: 0.7688388824462891 \n",
      "Epoch 42: > loss: 0.680118203163147 \n",
      "Epoch 43: > loss: 0.7152805924415588 \n",
      "Epoch 44: > loss: 0.6974740624427795 \n",
      "Epoch 45: > loss: 0.7296553254127502 \n",
      "Epoch 46: > loss: 0.5901241898536682 \n",
      "Epoch 47: > loss: 0.5591772794723511 \n",
      "Epoch 48: > loss: 0.5337479710578918 \n",
      "Epoch 49: > loss: 0.5547522902488708 \n",
      "Epoch 50: > loss: 0.5536705255508423  accuracy: 0.35833333333333334\n",
      "Epoch 51: > loss: 0.635012686252594 \n",
      "Epoch 52: > loss: 0.6556847095489502 \n",
      "Epoch 53: > loss: 0.38206586241722107 \n",
      "Epoch 54: > loss: 0.4814651608467102 \n",
      "Epoch 55: > loss: 0.39846500754356384 \n",
      "Epoch 56: > loss: 0.3942597508430481 \n",
      "Epoch 57: > loss: 0.3217249810695648 \n",
      "Epoch 58: > loss: 0.29793310165405273 \n",
      "Epoch 59: > loss: 0.3307899534702301 \n",
      "Epoch 60: > loss: 0.4014688730239868  accuracy: 0.375\n",
      "Epoch 61: > loss: 0.4788481593132019 \n",
      "Epoch 62: > loss: 0.40701156854629517 \n",
      "Epoch 63: > loss: 0.4412415623664856 \n",
      "Epoch 64: > loss: 0.23768296837806702 \n",
      "Epoch 65: > loss: 0.26464104652404785 \n",
      "Epoch 66: > loss: 0.3254295587539673 \n",
      "Epoch 67: > loss: 0.30062204599380493 \n",
      "Epoch 68: > loss: 0.25493332743644714 \n",
      "Epoch 69: > loss: 0.19024008512496948 \n",
      "Epoch 70: > loss: 0.13257436454296112  accuracy: 0.4166666666666667\n",
      "Epoch 71: > loss: 0.2873578369617462 \n",
      "Epoch 72: > loss: 0.1621413379907608 \n",
      "Epoch 73: > loss: 0.1614365130662918 \n",
      "Epoch 74: > loss: 0.16209550201892853 \n",
      "Epoch 75: > loss: 0.11788368970155716 \n",
      "Epoch 76: > loss: 0.141363263130188 \n",
      "Epoch 77: > loss: 0.08702445775270462 \n",
      "Epoch 78: > loss: 0.060533471405506134 \n",
      "Epoch 79: > loss: 0.07066565006971359 \n",
      "Epoch 80: > loss: 0.11379285901784897  accuracy: 0.5166666666666667\n",
      "Epoch 81: > loss: 0.06987085938453674 \n",
      "Epoch 82: > loss: 0.08042913675308228 \n",
      "Epoch 83: > loss: 0.07391922175884247 \n",
      "Epoch 84: > loss: 0.08552145957946777 \n",
      "Epoch 85: > loss: 0.04204283654689789 \n",
      "Epoch 86: > loss: 0.032569993287324905 \n",
      "Epoch 87: > loss: 0.05857427418231964 \n",
      "Epoch 88: > loss: 0.05582420527935028 \n",
      "Epoch 89: > loss: 0.15703114867210388 \n",
      "Epoch 90: > loss: 0.5072639584541321  accuracy: 0.45\n",
      "Epoch 91: > loss: 2.8447177410125732 \n",
      "Epoch 92: > loss: 0.24341590702533722 \n",
      "Epoch 93: > loss: 2.961695671081543 \n",
      "Epoch 94: > loss: 1.0411922931671143 \n",
      "Epoch 95: > loss: 0.6390255689620972 \n",
      "Epoch 96: > loss: 1.120718240737915 \n",
      "Epoch 97: > loss: 1.2451410293579102 \n",
      "Epoch 98: > loss: 1.1735031604766846 \n",
      "Epoch 99: > loss: 1.0910799503326416 \n",
      "Epoch 100: > loss: 0.9792144894599915  accuracy: 0.4083333333333333\n",
      "Epoch 101: > loss: 0.9207570552825928 \n",
      "Epoch 102: > loss: 0.8450179100036621 \n",
      "Epoch 103: > loss: 0.8517996072769165 \n",
      "Epoch 104: > loss: 0.7037023305892944 \n",
      "Epoch 105: > loss: 0.6543673872947693 \n",
      "Epoch 106: > loss: 0.6159247159957886 \n",
      "Epoch 107: > loss: 0.4570564329624176 \n",
      "Epoch 108: > loss: 0.41408318281173706 \n",
      "Epoch 109: > loss: 0.4206494987010956 \n",
      "Epoch 110: > loss: 0.29756733775138855  accuracy: 0.525\n",
      "Epoch 111: > loss: 0.2570459246635437 \n",
      "Epoch 112: > loss: 0.24981117248535156 \n",
      "Epoch 113: > loss: 0.25483542680740356 \n",
      "Epoch 114: > loss: 0.13177627325057983 \n",
      "Epoch 115: > loss: 0.13250800967216492 \n",
      "Epoch 116: > loss: 0.1744331419467926 \n",
      "Epoch 117: > loss: 0.11357395350933075 \n",
      "Epoch 118: > loss: 0.09542874246835709 \n",
      "Epoch 119: > loss: 0.14512011408805847 \n",
      "Epoch 120: > loss: 0.06159806624054909  accuracy: 0.48333333333333334\n",
      "Epoch 121: > loss: 0.05257774516940117 \n",
      "Epoch 122: > loss: 0.06314823776483536 \n",
      "Epoch 123: > loss: 0.07801362127065659 \n",
      "Epoch 124: > loss: 0.04467756673693657 \n",
      "Epoch 125: > loss: 0.02431655488908291 \n",
      "Epoch 126: > loss: 0.059305593371391296 \n",
      "Epoch 127: > loss: 0.1168544739484787 \n",
      "Epoch 128: > loss: 0.17301064729690552 \n",
      "Epoch 129: > loss: 0.051987048238515854 \n",
      "Epoch 130: > loss: 0.0890270546078682  accuracy: 0.5416666666666666\n",
      "Epoch 131: > loss: 0.03744416683912277 \n",
      "Epoch 132: > loss: 0.009053408168256283 \n",
      "Epoch 133: > loss: 0.18501614034175873 \n",
      "Epoch 134: > loss: 0.04102104529738426 \n",
      "Epoch 135: > loss: 0.0662599727511406 \n",
      "Epoch 136: > loss: 0.10680095851421356 \n",
      "Epoch 137: > loss: 0.062091369181871414 \n",
      "Epoch 138: > loss: 0.17665958404541016 \n",
      "Epoch 139: > loss: 0.040720392018556595 \n",
      "Epoch 140: > loss: 0.030681096017360687  accuracy: 0.375\n",
      "Epoch 141: > loss: 0.05143554136157036 \n",
      "Epoch 142: > loss: 0.07197219133377075 \n",
      "Epoch 143: > loss: 0.04925103485584259 \n",
      "Epoch 144: > loss: 0.06785941123962402 \n",
      "Epoch 145: > loss: 0.024803312495350838 \n",
      "Epoch 146: > loss: 0.05621819198131561 \n",
      "Epoch 147: > loss: 0.10027657449245453 \n",
      "Epoch 148: > loss: 0.026692193001508713 \n",
      "Epoch 149: > loss: 0.04696293920278549 \n",
      "Epoch 150: > loss: 0.06120503321290016  accuracy: 0.5833333333333334\n",
      "Epoch 151: > loss: 0.027202004566788673 \n",
      "Epoch 152: > loss: 0.01792643405497074 \n",
      "Epoch 153: > loss: 0.02658497728407383 \n",
      "Epoch 154: > loss: 0.011114131659269333 \n",
      "Epoch 155: > loss: 0.00798990111798048 \n",
      "Epoch 156: > loss: 0.040078770369291306 \n",
      "Epoch 157: > loss: 0.015696151182055473 \n",
      "Epoch 158: > loss: 0.015268568880856037 \n",
      "Epoch 159: > loss: 0.00752372108399868 \n",
      "Epoch 160: > loss: 0.003419344313442707  accuracy: 0.4666666666666667\n",
      "Epoch 161: > loss: 0.002961335238069296 \n",
      "Epoch 162: > loss: 0.06175713613629341 \n",
      "Epoch 163: > loss: 0.016908183693885803 \n",
      "Epoch 164: > loss: 0.006614120677113533 \n",
      "Epoch 165: > loss: 0.019471514970064163 \n",
      "Epoch 166: > loss: 0.018370691686868668 \n",
      "Epoch 167: > loss: 0.008607226423919201 \n",
      "Epoch 168: > loss: 0.015094275586307049 \n",
      "Epoch 169: > loss: 0.0021276930347085 \n",
      "Epoch 170: > loss: 0.014513392001390457  accuracy: 0.45\n",
      "Epoch 171: > loss: 0.027288222685456276 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172: > loss: 0.0373898521065712 \n",
      "Epoch 173: > loss: 0.01473834365606308 \n",
      "Epoch 174: > loss: 0.0014653977705165744 \n",
      "Epoch 175: > loss: 0.00825271662324667 \n",
      "Epoch 176: > loss: 0.013887831941246986 \n",
      "Epoch 177: > loss: 0.00657149450853467 \n",
      "Epoch 178: > loss: 0.012339239940047264 \n",
      "Epoch 179: > loss: 0.006471495609730482 \n",
      "Epoch 180: > loss: 0.0012027048505842686  accuracy: 0.5083333333333333\n",
      "Epoch 181: > loss: 0.005075946915894747 \n",
      "Epoch 182: > loss: 0.004031313583254814 \n",
      "Epoch 183: > loss: 0.0012042627204209566 \n",
      "Epoch 184: > loss: 0.02093362808227539 \n",
      "Epoch 185: > loss: 0.00102301687002182 \n",
      "Epoch 186: > loss: 0.009279696270823479 \n",
      "Epoch 187: > loss: 0.0010144306579604745 \n",
      "Epoch 188: > loss: 0.01232072338461876 \n",
      "Epoch 189: > loss: 0.005452505312860012 \n",
      "Epoch 190: > loss: 0.005061659961938858  accuracy: 0.45\n",
      "Epoch 191: > loss: 0.0007396189030259848 \n",
      "Epoch 192: > loss: 0.003551286179572344 \n",
      "Epoch 193: > loss: 0.0006428661290556192 \n",
      "Epoch 194: > loss: 0.03279127553105354 \n",
      "Epoch 195: > loss: 0.001927548903040588 \n",
      "Epoch 196: > loss: 0.014888019300997257 \n",
      "Epoch 197: > loss: 0.013740252703428268 \n",
      "Epoch 198: > loss: 0.0008233741391450167 \n",
      "Epoch 199: > loss: 0.013631877489387989 \n",
      "Epoch 200: > loss: 0.0012844770681113005  accuracy: 0.5916666666666667\n",
      "                  0          1          2          3         4          5  \\\n",
      "f1-score   0.533333   0.333333   0.342857   0.410256  0.470588   0.869565   \n",
      "precision  0.666667   0.350000   0.315789   0.333333  0.444444   0.952381   \n",
      "recall     0.444444   0.318182   0.375000   0.533333  0.500000   0.800000   \n",
      "support    9.000000  22.000000  16.000000  15.000000  8.000000  50.000000   \n",
      "\n",
      "           accuracy   macro avg  weighted avg  \n",
      "f1-score      0.575    0.493322      0.591799  \n",
      "precision     0.575    0.510436      0.624394  \n",
      "recall        0.575    0.495160      0.575000  \n",
      "support       0.575  120.000000    120.000000  \n",
      "\n",
      "\n",
      "sampling:  weight \n",
      "\n",
      "X_train:  (477, 1, 65, 501) \ty_train:  (477,) \tX_test:  (120, 1, 65, 501) \ty_test:  (120,)\n",
      "class:  ['lay' 'pickup' 'sit' 'stand' 'standff' 'walk']\n",
      "class_size:  tensor([0.0734, 0.1803, 0.1384, 0.1300, 0.0629, 0.4151])\n",
      "Start Training\n",
      "Epoch 1: >>>>>>> loss: 1.709364414215088 \n",
      "Epoch 2: >>>>>>> loss: 1.7531540393829346 \n",
      "Epoch 3: >>>>>>> loss: 1.5300933122634888 \n",
      "Epoch 4: >>>>>>> loss: 1.396539330482483 \n",
      "Epoch 5: >>>>>>> loss: 1.1668075323104858 \n",
      "Epoch 6: >>>>>>> loss: 1.2497321367263794 \n",
      "Epoch 7: >>>>>>> loss: 1.1478617191314697 \n",
      "Epoch 8: >>>>>>> loss: 1.2405918836593628 \n",
      "Epoch 9: >>>>>>> loss: 1.0840681791305542 \n",
      "Epoch 10: >>>>>>> loss: 1.0399361848831177  accuracy: 0.6333333333333333\n",
      "Epoch 11: >>>>>>> loss: 0.9910013675689697 \n",
      "Epoch 12: >>>>>>> loss: 0.8998749852180481 \n",
      "Epoch 13: >>>>>>> loss: 0.8629675507545471 \n",
      "Epoch 14: >>>>>>> loss: 0.9027512669563293 \n",
      "Epoch 15: >>>>>>> loss: 0.9392634034156799 \n",
      "Epoch 16: >>>>>>> loss: 1.0095486640930176 \n",
      "Epoch 17: >>>>>>> loss: 0.8174657225608826 \n",
      "Epoch 18: >>>>>>> loss: 0.8785244226455688 \n",
      "Epoch 19: >>>>>>> loss: 0.8117861151695251 \n",
      "Epoch 20: >>>>>>> loss: 0.8251720666885376  accuracy: 0.6666666666666666\n",
      "Epoch 21: >>>>>>> loss: 0.8598810434341431 \n",
      "Epoch 22: >>>>>>> loss: 0.5951851606369019 \n",
      "Epoch 23: >>>>>>> loss: 0.7075374722480774 \n",
      "Epoch 24: >>>>>>> loss: 0.6163960695266724 \n",
      "Epoch 25: >>>>>>> loss: 0.5677306056022644 \n",
      "Epoch 26: >>>>>>> loss: 0.44464054703712463 \n",
      "Epoch 27: >>>>>>> loss: 0.9471256732940674 \n",
      "Epoch 28: >>>>>>> loss: 0.9244289994239807 \n",
      "Epoch 29: >>>>>>> loss: 0.6980485916137695 \n",
      "Epoch 30: >>>>>>> loss: 0.6975095868110657  accuracy: 0.75\n",
      "Epoch 31: >>>>>>> loss: 0.6703264713287354 \n",
      "Epoch 32: >>>>>>> loss: 0.5396591424942017 \n",
      "Epoch 33: >>>>>>> loss: 0.5483687520027161 \n",
      "Epoch 34: >>>>>>> loss: 0.4697830080986023 \n",
      "Epoch 35: >>>>>>> loss: 0.7131108045578003 \n",
      "Epoch 36: >>>>>>> loss: 1.0176258087158203 \n",
      "Epoch 37: >>>>>>> loss: 0.8548492789268494 \n",
      "Epoch 38: >>>>>>> loss: 0.4670138955116272 \n",
      "Epoch 39: >>>>>>> loss: 0.573596715927124 \n",
      "Epoch 40: >>>>>>> loss: 0.3688993752002716  accuracy: 0.7166666666666667\n",
      "Epoch 41: >>>>>>> loss: 0.38732099533081055 \n",
      "Epoch 42: >>>>>>> loss: 0.30620720982551575 \n",
      "Epoch 43: >>>>>>> loss: 0.20839007198810577 \n",
      "Epoch 44: >>>>>>> loss: 0.25412502884864807 \n",
      "Epoch 45: >>>>>>> loss: 0.2393563687801361 \n",
      "Epoch 46: >>>>>>> loss: 0.2386629730463028 \n",
      "Epoch 47: >>>>>>> loss: 0.39765316247940063 \n",
      "Epoch 48: >>>>>>> loss: 0.13989481329917908 \n",
      "Epoch 49: >>>>>>> loss: 0.15655939280986786 \n",
      "Epoch 50: >>>>>>> loss: 0.2207152098417282  accuracy: 0.7333333333333333\n",
      "Epoch 51: >>>>>>> loss: 0.21463169157505035 \n",
      "Epoch 52: >>>>>>> loss: 0.14876967668533325 \n",
      "Epoch 53: >>>>>>> loss: 0.154390349984169 \n",
      "Epoch 54: >>>>>>> loss: 0.20459450781345367 \n",
      "Epoch 55: >>>>>>> loss: 0.07969056069850922 \n",
      "Epoch 56: >>>>>>> loss: 0.2174864113330841 \n",
      "Epoch 57: >>>>>>> loss: 0.16899405419826508 \n",
      "Epoch 58: >>>>>>> loss: 0.1225312203168869 \n",
      "Epoch 59: >>>>>>> loss: 0.15936440229415894 \n",
      "Epoch 60: >>>>>>> loss: 0.08955841511487961  accuracy: 0.7333333333333333\n",
      "Epoch 61: >>>>>>> loss: 0.22662638127803802 \n",
      "Epoch 62: >>>>>>> loss: 0.033315226435661316 \n",
      "Epoch 63: >>>>>>> loss: 0.0930371955037117 \n",
      "Epoch 64: >>>>>>> loss: 0.1264687478542328 \n",
      "Epoch 65: >>>>>>> loss: 0.03070390224456787 \n",
      "Epoch 66: >>>>>>> loss: 0.03765716776251793 \n",
      "Epoch 67: >>>>>>> loss: 0.06986741721630096 \n",
      "Epoch 68: >>>>>>> loss: 0.035085637122392654 \n",
      "Epoch 69: >>>>>>> loss: 0.017782967537641525 \n",
      "Epoch 70: >>>>>>> loss: 0.06285597383975983  accuracy: 0.7\n",
      "Epoch 71: >>>>>>> loss: 0.021692540496587753 \n",
      "Epoch 72: >>>>>>> loss: 0.07095632702112198 \n",
      "Epoch 73: >>>>>>> loss: 0.008428170345723629 \n",
      "Epoch 74: >>>>>>> loss: 0.023308582603931427 \n",
      "Epoch 75: >>>>>>> loss: 0.02152136154472828 \n",
      "Epoch 76: >>>>>>> loss: 0.0065001375041902065 \n",
      "Epoch 77: >>>>>>> loss: 0.018497304990887642 \n",
      "Epoch 78: >>>>>>> loss: 0.04624775052070618 \n",
      "Epoch 79: >>>>>>> loss: 0.038503993302583694 \n",
      "Epoch 80: >>>>>>> loss: 0.062010761350393295  accuracy: 0.6916666666666667\n",
      "Epoch 81: >>>>>>> loss: 0.031145596876740456 \n",
      "Epoch 82: >>>>>>> loss: 0.002407432533800602 \n",
      "Epoch 83: >>>>>>> loss: 0.006256301887333393 \n",
      "Epoch 84: >>>>>>> loss: 0.003909722436219454 \n",
      "Epoch 85: >>>>>>> loss: 0.014666258357465267 \n",
      "Epoch 86: >>>>>>> loss: 0.014102115295827389 \n",
      "Epoch 87: >>>>>>> loss: 0.02292238175868988 \n",
      "Epoch 88: >>>>>>> loss: 0.026494957506656647 \n",
      "Epoch 89: >>>>>>> loss: 0.01742815598845482 \n",
      "Epoch 90: >>>>>>> loss: 0.007643294520676136  accuracy: 0.7166666666666667\n",
      "Epoch 91: >>>>>>> loss: 0.00293162208981812 \n",
      "Epoch 92: >>>>>>> loss: 0.0009742691763676703 \n",
      "Epoch 93: >>>>>>> loss: 0.009384829550981522 \n",
      "Epoch 94: >>>>>>> loss: 0.009664668701589108 \n",
      "Epoch 95: >>>>>>> loss: 0.0026052084285765886 \n",
      "Epoch 96: >>>>>>> loss: 0.24620051681995392 \n",
      "Epoch 97: >>>>>>> loss: 0.037997059524059296 \n",
      "Epoch 98: >>>>>>> loss: 0.04839903116226196 \n",
      "Epoch 99: >>>>>>> loss: 0.0679607167840004 \n",
      "Epoch 100: >>>>>>> loss: 0.03488495200872421  accuracy: 0.7166666666666667\n",
      "Epoch 101: >>>>>>> loss: 0.07124040275812149 \n",
      "Epoch 102: >>>>>>> loss: 0.02239675261080265 \n",
      "Epoch 103: >>>>>>> loss: 0.011099111288785934 \n",
      "Epoch 104: >>>>>>> loss: 0.01637127809226513 \n",
      "Epoch 105: >>>>>>> loss: 0.010163712315261364 \n",
      "Epoch 106: >>>>>>> loss: 0.017848050221800804 \n",
      "Epoch 107: >>>>>>> loss: 0.12769393622875214 \n",
      "Epoch 108: >>>>>>> loss: 0.1582065373659134 \n",
      "Epoch 109: >>>>>>> loss: 0.04465518519282341 \n",
      "Epoch 110: >>>>>>> loss: 0.1696891486644745  accuracy: 0.6833333333333333\n",
      "Epoch 111: >>>>>>> loss: 0.07554636895656586 \n",
      "Epoch 112: >>>>>>> loss: 0.014346228912472725 \n",
      "Epoch 113: >>>>>>> loss: 0.006448378786444664 \n",
      "Epoch 114: >>>>>>> loss: 0.004051612224429846 \n",
      "Epoch 115: >>>>>>> loss: 0.011954396963119507 \n",
      "Epoch 116: >>>>>>> loss: 0.012457180768251419 \n",
      "Epoch 117: >>>>>>> loss: 0.038276080042123795 \n",
      "Epoch 118: >>>>>>> loss: 0.011589588597416878 \n",
      "Epoch 119: >>>>>>> loss: 0.14610011875629425 \n",
      "Epoch 120: >>>>>>> loss: 0.18688875436782837  accuracy: 0.7166666666666667\n",
      "Epoch 121: >>>>>>> loss: 0.03358718380331993 \n",
      "Epoch 122: >>>>>>> loss: 0.04999041184782982 \n",
      "Epoch 123: >>>>>>> loss: 0.06348565220832825 \n",
      "Epoch 124: >>>>>>> loss: 0.008611363358795643 \n",
      "Epoch 125: >>>>>>> loss: 0.010052318684756756 \n",
      "Epoch 126: >>>>>>> loss: 0.0009742369875311852 \n",
      "Epoch 127: >>>>>>> loss: 0.0011496945517137647 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 128: >>>>>>> loss: 0.001568351755850017 \n",
      "Epoch 129: >>>>>>> loss: 0.002378604607656598 \n",
      "Epoch 130: >>>>>>> loss: 0.0006587313255295157  accuracy: 0.7083333333333334\n",
      "Epoch 131: >>>>>>> loss: 0.0004512361774686724 \n",
      "Epoch 132: >>>>>>> loss: 0.0006462421151809394 \n",
      "Epoch 133: >>>>>>> loss: 8.327377872774377e-05 \n",
      "Epoch 134: >>>>>>> loss: 0.00037356436951085925 \n",
      "Epoch 135: >>>>>>> loss: 0.002102221129462123 \n",
      "Epoch 136: >>>>>>> loss: 0.00013752507220488042 \n",
      "Epoch 137: >>>>>>> loss: 0.00061996333533898 \n",
      "Epoch 138: >>>>>>> loss: 0.00023308659729082137 \n",
      "Epoch 139: >>>>>>> loss: 8.741179772187024e-05 \n",
      "Epoch 140: >>>>>>> loss: 0.0005984487943351269  accuracy: 0.7166666666666667\n",
      "Epoch 141: >>>>>>> loss: 1.682177207840141e-05 \n",
      "Epoch 142: >>>>>>> loss: 3.487684443825856e-05 \n",
      "Epoch 143: >>>>>>> loss: 0.00016911157581489533 \n",
      "Epoch 144: >>>>>>> loss: 4.306021946831606e-05 \n",
      "Epoch 145: >>>>>>> loss: 4.484940291149542e-05 \n",
      "Epoch 146: >>>>>>> loss: 0.002215304411947727 \n",
      "Epoch 147: >>>>>>> loss: 0.00012108596274629235 \n",
      "Epoch 148: >>>>>>> loss: 0.00023883690300863236 \n",
      "Epoch 149: >>>>>>> loss: 0.00035981126711703837 \n",
      "Epoch 150: >>>>>>> loss: 4.224103759042919e-05  accuracy: 0.725\n",
      "Epoch 151: >>>>>>> loss: 6.235880573512986e-05 \n",
      "Epoch 152: >>>>>>> loss: 0.0127071812748909 \n",
      "Epoch 153: >>>>>>> loss: 0.0002801294904202223 \n",
      "Epoch 154: >>>>>>> loss: 0.0017562631983309984 \n",
      "Epoch 155: >>>>>>> loss: 3.9152553654275835e-05 \n",
      "Epoch 156: >>>>>>> loss: 7.69925900385715e-05 \n",
      "Epoch 157: >>>>>>> loss: 0.0001297851704293862 \n",
      "Epoch 158: >>>>>>> loss: 0.00027858008979819715 \n",
      "Epoch 159: >>>>>>> loss: 0.006618456449359655 \n",
      "Epoch 160: >>>>>>> loss: 0.002036457182839513  accuracy: 0.6916666666666667\n",
      "Epoch 161: >>>>>>> loss: 0.000540715700481087 \n",
      "Epoch 162: >>>>>>> loss: 0.0007023830548860133 \n",
      "Epoch 163: >>>>>>> loss: 0.004725162871181965 \n",
      "Epoch 164: >>>>>>> loss: 5.986392716295086e-05 \n",
      "Epoch 165: >>>>>>> loss: 0.0004426782252267003 \n",
      "Epoch 166: >>>>>>> loss: 0.00016659192624501884 \n",
      "Epoch 167: >>>>>>> loss: 0.000981822027824819 \n",
      "Epoch 168: >>>>>>> loss: 3.4626642445800826e-05 \n",
      "Epoch 169: >>>>>>> loss: 6.821412534918636e-05 \n",
      "Epoch 170: >>>>>>> loss: 0.00010893143189605325  accuracy: 0.7\n",
      "Epoch 171: >>>>>>> loss: 0.0011214090045541525 \n",
      "Epoch 172: >>>>>>> loss: 0.00048409579903818667 \n",
      "Epoch 173: >>>>>>> loss: 2.3344064175034873e-05 \n",
      "Epoch 174: >>>>>>> loss: 0.0001462688815081492 \n",
      "Epoch 175: >>>>>>> loss: 0.0001430130796507001 \n",
      "Epoch 176: >>>>>>> loss: 4.5607313950313255e-05 \n",
      "Epoch 177: >>>>>>> loss: 0.00010924406524281949 \n",
      "Epoch 178: >>>>>>> loss: 3.293325426056981e-05 \n",
      "Epoch 179: >>>>>>> loss: 4.310535223339684e-05 \n",
      "Epoch 180: >>>>>>> loss: 0.0018402652349323034  accuracy: 0.7\n",
      "Epoch 181: >>>>>>> loss: 0.00011537285899976268 \n",
      "Epoch 182: >>>>>>> loss: 0.00015335759962908924 \n",
      "Epoch 183: >>>>>>> loss: 0.0003873835375998169 \n",
      "Epoch 184: >>>>>>> loss: 0.00037890556268393993 \n",
      "Epoch 185: >>>>>>> loss: 3.048948747164104e-05 \n",
      "Epoch 186: >>>>>>> loss: 6.142570782685652e-05 \n",
      "Epoch 187: >>>>>>> loss: 5.144441092852503e-05 \n",
      "Epoch 188: >>>>>>> loss: 0.0012444013264030218 \n",
      "Epoch 189: >>>>>>> loss: 0.00011723602801794186 \n",
      "Epoch 190: >>>>>>> loss: 7.15292917448096e-05  accuracy: 0.7\n",
      "Epoch 191: >>>>>>> loss: 2.6197541956207715e-05 \n",
      "Epoch 192: >>>>>>> loss: 0.0001823392085498199 \n",
      "Epoch 193: >>>>>>> loss: 6.944716460566269e-06 \n",
      "Epoch 194: >>>>>>> loss: 0.00012140202306909487 \n",
      "Epoch 195: >>>>>>> loss: 0.00017366647080052644 \n",
      "Epoch 196: >>>>>>> loss: 0.00019541053916327655 \n",
      "Epoch 197: >>>>>>> loss: 0.0003239158249925822 \n",
      "Epoch 198: >>>>>>> loss: 2.2909727704245597e-05 \n",
      "Epoch 199: >>>>>>> loss: 1.1666255886666477e-05 \n",
      "Epoch 200: >>>>>>> loss: 2.6421081201988272e-05  accuracy: 0.7166666666666667\n",
      "                  0          1          2     3         4          5  \\\n",
      "f1-score   0.750000   0.488889   0.428571   0.6  0.526316   0.980392   \n",
      "precision  0.857143   0.478261   0.500000   0.6  0.454545   0.961538   \n",
      "recall     0.666667   0.500000   0.375000   0.6  0.625000   1.000000   \n",
      "support    9.000000  22.000000  16.000000  15.0  8.000000  50.000000   \n",
      "\n",
      "           accuracy   macro avg  weighted avg  \n",
      "f1-score      0.725    0.629028      0.721607  \n",
      "precision     0.725    0.641915      0.724578  \n",
      "recall        0.725    0.627778      0.725000  \n",
      "support       0.725  120.000000    120.000000  \n",
      "\n",
      "\n",
      "sampling:  undersampling \n",
      "\n",
      "X_train:  (180, 1, 65, 501) \ty_train:  (180,) \tX_test:  (120, 1, 65, 501) \ty_test:  (120,)\n",
      "class:  ['lay' 'pickup' 'sit' 'stand' 'standff' 'walk']\n",
      "class_size:  tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n",
      "Start Training\n",
      "Epoch 1: >> loss: 2.207775354385376 \n",
      "Epoch 2: >> loss: 1.8000694513320923 \n",
      "Epoch 3: >> loss: 1.793473720550537 \n",
      "Epoch 4: >> loss: 1.7766027450561523 \n",
      "Epoch 5: >> loss: 1.7718983888626099 \n",
      "Epoch 6: >> loss: 1.6947383880615234 \n",
      "Epoch 7: >> loss: 1.6479209661483765 \n",
      "Epoch 8: >> loss: 1.524078130722046 \n",
      "Epoch 9: >> loss: 1.7545031309127808 \n",
      "Epoch 10: >> loss: 1.586243987083435  accuracy: 0.25833333333333336\n",
      "Epoch 11: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> loss: 1.5130113363265991 \n",
      "Epoch 12: >> loss: 1.5833712816238403 \n",
      "Epoch 13: >> loss: 1.5109893083572388 \n",
      "Epoch 14: >> loss: 1.4857940673828125 \n",
      "Epoch 15: >> loss: 1.5650359392166138 \n",
      "Epoch 16: >> loss: 1.3761577606201172 \n",
      "Epoch 17: >> loss: 1.3612818717956543 \n",
      "Epoch 18: >> loss: 1.3748964071273804 \n",
      "Epoch 19: >> loss: 1.4082597494125366 \n",
      "Epoch 20: >> loss: 1.4037549495697021  accuracy: 0.5166666666666667\n",
      "Epoch 21: >> loss: 1.3093256950378418 \n",
      "Epoch 22: >> loss: 1.2563413381576538 \n",
      "Epoch 23: >> loss: 1.223339557647705 \n",
      "Epoch 24: >> loss: 1.0535532236099243 \n",
      "Epoch 25: >> loss: 0.9931107759475708 \n",
      "Epoch 26: >> loss: 1.0639467239379883 \n",
      "Epoch 27: >> loss: 1.3275794982910156 \n",
      "Epoch 28: >> loss: 1.169001579284668 \n",
      "Epoch 29: >> loss: 1.0179144144058228 \n",
      "Epoch 30: >> loss: 1.0639212131500244  accuracy: 0.6166666666666667\n",
      "Epoch 31: >> loss: 1.0175299644470215 \n",
      "Epoch 32: >> loss: 0.9747475385665894 \n",
      "Epoch 33: >> loss: 0.9513515830039978 \n",
      "Epoch 34: >> loss: 0.9870182275772095 \n",
      "Epoch 35: >> loss: 0.7809887528419495 \n",
      "Epoch 36: >> loss: 0.741813063621521 \n",
      "Epoch 37: >> loss: 0.7136262059211731 \n",
      "Epoch 38: >> loss: 0.9079931378364563 \n",
      "Epoch 39: >> loss: 0.6907675862312317 \n",
      "Epoch 40: >> loss: 0.6926288604736328  accuracy: 0.6166666666666667\n",
      "Epoch 41: >> loss: 0.570737898349762 \n",
      "Epoch 42: >> loss: 0.5728026628494263 \n",
      "Epoch 43: >> loss: 0.670009970664978 \n",
      "Epoch 44: >> loss: 0.8755940794944763 \n",
      "Epoch 45: >> loss: 0.6702933311462402 \n",
      "Epoch 46: >> loss: 0.6090205311775208 \n",
      "Epoch 47: >> loss: 0.6984829306602478 \n",
      "Epoch 48: >> loss: 0.4414367973804474 \n",
      "Epoch 49: >> loss: 0.5522279739379883 \n",
      "Epoch 50: >> loss: 0.6116150617599487  accuracy: 0.6583333333333333\n",
      "Epoch 51: >> loss: 0.45867159962654114 \n",
      "Epoch 52: >> loss: 0.5112581253051758 \n",
      "Epoch 53: >> loss: 0.47093695402145386 \n",
      "Epoch 54: >> loss: 0.6690282821655273 \n",
      "Epoch 55: >> loss: 0.5840708613395691 \n",
      "Epoch 56: >> loss: 0.5743108987808228 \n",
      "Epoch 57: >> loss: 0.46726489067077637 \n",
      "Epoch 58: >> loss: 0.5649123191833496 \n",
      "Epoch 59: >> loss: 0.5476222038269043 \n",
      "Epoch 60: >> loss: 0.41462498903274536  accuracy: 0.65\n",
      "Epoch 61: >> loss: 0.38344451785087585 \n",
      "Epoch 62: >> loss: 0.4981158971786499 \n",
      "Epoch 63: >> loss: 0.30970171093940735 \n",
      "Epoch 64: >> loss: 0.40551671385765076 \n",
      "Epoch 65: >> loss: 0.3342926502227783 \n",
      "Epoch 66: >> loss: 0.2144596427679062 \n",
      "Epoch 67: >> loss: 0.403957337141037 \n",
      "Epoch 68: >> loss: 0.42779409885406494 \n",
      "Epoch 69: >> loss: 0.3797053098678589 \n",
      "Epoch 70: >> loss: 0.8300089836120605  accuracy: 0.5916666666666667\n",
      "Epoch 71: >> loss: 0.37582117319107056 \n",
      "Epoch 72: >> loss: 0.4256857931613922 \n",
      "Epoch 73: >> loss: 0.24774642288684845 \n",
      "Epoch 74: >> loss: 0.33712077140808105 \n",
      "Epoch 75: >> loss: 0.34990260004997253 \n",
      "Epoch 76: >> loss: 0.21686236560344696 \n",
      "Epoch 77: >> loss: 0.15596678853034973 \n",
      "Epoch 78: >> loss: 0.2171715945005417 \n",
      "Epoch 79: >> loss: 0.2343737781047821 \n",
      "Epoch 80: >> loss: 0.1054537445306778  accuracy: 0.5583333333333333\n",
      "Epoch 81: >> loss: 0.2190169394016266 \n",
      "Epoch 82: >> loss: 0.13672879338264465 \n",
      "Epoch 83: >> loss: 0.1044372096657753 \n",
      "Epoch 84: >> loss: 0.11692074686288834 \n",
      "Epoch 85: >> loss: 0.1046779677271843 \n",
      "Epoch 86: >> loss: 0.07115219533443451 \n",
      "Epoch 87: >> loss: 0.0620160847902298 \n",
      "Epoch 88: >> loss: 0.08957769721746445 \n",
      "Epoch 89: >> loss: 0.07892395555973053 \n",
      "Epoch 90: >> loss: 0.06559818983078003  accuracy: 0.7083333333333334\n",
      "Epoch 91: >> loss: 0.06714576482772827 \n",
      "Epoch 92: >> loss: 0.07798977941274643 \n",
      "Epoch 93: >> loss: 0.19918131828308105 \n",
      "Epoch 94: >> loss: 0.24185810983181 \n",
      "Epoch 95: >> loss: 0.7353695631027222 \n",
      "Epoch 96: >> loss: 0.3722744584083557 \n",
      "Epoch 97: >> loss: 1.029329538345337 \n",
      "Epoch 98: >> loss: 0.5082410573959351 \n",
      "Epoch 99: >> loss: 0.7523680925369263 \n",
      "Epoch 100: >> loss: 0.40269988775253296  accuracy: 0.55\n",
      "Epoch 101: >> loss: 0.35101795196533203 \n",
      "Epoch 102: >> loss: 0.40451762080192566 \n",
      "Epoch 103: >> loss: 0.3380206823348999 \n",
      "Epoch 104: >> loss: 0.22589749097824097 \n",
      "Epoch 105: >> loss: 0.40684521198272705 \n",
      "Epoch 106: >> loss: 0.21771100163459778 \n",
      "Epoch 107: >> loss: 0.1248326450586319 \n",
      "Epoch 108: >> loss: 0.18439052999019623 \n",
      "Epoch 109: >> loss: 0.06239059194922447 \n",
      "Epoch 110: >> loss: 0.09610027074813843  accuracy: 0.6083333333333333\n",
      "Epoch 111: >> loss: 0.10719601809978485 \n",
      "Epoch 112: >> loss: 0.08545853197574615 \n",
      "Epoch 113: >> loss: 0.0671379566192627 \n",
      "Epoch 114: >> loss: 0.047134317457675934 \n",
      "Epoch 115: >> loss: 0.04050911217927933 \n",
      "Epoch 116: >> loss: 0.022248264402151108 \n",
      "Epoch 117: >> loss: 0.06284121423959732 \n",
      "Epoch 118: >> loss: 0.011625743471086025 \n",
      "Epoch 119: >> loss: 0.09964675456285477 \n",
      "Epoch 120: >> loss: 0.04301874339580536  accuracy: 0.6333333333333333\n",
      "Epoch 121: >> loss: 0.08241724967956543 \n",
      "Epoch 122: >> loss: 0.009970227256417274 \n",
      "Epoch 123: >> loss: 0.014787050895392895 \n",
      "Epoch 124: >> loss: 0.015149685554206371 \n",
      "Epoch 125: >> loss: 0.03378177061676979 \n",
      "Epoch 126: >> loss: 0.008341075852513313 \n",
      "Epoch 127: >> loss: 0.00947135966271162 \n",
      "Epoch 128: >> loss: 0.010063154622912407 \n",
      "Epoch 129: >> loss: 0.01043833326548338 \n",
      "Epoch 130: >> loss: 0.009065104648470879  accuracy: 0.6083333333333333\n",
      "Epoch 131: >> loss: 0.029517047107219696 \n",
      "Epoch 132: >> loss: 0.015940437093377113 \n",
      "Epoch 133: >> loss: 0.002265503164380789 \n",
      "Epoch 134: >> loss: 0.0030432885978370905 \n",
      "Epoch 135: >> loss: 0.10201384127140045 \n",
      "Epoch 136: >> loss: 0.015649456530809402 \n",
      "Epoch 137: >> loss: 0.014878321439027786 \n",
      "Epoch 138: >> loss: 0.005483508575707674 \n",
      "Epoch 139: >> loss: 0.009011581540107727 \n",
      "Epoch 140: >> loss: 0.010737036354839802  accuracy: 0.6\n",
      "Epoch 141: >> loss: 0.009041649289429188 \n",
      "Epoch 142: >> loss: 0.004047296475619078 \n",
      "Epoch 143: >> loss: 0.003909628838300705 \n",
      "Epoch 144: >> loss: 0.004433343652635813 \n",
      "Epoch 145: >> loss: 0.0033204294741153717 \n",
      "Epoch 146: >> loss: 0.014067761600017548 \n",
      "Epoch 147: >> loss: 0.011238401755690575 \n",
      "Epoch 148: >> loss: 0.03512203320860863 \n",
      "Epoch 149: >> loss: 0.00315363728441298 \n",
      "Epoch 150: >> loss: 0.013819819316267967  accuracy: 0.6666666666666666\n",
      "Epoch 151: >> loss: 0.0061153932474553585 \n",
      "Epoch 152: >> loss: 0.006133734714239836 \n",
      "Epoch 153: >> loss: 0.003053288906812668 \n",
      "Epoch 154: >> loss: 0.0021177788730710745 \n",
      "Epoch 155: >> loss: 0.0005558126722462475 \n",
      "Epoch 156: >> loss: 0.0032234536483883858 \n",
      "Epoch 157: >> loss: 0.0002990703796967864 \n",
      "Epoch 158: >> loss: 0.007442737463861704 \n",
      "Epoch 159: >> loss: 0.0013422248885035515 \n",
      "Epoch 160: >> loss: 0.0018815156072378159  accuracy: 0.6583333333333333\n",
      "Epoch 161: >> loss: 0.010218054056167603 \n",
      "Epoch 162: >> loss: 0.0030009911861270666 \n",
      "Epoch 163: >> loss: 0.0004721989971585572 \n",
      "Epoch 164: >> loss: 0.002214481122791767 \n",
      "Epoch 165: >> loss: 0.006693726405501366 \n",
      "Epoch 166: >> loss: 0.00023390122805722058 \n",
      "Epoch 167: >> loss: 0.0046745287254452705 \n",
      "Epoch 168: >> loss: 0.0005738273612223566 \n",
      "Epoch 169: >> loss: 0.0005832131137140095 \n",
      "Epoch 170: >> loss: 0.004129189997911453  accuracy: 0.6416666666666667\n",
      "Epoch 171: >> loss: 0.00031113531440496445 \n",
      "Epoch 172: >> loss: 0.11754144728183746 \n",
      "Epoch 173: >> loss: 0.004254614003002644 \n",
      "Epoch 174: >> loss: 0.05666698142886162 \n",
      "Epoch 175: >> loss: 0.007684684824198484 \n",
      "Epoch 176: >> loss: 0.0033711963333189487 \n",
      "Epoch 177: >> loss: 0.012087457813322544 \n",
      "Epoch 178: >> loss: 0.06913929432630539 \n",
      "Epoch 179: >> loss: 0.012107553891837597 \n",
      "Epoch 180: >> loss: 0.016470396891236305  accuracy: 0.6083333333333333\n",
      "Epoch 181: >> loss: 0.11632426828145981 \n",
      "Epoch 182: >> loss: 0.01356868352741003 \n",
      "Epoch 183: >> loss: 0.036977820098400116 \n",
      "Epoch 184: >> loss: 0.032286714762449265 \n",
      "Epoch 185: >> loss: 0.03640257567167282 \n",
      "Epoch 186: >> loss: 0.006718222517520189 \n",
      "Epoch 187: >> loss: 0.04191107302904129 \n",
      "Epoch 188: >> loss: 0.0053145005367696285 \n",
      "Epoch 189: >> loss: 0.018452288582921028 \n",
      "Epoch 190: >> loss: 0.07582174241542816  accuracy: 0.6416666666666667\n",
      "Epoch 191: >> loss: 0.006316741928458214 \n",
      "Epoch 192: >> loss: 0.10979699343442917 \n",
      "Epoch 193: >> loss: 0.016468791291117668 \n",
      "Epoch 194: >> loss: 0.012115657329559326 \n",
      "Epoch 195: >> loss: 0.01093057170510292 \n",
      "Epoch 196: >> loss: 0.013543561100959778 \n",
      "Epoch 197: >> loss: 0.014955179765820503 \n",
      "Epoch 198: >> loss: 0.0024732393212616444 \n",
      "Epoch 199: >> loss: 0.002078308491036296 \n",
      "Epoch 200: >> loss: 0.01942719891667366  accuracy: 0.6416666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  0          1          2          3         4          5  \\\n",
      "f1-score   0.400000   0.487805   0.514286   0.400000  0.500000   0.872340   \n",
      "precision  0.312500   0.526316   0.473684   0.500000  0.416667   0.931818   \n",
      "recall     0.555556   0.454545   0.562500   0.333333  0.625000   0.820000   \n",
      "support    9.000000  22.000000  16.000000  15.000000  8.000000  50.000000   \n",
      "\n",
      "           accuracy   macro avg  weighted avg  \n",
      "f1-score      0.625    0.529072      0.634811  \n",
      "precision     0.625    0.526831      0.661622  \n",
      "recall        0.625    0.558489      0.625000  \n",
      "support       0.625  120.000000    120.000000  \n",
      "\n",
      "\n",
      "sampling:  oversampling \n",
      "\n",
      "X_train:  (1188, 1, 65, 501) \ty_train:  (1188,) \tX_test:  (120, 1, 65, 501) \ty_test:  (120,)\n",
      "class:  ['lay' 'pickup' 'sit' 'stand' 'standff' 'walk']\n",
      "class_size:  tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n",
      "Start Training\n",
      "Epoch 1: >>>>>>>>>>>>>>>>>> loss: 1.572352647781372 \n",
      "Epoch 2: >>>>>>>>>>>>>>>>>> loss: 1.4893513917922974 \n",
      "Epoch 3: >>>>>>>>>>>>>>>>>> loss: 1.437414288520813 \n",
      "Epoch 4: >>>>>>>>>>>>>>>>>> loss: 1.4622762203216553 \n",
      "Epoch 5: >>>>>>>>>>>>>>>>>> loss: 1.1449689865112305 \n",
      "Epoch 6: >>>>>>>>>>>>>>>>>> loss: 1.2117674350738525 \n",
      "Epoch 7: >>>>>>>>>>>>>>>>>> loss: 0.932181179523468 \n",
      "Epoch 8: >>>>>>>>>>>>>>>>>> loss: 0.8190175890922546 \n",
      "Epoch 9: >>>>>>>>>>>>>>>>>> loss: 0.5383507609367371 \n",
      "Epoch 10: >>>>>>>>>>>>>>>>>> loss: 0.6050463914871216  accuracy: 0.6083333333333333\n",
      "Epoch 11: >>>>>>>>>>>>>>>>>> loss: 0.5510497689247131 \n",
      "Epoch 12: >>>>>>>>>>>>>>>>>> loss: 0.607607901096344 \n",
      "Epoch 13: >>>>>>>>>>>>>>>>>> loss: 0.444917231798172 \n",
      "Epoch 14: >>>>>>>>>>>>>>>>>> loss: 0.3163328468799591 \n",
      "Epoch 15: >>>>>>>>>>>>>>>>>> loss: 0.3782173991203308 \n",
      "Epoch 16: >>>>>>>>>>>>>>>>>> loss: 0.19519367814064026 \n",
      "Epoch 17: >>>>>>>>>>>>>>>>>> loss: 0.2861393690109253 \n",
      "Epoch 18: >>>>>>>>>>>>>>>>>> loss: 0.179897278547287 \n",
      "Epoch 19: >>>>>>>>>>>>>>>>>> loss: 0.07672841846942902 \n",
      "Epoch 20: >>>>>>>>>>>>>>>>>> loss: 0.06231209635734558  accuracy: 0.6916666666666667\n",
      "Epoch 21: >>>>>>>>>>>>>>>>>> loss: 0.2443183958530426 \n",
      "Epoch 22: >>>>>>>>>>>>>>>>>> loss: 0.05741339176893234 \n",
      "Epoch 23: >>>>>>>>>>>>>>>>>> loss: 0.27170032262802124 \n",
      "Epoch 24: >>>>>>>>>>>>>>>>>> loss: 0.07341758906841278 \n",
      "Epoch 25: >>>>>>>>>>>>>>>>>> loss: 0.027078282088041306 \n",
      "Epoch 26: >>>>>>>>>>>>>>>>>> loss: 0.009734108112752438 \n",
      "Epoch 27: >>>>>>>>>>>>>>>>>> loss: 0.0205488670617342 \n",
      "Epoch 28: >>>>>>>>>>>>>>>>>> loss: 0.02391839772462845 \n",
      "Epoch 29: >>>>>>>>>>>>>>>>>> loss: 0.030207263305783272 \n",
      "Epoch 30: >>>>>>>>>>>>>>>>>> loss: 0.023259593173861504  accuracy: 0.6833333333333333\n",
      "Epoch 31: >>>>>>>>>>>>>>>>>> loss: 0.29378408193588257 \n",
      "Epoch 32: >>>>>>>>>>>>>>>>>> loss: 0.1806519329547882 \n",
      "Epoch 33: >>>>>>>>>>>>>>>>>> loss: 0.07183738797903061 \n",
      "Epoch 34: >>>>>>>>>>>>>>>>>> loss: 0.026867657899856567 \n",
      "Epoch 35: >>>>>>>>>>>>>>>>>> loss: 0.0707072839140892 \n",
      "Epoch 36: >>>>>>>>>>>>>>>>>> loss: 0.04990768060088158 \n",
      "Epoch 37: >>>>>>>>>>>>>>>>>> loss: 0.014053351245820522 \n",
      "Epoch 38: >>>>>>>>>>>>>>>>>> loss: 0.0026103549171239138 \n",
      "Epoch 39: >>>>>>>>>>>>>>>>>> loss: 0.0010630626929923892 \n",
      "Epoch 40: >>>>>>>>>>>>>>>>>> loss: 0.0066442107781767845  accuracy: 0.7166666666666667\n",
      "Epoch 41: >>>>>>>>>>>>>>>>>> loss: 0.04689987748861313 \n",
      "Epoch 42: >>>>>>>>>>>>>>>>>> loss: 0.001484784996137023 \n",
      "Epoch 43: >>>>>>>>>>>>>>>>>> loss: 0.39281702041625977 \n",
      "Epoch 44: >>>>>>>>>>>>>>>>>> loss: 0.0191871989518404 \n",
      "Epoch 45: >>>>>>>>>>>>>>>>>> loss: 0.024345673620700836 \n",
      "Epoch 46: >>>>>>>>>>>>>>>>>> loss: 0.0010346450144425035 \n",
      "Epoch 47: >>>>>>>>>>>>>>>>>> loss: 0.00038253923412412405 \n",
      "Epoch 48: >>>>>>>>>>>>>>>>>> loss: 0.016506221145391464 \n",
      "Epoch 49: >>>>>>>>>>>>>>>>>> loss: 0.0016603412805125117 \n",
      "Epoch 50: >>>>>>>>>>>>>>>>>> loss: 0.007092236541211605  accuracy: 0.7\n",
      "Epoch 51: >>>>>>>>>>>>>>>>>> loss: 0.02112598903477192 \n",
      "Epoch 52: >>>>>>>>>>>>>>>>>> loss: 0.01749630644917488 \n",
      "Epoch 53: >>>>>>>>>>>>>>>>>> loss: 0.0026421700604259968 \n",
      "Epoch 54: >>>>>>>>>>>>>>>>>> loss: 0.0042349654249846935 \n",
      "Epoch 55: >>>>>>>>>>>>>>>>>> loss: 0.0062176818028092384 \n",
      "Epoch 56: >>>>>>>>>>>>>>>>>> loss: 0.007286351639777422 \n",
      "Epoch 57: >>>>>>>>>>>>>>>>>> loss: 0.12421058863401413 \n",
      "Epoch 58: >>>>>>>>>>>>>>>>>> loss: 0.00409229751676321 \n",
      "Epoch 59: >>>>>>>>>>>>>>>>>> loss: 0.0009128506062552333 \n",
      "Epoch 60: >>>>>>>>>>>>>>>>>> loss: 0.0003396625106688589  accuracy: 0.7\n",
      "Epoch 61: >>>>>>>>>>>>>>>>>> loss: 9.257693454856053e-05 \n",
      "Epoch 62: >>>>>>>>>>>>>>>>>> loss: 0.0015200509224087 \n",
      "Epoch 63: >>>>>>>>>>>>>>>>>> loss: 0.0001339444424957037 \n",
      "Epoch 64: >>>>>>>>>>>>>>>>>> loss: 0.00036280660424381495 \n",
      "Epoch 65: >>>>>>>>>>>>>>>>>> loss: 0.0001040424540406093 \n",
      "Epoch 66: >>>>>>>>>>>>>>>>>> loss: 2.4037797629716806e-05 \n",
      "Epoch 67: >>>>>>>>>>>>>>>>>> loss: 3.557978561730124e-05 \n",
      "Epoch 68: >>>>>>>>>>>>>>>>>> loss: 0.0003103140043094754 \n",
      "Epoch 69: >>>>>>>>>>>>>>>>>> loss: 0.03211424499750137 \n",
      "Epoch 70: >>>>>>>>>>>>>>>>>> loss: 0.1145344078540802  accuracy: 0.625\n",
      "Epoch 71: >>>>>>>>>>>>>>>>>> loss: 0.054732486605644226 \n",
      "Epoch 72: >>>>>>>>>>>>>>>>>> loss: 0.11691892892122269 \n",
      "Epoch 73: >>>>>>>>>>>>>>>>>> loss: 0.050492167472839355 \n",
      "Epoch 74: >>>>>>>>>>>>>>>>>> loss: 0.09006481617689133 \n",
      "Epoch 75: >>>>>>>>>>>>>>>>>> loss: 0.04326711222529411 \n",
      "Epoch 76: >>>>>>>>>>>>>>>>>> loss: 0.14757764339447021 \n",
      "Epoch 77: >>>>>>>>>>>>>>>>>> loss: 0.003738357685506344 \n",
      "Epoch 78: >>>>>>>>>>>>>>>>>> loss: 0.040667083114385605 \n",
      "Epoch 79: >>>>>>>>>>>>>>>>>> loss: 0.0057236794382333755 \n",
      "Epoch 80: >>>>>>>>>>>>>>>>>> loss: 0.030440038070082664  accuracy: 0.675\n",
      "Epoch 81: >>>>>>>>>>>>>>>>>> loss: 0.011187933385372162 \n",
      "Epoch 82: >>>>>>>>>>>>>>>>>> loss: 0.009696087799966335 \n",
      "Epoch 83: >>>>>>>>>>>>>>>>>> loss: 0.00011518601968418807 \n",
      "Epoch 84: >>>>>>>>>>>>>>>>>> loss: 0.0006722240941599011 \n",
      "Epoch 85: >>>>>>>>>>>>>>>>>> loss: 0.00040369556518271565 \n",
      "Epoch 86: >>>>>>>>>>>>>>>>>> loss: 0.00015644007362425327 \n",
      "Epoch 87: >>>>>>>>>>>>>>>>>> loss: 6.015499820932746e-05 \n",
      "Epoch 88: >>>>>>>>>>>>>>>>>> loss: 0.000588107795920223 \n",
      "Epoch 89: >>>>>>>>>>>>>>>>>> loss: 0.0005589064676314592 \n",
      "Epoch 90: >>>>>>>>>>>>>>>>>> loss: 9.150653932010755e-05  accuracy: 0.675\n",
      "Epoch 91: >>>>>>>>>>>>>>>>>> loss: 0.0014256094582378864 \n",
      "Epoch 92: >>>>>>>>>>>>>>>>>> loss: 0.0012052964884787798 \n",
      "Epoch 93: >>>>>>>>>>>>>>>>>> loss: 0.00126706063747406 \n",
      "Epoch 94: >>>>>>>>>>>>>>>>>> loss: 0.00011310852278256789 \n",
      "Epoch 95: >>>>>>>>>>>>>>>>>> loss: 0.0017328796675428748 \n",
      "Epoch 96: >>>>>>>>>>>>>>>>>> loss: 0.0003211527073290199 \n",
      "Epoch 97: >>>>>>>>>>>>>>>>>> loss: 0.00010530578583711758 \n",
      "Epoch 98: >>>>>>>>>>>>>>>>>> loss: 1.0858556379389483e-05 \n",
      "Epoch 99: >>>>>>>>>>>>>>>>>> loss: 6.129033863544464e-05 \n",
      "Epoch 100: >>>>>>>>>>>>>>>>>> loss: 5.610775406239554e-05  accuracy: 0.6833333333333333\n",
      "Epoch 101: >>>>>>>>>>>>>>>>>> loss: 3.66868480341509e-05 \n",
      "Epoch 102: >>>>>>>>>>>>>>>>>> loss: 0.0010476976167410612 \n",
      "Epoch 103: >>>>>>>>>>>>>>>>>> loss: 0.0004157327057328075 \n",
      "Epoch 104: >>>>>>>>>>>>>>>>>> loss: 2.7970507289865054e-05 \n",
      "Epoch 105: >>>>>>>>>>>>>>>>>> loss: 1.3806771676172502e-05 \n",
      "Epoch 106: >>>>>>>>>>>>>>>>>> loss: 5.326954124029726e-05 \n",
      "Epoch 107: >>>>>>>>>>>>>>>>>> loss: 3.407863914617337e-05 \n",
      "Epoch 108: >>>>>>>>>>>>>>>>>> loss: 0.000129569714772515 \n",
      "Epoch 109: >>>>>>>>>>>>>>>>>> loss: 6.608420517295599e-05 \n",
      "Epoch 110: >>>>>>>>>>>>>>>>>> loss: 5.621272975986358e-06  accuracy: 0.6833333333333333\n",
      "Epoch 111: >>>>>>>>>>>>>>>>>> loss: 0.006038560066372156 \n",
      "Epoch 112: >>>>>>>>>>>>>>>>>> loss: 0.00047795241698622704 \n",
      "Epoch 113: >>>>>>>>>>>>>>>>>> loss: 5.046039223088883e-05 \n",
      "Epoch 114: >>>>>>>>>>>>>>>>>> loss: 1.5594348951708525e-05 \n",
      "Epoch 115: >>>>>>>>>>>>>>>>>> loss: 4.250144775141962e-05 \n",
      "Epoch 116: >>>>>>>>>>>>>>>>>> loss: 0.001095108687877655 \n",
      "Epoch 117: >>>>>>>>>>>>>>>>>> loss: 4.664983134716749e-05 \n",
      "Epoch 118: >>>>>>>>>>>>>>>>>> loss: 3.090211976086721e-05 \n",
      "Epoch 119: >>>>>>>>>>>>>>>>>> loss: 0.0007417688029818237 \n",
      "Epoch 120: >>>>>>>>>>>>>>>>>> loss: 0.0002932808711193502  accuracy: 0.7333333333333333\n",
      "Epoch 121: >>>>>>>>>>>>>>>>>> loss: 0.12218214571475983 \n",
      "Epoch 122: >>>>>>>>>>>>>>>>>> loss: 0.28142473101615906 \n",
      "Epoch 123: >>>>>>>>>>>>>>>>>> loss: 0.03473289683461189 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124: >>>>>>>>>>>>>>>>>> loss: 0.05176563188433647 \n",
      "Epoch 125: >>>>>>>>>>>>>>>>>> loss: 0.05626789852976799 \n",
      "Epoch 126: >>>>>>>>>>>>>>>>>> loss: 0.08869876712560654 \n",
      "Epoch 127: >>>>>>>>>>>>>>>>>> loss: 0.03765399008989334 \n",
      "Epoch 128: >>>>>>>>>>>>>>>>>> loss: 0.012035449966788292 \n",
      "Epoch 129: >>>>>>>>>>>>>>>>>> loss: 0.0005959714762866497 \n",
      "Epoch 130: >>>>>>>>>>>>>>>>>> loss: 0.0003537412849254906  accuracy: 0.675\n",
      "Epoch 131: >>>>>>>>>>>>>>>>>> loss: 0.0031355239916592836 \n",
      "Epoch 132: >>>>>>>>>>>>>>>>>> loss: 0.0015164125943556428 \n",
      "Epoch 133: >>>>>>>>>>>>>>>>>> loss: 3.9093371015042067e-05 \n",
      "Epoch 134: >>>>>>>>>>>>>>>>>> loss: 5.382402741815895e-05 \n",
      "Epoch 135: >>>>>>>>>>>>>>>>>> loss: 0.00028713420033454895 \n",
      "Epoch 136: >>>>>>>>>>>>>>>>>> loss: 0.00026747799711301923 \n",
      "Epoch 137: >>>>>>>>>>>>>>>>>> loss: 0.0011456637876108289 \n",
      "Epoch 138: >>>>>>>>>>>>>>>>>> loss: 0.0020725037902593613 \n",
      "Epoch 139: >>>>>>>>>>>>>>>>>> loss: 1.6000154573703185e-05 \n",
      "Epoch 140: >>>>>>>>>>>>>>>>>> loss: 0.00010651152115315199  accuracy: 0.6833333333333333\n",
      "Epoch 141: >>>>>>>>>>>>>>>>>> loss: 0.00034101909841410816 \n",
      "Epoch 142: >>>>>>>>>>>>>>>>>> loss: 0.006843335926532745 \n",
      "Epoch 143: >>>>>>>>>>>>>>>>>> loss: 0.0011812555603682995 \n",
      "Epoch 144: >>>>>>>>>>>>>>>>>> loss: 0.00017999640840571374 \n",
      "Epoch 145: >>>>>>>>>>>>>>>>>> loss: 4.971749513060786e-05 \n",
      "Epoch 146: >>>>>>>>>>>>>>>>>> loss: 2.5881494366331026e-05 \n",
      "Epoch 147: >>>>>>>>>>>>>>>>>> loss: 3.038640170416329e-05 \n",
      "Epoch 148: >>>>>>>>>>>>>>>>>> loss: 8.21763533167541e-05 \n",
      "Epoch 149: >>>>>>>>>>>>>>>>>> loss: 9.262112143915147e-06 \n",
      "Epoch 150: >>>>>>>>>>>>>>>>>> loss: 3.2568423193879426e-05  accuracy: 0.7083333333333334\n",
      "Epoch 151: >>>>>>>>>>>>>>>>>> loss: 7.411975821014494e-05 \n",
      "Epoch 152: >>>>>>>>>>>>>>>>>> loss: 4.581670509651303e-06 \n",
      "Epoch 153: >>>>>>>>>>>>>>>>>> loss: 1.4770585039514117e-06 \n",
      "Epoch 154: >>>>>>>>>>>>>>>>>> loss: 1.342956920780125e-06 \n",
      "Epoch 155: >>>>>>>>>>>>>>>>>> loss: 6.2412132137978915e-06 \n",
      "Epoch 156: >>>>>>>>>>>>>>>>>> loss: 6.0756533457606565e-06 \n",
      "Epoch 157: >>>>>>>>>>>>>>>>>> loss: 0.0003396104439161718 \n",
      "Epoch 158: >>>>>>>>>>>>>>>>>> loss: 1.0691516081351438e-06 \n",
      "Epoch 159: >>>>>>>>>>>>>>>>>> loss: 1.4882331242915825e-06 \n",
      "Epoch 160: >>>>>>>>>>>>>>>>>> loss: 1.2400248124322388e-05  accuracy: 0.7083333333333334\n",
      "Epoch 161: >>>>>>>>>>>>>>>>>> loss: 1.4070764336793218e-05 \n",
      "Epoch 162: >>>>>>>>>>>>>>>>>> loss: 8.915007128962316e-06 \n",
      "Epoch 163: >>>>>>>>>>>>>>>>>> loss: 1.978096179300337e-06 \n",
      "Epoch 164: >>>>>>>>>>>>>>>>>> loss: 3.959807145292871e-06 \n",
      "Epoch 165: >>>>>>>>>>>>>>>>>> loss: 4.224339591019088e-06 \n",
      "Epoch 166: >>>>>>>>>>>>>>>>>> loss: 8.493589120917022e-07 \n",
      "Epoch 167: >>>>>>>>>>>>>>>>>> loss: 1.3109690371493343e-05 \n",
      "Epoch 168: >>>>>>>>>>>>>>>>>> loss: 1.1771688832595828e-06 \n",
      "Epoch 169: >>>>>>>>>>>>>>>>>> loss: 8.866125540407666e-07 \n",
      "Epoch 170: >>>>>>>>>>>>>>>>>> loss: 3.48894354829099e-05  accuracy: 0.725\n",
      "Epoch 171: >>>>>>>>>>>>>>>>>> loss: 2.5107397050305735e-06 \n",
      "Epoch 172: >>>>>>>>>>>>>>>>>> loss: 2.0495737771852873e-05 \n",
      "Epoch 173: >>>>>>>>>>>>>>>>>> loss: 2.711862862270209e-06 \n",
      "Epoch 174: >>>>>>>>>>>>>>>>>> loss: 9.176390449283645e-06 \n",
      "Epoch 175: >>>>>>>>>>>>>>>>>> loss: 6.332675638986984e-06 \n",
      "Epoch 176: >>>>>>>>>>>>>>>>>> loss: 2.5509049009997398e-05 \n",
      "Epoch 177: >>>>>>>>>>>>>>>>>> loss: 9.834435331868008e-06 \n",
      "Epoch 178: >>>>>>>>>>>>>>>>>> loss: 0.001352566760033369 \n",
      "Epoch 179: >>>>>>>>>>>>>>>>>> loss: 3.4959491586050717e-06 \n",
      "Epoch 180: >>>>>>>>>>>>>>>>>> loss: 0.0001710819487925619  accuracy: 0.7\n",
      "Epoch 181: >>>>>>>>>>>>>>>>>> loss: 3.650772555374715e-07 \n",
      "Epoch 182: >>>>>>>>>>>>>>>>>> loss: 5.705295916413888e-05 \n",
      "Epoch 183: >>>>>>>>>>>>>>>>>> loss: 1.8495704807719449e-06 \n",
      "Epoch 184: >>>>>>>>>>>>>>>>>> loss: 1.6725826981200953e-06 \n",
      "Epoch 185: >>>>>>>>>>>>>>>>>> loss: 4.795666882273508e-06 \n",
      "Epoch 186: >>>>>>>>>>>>>>>>>> loss: 1.9869550669682212e-05 \n",
      "Epoch 187: >>>>>>>>>>>>>>>>>> loss: 7.597793592140079e-05 \n",
      "Epoch 188: >>>>>>>>>>>>>>>>>> loss: 1.7422846212866716e-05 \n",
      "Epoch 189: >>>>>>>>>>>>>>>>>> loss: 4.4144533717371814e-07 \n",
      "Epoch 190: >>>>>>>>>>>>>>>>>> loss: 7.028225354588358e-06  accuracy: 0.7\n",
      "Epoch 191: >>>>>>>>>>>>>>>>>> loss: 1.1436478644100134e-06 \n",
      "Epoch 192: >>>>>>>>>>>>>>>>>> loss: 1.1380656133042066e-06 \n",
      "Epoch 193: >>>>>>>>>>>>>>>>>> loss: 9.72289171841112e-07 \n",
      "Epoch 194: >>>>>>>>>>>>>>>>>> loss: 2.3263150978891645e-06 \n",
      "Epoch 195: >>>>>>>>>>>>>>>>>> loss: 3.54801682078687e-06 \n",
      "Epoch 196: >>>>>>>>>>>>>>>>>> loss: 1.6204518260565237e-06 \n",
      "Epoch 197: >>>>>>>>>>>>>>>>>> loss: 8.065177894422959e-07 \n",
      "Epoch 198: >>>>>>>>>>>>>>>>>> loss: 7.897573368609301e-07 \n",
      "Epoch 199: >>>>>>>>>>>>>>>>>> loss: 1.5832189319553436e-06 \n",
      "Epoch 200: >>>>>>>>>>>>>>>>>> loss: 4.116425031952531e-07  accuracy: 0.6833333333333333\n",
      "                  0          1     2          3         4          5  \\\n",
      "f1-score   0.571429   0.510638   0.5   0.551724  0.428571   0.942308   \n",
      "precision  0.800000   0.480000   0.5   0.571429  0.500000   0.907407   \n",
      "recall     0.444444   0.545455   0.5   0.533333  0.375000   0.980000   \n",
      "support    9.000000  22.000000  16.0  15.000000  8.000000  50.000000   \n",
      "\n",
      "           accuracy   macro avg  weighted avg  \n",
      "f1-score        0.7    0.584112      0.693306  \n",
      "precision       0.7    0.626473      0.697515  \n",
      "recall          0.7    0.563039      0.700000  \n",
      "support         0.7  120.000000    120.000000  \n"
     ]
    }
   ],
   "source": [
    "################################ Lab-Finetuning-phase ################################\n",
    "samplings = [1,5,10,'weight','undersampling','oversampling',]\n",
    "#samplings = ['undersampling','oversampling']\n",
    "\n",
    "inital = {'lab':True,'field':True}\n",
    "for sampling in samplings:\n",
    "    \n",
    "    print('\\n\\nsampling: ',sampling,'\\n')\n",
    "    \n",
    "    ### Sampling data ###\n",
    "    X_train, X_test, y_train, y_test = select_train_test_dataset(X1_train, X1_test, X2_train, X2_test, y_train_, y_test_, joint)\n",
    "    X_train, X_test, y_train, y_test, lb = filtering_activities_and_label_encoding(X_train, X_test, y_train, y_test, activities)\n",
    "    lab_finetune_loader, lab_validatn_loader, class_weight = combine1(X_train, X_test, y_train, y_test, \n",
    "                                                                      sampling, lb, batch_size, num_workers, \n",
    "                                                                      y_sampling=y_sampling)\n",
    "    print(\"class: \",lb.classes_)\n",
    "    print(\"class_size: \",1-class_weight)\n",
    "    \n",
    "    ### model \n",
    "    encoder, outsize = load_encoder(network, encoder_fp)\n",
    "    model = add_classifier(encoder,in_size=outsize,out_size=len(lb.classes_),freeze=freeze)\n",
    "    \n",
    "    # initialization\n",
    "    phase = 'lab-initial'\n",
    "    if inital['lab']:\n",
    "        cmtx,cls = evaluation(model,lab_finetune_loader,label_encoder=lb)\n",
    "        record_log(record_outpath,exp_name,phase,cmtx=cmtx,cls=cls)\n",
    "        inital['lab'] = False\n",
    "    \n",
    "    # finetuning \n",
    "    phase = 'lab-finetune'+'-'+str(sampling)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weight).to(device)\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=0.0005)\n",
    "    model, record = train(model=model,\n",
    "                          train_loader= lab_finetune_loader,\n",
    "                          criterion=criterion,\n",
    "                          optimizer=optimizer,\n",
    "                          end= lab_finetune_epochs,\n",
    "                          test_loader = lab_validatn_loader,\n",
    "                          device = device,\n",
    "                          regularize = regularize)\n",
    "    \n",
    "    # record and save\n",
    "    cmtx,cls = evaluation(model,lab_validatn_loader,label_encoder=lb)\n",
    "    record_log(record_outpath,exp_name,phase,record=record,cmtx=cmtx,cls=cls,acc_rec=True)\n",
    "    \n",
    "    if sampling != 'weight':\n",
    "\n",
    "        del encoder,model,criterion,optimizer# ,record,cmtx,cls\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    elif sampling == 'weight':\n",
    "        \n",
    "        model_fp = save_model(model_outpath,exp_name,phase,model)\n",
    "        del encoder,model,criterion,optimizer# ,record,cmtx,cls\n",
    "        torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simclr\n",
    "\n",
    "       'lay'      'pickup'   'sit'      'stand'     'standff'  'walk'    'waving'  \n",
    "\n",
    "'full'  11         33         12         19                    99\n",
    "'full'  12         34         11         19                    98         57\n",
    "'full'  9          32         13         19          5         99\n",
    "'full'  7          27         13         17          6         95         57    \n",
    "    \n",
    "#vgg16             #joint\n",
    "'1pcs'  0.48       0.485981   0.153846   0.478873               0.805825\n",
    "'5pcs'  0.062500   0.536082   0.410256   0.307692               0.931217\n",
    "'10pcs' 0.612245   0.349206   0.354430   0.562500               0.953368\n",
    "'full'  0.687500   0.666667   0.428571   0.622951               0.99\n",
    "\n",
    "'1pcs'  0.512821   0.366197   0.400000   0.482759               0.563536   0.493151\n",
    "'5pcs'  0.5        0.358974   0.400000   0.422535               0.666667   0.496000\n",
    "'10pcs' 0.727273   0.623377   0.405797   0.448276               0.649485   0.521739 \n",
    "'full'  0.727273   0.673267   0.407407   0.633333               0.942308   0.919355\n",
    "\n",
    "'1pcs'  0.219178   0.041667   0.186047   0.506329    0.12500    0.613757\n",
    "'5pcs'  0.461538   0.578947   0.318841   0.375000    0.342857   0.954315 \n",
    "'10pcs' 0.387097   0.488372   0.493506   0.520000    0.461538   0.964467 \n",
    "'full'  0.514286   0.666667   0.448276   0.622951    0.370370   0.975369\n",
    "\n",
    "'1pcs'  0.214286   0.378947   0.2500     0.347826    0.272727   0.283688   0.457447\n",
    "'5pcs'  0.228571   0.595745   0.2500     0.509091    0.448980   0.520833   0.359712\n",
    "'10pcs' 0.5        0.539326   0.412698   0.400000    0.424242   0.682927   0.512397\n",
    "'full'  0.437500   0.586957   0.426230   0.557377    0.400000   0.917874   0.883721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simclr\n",
    "\n",
    "        'lay'      'pickup'   'sit'      'stand'     'standff'  'walk'    'waving' \n",
    "    \n",
    "#vgg16             #first'    #L\n",
    "'1pcs'  0.0        0.638297   0.3783783  0.222222    0.142857   0.328767   0.479166 \n",
    "'5pcs'  0.526315   0.627450   0.5        0.476190    0.133333   0.66       0.441176\n",
    "'10pcs' 0.777777   0.651162   0.5714285  0.5         0.526315   0.64       0.578947\n",
    "'full'  0.666667   0.708333   0.6875     0.592593    0.363636   0.893204   0.835821\n",
    "'under' 0.700000   0.714286   0.533333   0.482759    0.421053   0.755102   0.617647\n",
    "'over'  0.736842   0.681818   0.647059   0.642857    0.461538   0.88       0.823529\n",
    "                                \n",
    "                              #M\n",
    "'full'  0.571429   0.653061   0.666667   0.538462    0.631579   0.921569  0.878788\n",
    "'under' 0.555556   0.600000   0.578947   0.500000    0.583333   0.835165  0.760563\n",
    "'over'  0.625000   0.723404   0.600000   0.533333    0.823529   0.875000  0.828571\n",
    "\n",
    "                              #H\n",
    "'full'  0.428571   0.680851   0.647059   0.592593    0.588235   0.847826  0.826667 \n",
    "'under' 0.526316   0.590909   0.571429   0.592593    0.608696   0.727273 0.675325\n",
    "'over'  0.705882   0.638298   0.600000   0.571429    0.631579   0.891304  0.876712\n",
    "\n",
    "                   #joint\n",
    "'1pcs'  0.214286   0.378947   0.2500     0.347826    0.272727   0.283688   0.457447\n",
    "'5pcs'  0.228571   0.595745   0.2500     0.509091    0.448980   0.520833   0.359712\n",
    "'10pcs' 0.5        0.539326   0.412698   0.400000    0.424242   0.682927   0.512397\n",
    "'full'  0.437500   0.586957   0.426230   0.557377    0.400000   0.917874   0.883721\n",
    "\n",
    "                   #pwr\n",
    "'full'  0.0        0.775510   0.588235   0.666667    0.0        0.944000   0.833333\n",
    "'under' 0.4        0.618182   0.705882   0.583333    0.25       0.520833   0.368421\n",
    "'over'  0.285714   0.760000   0.500000   0.666667    0.40       0.918033   0.806452\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "#shallow           #joint\n",
    "'1pcs'  0.384615   0.037736   0.356164   0.046512    0.0        0.555556   0.268657\n",
    "'5pcs'  0.146341   0.169492   0.156863   0.484211    0.266667   0.357616   0.470588\n",
    "'10pcs' 0.408163   0.391753   0.222222   0.338983    0.186047   0.541935   0.468750\n",
    "'full'  0.333333   0.444444   0.356164   0.406780    0.370370   0.94       0.892086\n",
    "\n",
    "                   #first\n",
    "'full'  0.500000   0.5        0.4375     0.580645    0.615385   0.923077   0.909091  \n",
    "'under' 0.384615   0.514286   0.685714   0.787879    0.352941   0.800000   0.769231\n",
    "'over'  0.400000   0.465116   0.470588   0.580645    0.571429   0.812500   0.794521\n",
    "\n",
    "\n",
    "#alexnet\n",
    "'1pcs'  0.0        0.068182   0.170213   0.421053    0.0        0.375940   0.551351\n",
    "'5pcs'  0.274510   0.515464   0.393939   0.285714    0.166667   0.590909   0.525547\n",
    "'10pcs' 0.615385   0.530120   0.41791    0.285714    0.476190   0.604651   0.575163 \n",
    "'full'  0.592593   0.606061   0.433333   0.483871    0.5        0.920792   0.876923\n",
    " \n",
    "                   #first\n",
    "'full'  0.875000   0.681818   0.611111   0.571429    0.75       0.907216  0.898551\n",
    "'under' 0.666667   0.615385   0.666667   0.689655    0.695652   0.775510  0.603175\n",
    "'over'  0.823529   0.604651   0.457143   0.571429    0.777778   0.888889  0.848485\n",
    "\n",
    "# Resnet           #first\n",
    "\n",
    "'full'  0.400000   0.666667   0.611111   0.482759    0.428571   0.843137  0.823529\n",
    "'under' 0.400000   0.450000   0.611111   0.444444    0.315789   0.747253  0.576271\n",
    "'over'  0.526316   0.487805   0.550000   0.466667    0.307692   0.860215  0.828571"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal\n",
    "        'lay'      'pickup'   'sit'      'stand'     'standff'  'walk'    'waving'  \n",
    "#vgg16              #joint\n",
    "'1pcs'  0.074074   0.192771   0.385542   0.256410    0.0        0.133333   0.055556\n",
    "'5pcs'  0.378378   0.325581   0.204082   0.425000    0.210526   0.548571   0.359712\n",
    "'10pcs' 0.0        0.385965   0.372093   0.235294    0.204082   0.205128   0.500000\n",
    "'full'  0.421053   0.494382   0.327273   0.538462    0.648649   0.919431   0.876923\n",
    "\n",
    "                    #first\n",
    "'full'  0.714286   0.415094   0.258065   0.592593    0.363636   0.884615   0.818182  \n",
    "'under' 0.500000   0.550000   0.333333   0.400000    0.444444   0.634146   0.588235\n",
    "'over'  0.571429   0.428571   0.514286   0.666667    0.200      0.747475   0.684211\n",
    "\n",
    "#shallow           # joint\n",
    "'1pcs'  0.0        0.275862   0.093023   0.333333    0.133333   0.264706   0.335025\n",
    "'5pcs'  0.060606   0.368421   0.354839   0.307692    0.266667   0.560847   0.382166\n",
    "'10pcs' 0.500000   0.346667   0.333333   0.489362    0.466667   0.663317   0.484375\n",
    "'full'  0.518519   0.574468   0.459016   0.606061    0.444444   0.941176   0.932331\n",
    "\n",
    "                    #first\n",
    "'full'  0.571429   0.448980   0.424242   0.500000    0.500      0.938776   0.916667\n",
    "'under' 0.631579   0.342857   0.545455   0.578947    0.500000   0.786517   0.666667\n",
    "'over'  0.461538   0.372093   0.526316   0.580645    0.400000   0.969072   0.927536\n",
    "\n",
    "#alexnet\n",
    "'1pcs'  0.0        0.038462   0.311475   0.0         0.160000   0.472527   0.301370  \n",
    "'5pcs'  0.238806   0.338028   0.323529   0.195122    0.162162   0.380952   0.444444\n",
    "'10pcs' 0.430769   0.296296   0.301887   0.507937    0.338983   0.589744   0.488889\n",
    "'full'  0.645161   0.529412   0.327273   0.428571    0.514286   0.915423   0.878788\n",
    " \n",
    "        #first'\n",
    "'full'  0.500000   0.553191   0.388889   0.500000    0.428571   0.893617   0.788732\n",
    "'under' 0.470588   0.476190   0.411765   0.482759    0.526316   0.704545   0.649351\n",
    "'over'  0.714286   0.553191   0.4375     0.500000    0.714286   0.909091   0.777778\n",
    "\n",
    "# Resnet           #first\n",
    "\n",
    "'full'  0.400000   0.692308   0.500000   0.516129    0.571429   0.979592   0.941176\n",
    "'under' 0.222222   0.564103   0.470588   0.461538    0.461538   0.758621   0.698413\n",
    "'over'  0.166667   0.304348   0.375      0.486486    0.421053   0.698795   0.707692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall \n",
    "\n",
    "\n",
    "'lay'      'pickup'   'sit'      'stand'     'standff'  'walk'    'waving'\n",
    "0.666667   0.708333   0.6875     0.592593    0.363636   0.893204   0.835821  'simclr'  'first'  #vgg16\n",
    "0.714286   0.415094   0.258065   0.592593    0.363636   0.884615   0.818182  'normal'    \n",
    "0.421053   0.494382   0.327273   0.538462    0.648649   0.919431   0.876923            'joint'\n",
    "0.500000   0.619048   0.600000   0.533333    0.400000   0.769231   0.724638            'l2reg'  \n",
    "\n",
    "\n",
    "# comparison between models: larger model not always the best, clr \n",
    "\n",
    "'lay'      'pickup'   'sit'      'stand'     'standff'  'walk'    'waving'\n",
    "0.571428   0.448979   0.424242   0.5         0.5        0.938775   0.916666  'shallow'  'normal'\n",
    "0.500000   0.5        0.4375     0.580645    0.615385   0.923077   0.909091  'shallow'  'simclr' \n",
    "\n",
    "0.5        0.553191   0.388888   0.5         0.428571   0.893617   0.788732  'alexnet'  'normal'\n",
    "0.875000   0.681818   0.611111   0.571429    0.75       0.907216   0.898551  'alexnet'  'simclr'\n",
    "\n",
    "0.714286   0.415094   0.258065   0.592593    0.363636   0.884615   0.818182  'vgg16'    'normal'\n",
    "0.666667   0.708333   0.6875     0.592593    0.363636   0.893204   0.835821  'vgg16'    'simclr'\n",
    "\n",
    "0.400000   0.692308   0.500000   0.516129    0.571429   0.979592   0.941176  'resnet'   'normal'\n",
    "0.400000   0.666667   0.611111   0.482759    0.428571   0.843137   0.823529  'resnet'   'simclr'\n",
    "\n",
    "\n",
    "# effect on sample: simclr significantly improve sample efficiency \n",
    "\n",
    "'lay'      'pickup'   'sit'      'stand'     'standff'  'walk'    'waving'\n",
    "0.210526   0.133333   0.2285714  0.266666    0.0        0.594059   0.148148  '1pcs'  'normal'   #vgg16\n",
    "0.0,       0.638297   0.3783783  0.222222    0.142857   0.328767   0.479166  '1pcs'  'simclr'   \n",
    "\n",
    "0.125      0.384615   0.3999999  0.32        0.230769   0.536082   0.369230  '5pcs'  'normal'\n",
    "0.526315   0.627450   0.5        0.476190    0.133333   0.66       0.441176  '5pcs'  'simclr'\n",
    "\n",
    "0.285714   0.333333   0.4705882  0.538461    0.434782   0.447368   0.430379  '10pcs' 'normal'\n",
    "0.777777   0.651162   0.5714285  0.5         0.526315   0.64       0.578947  '10pcs' 'simclr'\n",
    "\n",
    "0.714286   0.415094   0.258065   0.592593    0.363636   0.884615   0.818182  'full'  'normal'\n",
    "0.666667   0.708333   0.6875     0.592593    0.363636   0.893204   0.835821  'full'  'simclr'\n",
    "\n",
    "0.500000   0.550000   0.333333   0.400000    0.444444   0.634146   0.588235  'under' 'normal'\n",
    "0.700000   0.714286   0.533333   0.482759    0.421053   0.755102   0.617647  'under' 'simclr'\n",
    "\n",
    "0.571429   0.428571   0.514286   0.666667    0.200      0.747475   0.684211  'over'  'normal'\n",
    "0.736842   0.681818   0.647059   0.642857    0.461538   0.88       0.823529  'over'  'simclr'\n",
    "\n",
    "\n",
    "# effect on modality/view: seperate multiview is more beneficial to the training \n",
    "'lay'      'pickup'   'sit'      'stand'     'standff'  'walk'    'waving'\n",
    "0.666667   0.708333   0.6875     0.592593    0.363636   0.893204   0.835821  'first'\n",
    "0.437500   0.586957   0.426230   0.557377    0.400000   0.917874   0.883721  'joint'\n",
    "0.0        0.775510   0.588235   0.666667    0.0        0.944000   0.833333  'pwr'\n",
    "\n",
    "\n",
    "# effect on temperature: seperate multiview is more beneficial to the training \n",
    "'lay'      'pickup'   'sit'      'stand'     'standff'  'walk'    'waving'\n",
    "0.666667   0.708333   0.6875     0.592593    0.363636   0.893204   0.835821   'L'\n",
    "0.571429   0.653061   0.666667   0.538462    0.631579   0.921569   0.878788   'M'\n",
    "0.428571   0.680851   0.647059   0.592593    0.588235   0.847826   0.826667   'H'\n",
    "\n",
    "\n",
    "# activity \n",
    "'lay'      'pickup'   'sit'      'stand'     'standff'  'walk'    'waving'  \n",
    "\n",
    "0.687500   0.666667   0.428571   0.622951               0.99                 #'vgg16' 'simclr'\n",
    "0.727273   0.673267   0.407407   0.633333               0.942308   0.919355\n",
    "0.514286   0.666667   0.448276   0.622951    0.370370   0.975369\n",
    "0.437500   0.586957   0.426230   0.557377    0.400000   0.917874   0.883721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################ Field-Finetuning-phase ################################\n",
    "    \n",
    "        for field_sampling in [1,5,10]:\n",
    "\n",
    "            Xf,yf = import_data(fp2)\n",
    "            field_finetune_loader, field_validatn_loader, lb = process_field_data(Xf,yf,num=field_sampling,lb=lb)\n",
    "            \n",
    "            \n",
    "            model = load_model(network, freeze, lb, model_fp)\n",
    "            \n",
    "            # initialization\n",
    "            phase = 'field-initial'\n",
    "            if inital['field'] == True:\n",
    "                cmtx,cls = evaluation(model,test_loader,label_encoder=lb)\n",
    "                record_log(record_outpath,exp_name,phase,cmtx=cmtx,cls=cls)\n",
    "                inital['field'] = False\n",
    "                \n",
    "            # finetuning \n",
    "            phase = 'field-finetune'\n",
    "            criterion = nn.CrossEntropyLoss(weight=class_weight).to(device)\n",
    "            optimizer = torch.optim.Adam(list(model.parameters()), lr=0.0005)\n",
    "            model, record = train(model=model,\n",
    "                                  train_loader= field_finetune_loader,\n",
    "                                  criterion=criterion,\n",
    "                                  optimizer=optimizer,\n",
    "                                  end= field_finetune_epochs,\n",
    "                                  test_loader = field_validatn_loader,\n",
    "                                  device = device,\n",
    "                                  regularize = regularize)\n",
    "            # record and save\n",
    "            evaluation(model,field_validatn_loader,label_encoder=lb)\n",
    "            record_log(record_outpath,exp_name,phase,record=record,cmtx=cmtx,cls=cls,acc_rec=True)\n",
    "            del encoder,model,record,cmtx,cls,criterion,optimizer\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
