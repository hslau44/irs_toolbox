{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for heatmaps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.spectrogram import import_data, import_pair_data\n",
    "from data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.baseline import *\n",
    "from models.cnn import *\n",
    "from models.self_supervised import *\n",
    "from models.utils import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import NT_Xent\n",
    "from contrastive_learning import pretrain\n",
    "from train import record_log,evaluation,train,load_checkpoint,save_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINMODE = 'simclr' # option: 'simclr','normal'           \n",
    "NETWORK =  'resnet'  # option: 'shallow','alexnet','vgg16','resnet'      \n",
    "PAIRING =  'nuc2'    # option: 'pwr','nuc2' \n",
    "TEMPERATURE = 'L'    # option: 'L', 'M', 'H'\n",
    "\n",
    "REGULARIZE = None    # option: True,None\n",
    "JOINT = 'first'      # option: 'first','second','joint'\n",
    "\n",
    "PRETRAIN_SKIP = ['waving'] # list of name of activities to be skiped in pretraining\n",
    "FINETUNE_SKIP = ['waving'] # list of name of activities to be skiped in finetuning/training\n",
    "\n",
    "EXTRA = None   # add extra string at the end of file for notation\n",
    "VAL = 'random'   # validation method; option: 'id', 'random'\n",
    "\n",
    "PRE_BATCH_SIZE = 64          # batch size for simclr\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 2\n",
    "PRETRAIN_EPOCHS = 750\n",
    "LAB_FINETUNE_EPOCHS = 200\n",
    "Y_SAMPLING = None            # option: None,'oversampling','undersampling'\n",
    "VAL_ID = 1                   # participant; option: 1,2,3,4,5\n",
    "\n",
    "\n",
    "fp2 = './data/experiment_data/exp_2/spectrogram' \n",
    "fp3 = './data/experiment_data/exp_3/spectrogram' \n",
    "fp4 = './data/experiment_data/exp_4/spectrogram_multi'\n",
    "\n",
    "OUT_PATH = './laboratory/experiment_b'\n",
    "FP = fp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(1024)\n",
    "torch.manual_seed(1024)\n",
    "\n",
    "# gpu setting\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\") # some computer set to \"cuda:0\"\n",
    "device = DEVICE\n",
    "torch.cuda.set_device(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_encoder(network, model_fp):\n",
    "    encoder, outsize = create_encoder(network,'nuc2')\n",
    "    encoder.load_state_dict(torch.load(model_fp))\n",
    "    return encoder, outsize\n",
    "\n",
    "def load_model(network, freeze, lb, model_fp):\n",
    "    encoder, outsize = create_encoder(network,'nuc2')\n",
    "    model = add_classifier(encoder,in_size=outsize,out_size=len(lb.classes_),freeze=freeze)\n",
    "    model.load_state_dict(torch.load(model_fp))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainmode-simclr-L_Network-resnet_Data-exp4csinuc2\n"
     ]
    }
   ],
   "source": [
    "trainmode = TRAINMODE\n",
    "pairing = PAIRING\n",
    "network = NETWORK\n",
    "temperature = TEMPERATURE\n",
    "joint = JOINT\n",
    "pretrain_skip = PRETRAIN_SKIP\n",
    "activities = FINETUNE_SKIP\n",
    "batch_size = BATCH_SIZE\n",
    "num_workers = NUM_WORKERS\n",
    "pretrain_epochs = PRETRAIN_EPOCHS \n",
    "lab_finetune_epochs = LAB_FINETUNE_EPOCHS \n",
    "regularize = REGULARIZE \n",
    "t = TEMPERATURE\n",
    "y_sampling = Y_SAMPLING\n",
    "val_method = VAL\n",
    "validation_id = VAL_ID\n",
    "pre_batch_size = PRE_BATCH_SIZE\n",
    "filepath = FP\n",
    "\n",
    "temperature_dict = {'L':0.1,'M':0.5,'H':1}    \n",
    "temperature = temperature_dict[t]\n",
    "\n",
    "if TRAINMODE == 'pwr':\n",
    "    joint = 'first'\n",
    "\n",
    "if TRAINMODE == 'normal':\n",
    "    if regularize == True:\n",
    "        t = 'r'\n",
    "    else:\n",
    "        t = ''\n",
    "    pairing = 'nuc2'\n",
    "    freeze=False\n",
    "else:\n",
    "    freeze=True\n",
    "\n",
    "if JOINT == 'joint':\n",
    "    j='-j'\n",
    "else:\n",
    "    j=''\n",
    "    \n",
    "if EXTRA == None:\n",
    "    extra = ''\n",
    "else:\n",
    "    extra = '-Extra-'+str(EXTRA)\n",
    "\n",
    "exp_name = f'Trainmode-{trainmode}-{t}_Network-{network}_Data-exp4csi{pairing}{j}{extra}'\n",
    "model_outpath  = OUT_PATH+'/'+'saved_models'\n",
    "record_outpath = OUT_PATH+'/'+'records'\n",
    "print(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Data >>>>>>> Complete\n"
     ]
    }
   ],
   "source": [
    "####################################################### Main #############################################################\n",
    "\n",
    "# importing data from filepath, need to decide the pairing \n",
    "datas = import_pair_data(filepath,modal=['csi',pairing],return_id=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering activities for pretraining (and finetuning)\n",
    "X1,X2,y,id_ls = initial_filtering_activities(datas,activities=pretrain_skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id for loov validation, else randomly spliting data\n",
    "if val_method == 'id':\n",
    "    X1_train, X1_test, X2_train, X2_test, y_train_, y_test_ = split_datasets(X1,X2,y,by='id',id_ls=id_ls,validation_id=validation_id)\n",
    "else:\n",
    "    X1_train, X1_test, X2_train, X2_test, y_train_, y_test_ = split_datasets(X1,X2,y,split=0.8,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1_train:  (477, 1, 65, 501) \tX2_train:  (477, 1, 65, 501)\n",
      "Start Training\n",
      "Epoch 1: >>>>>>> loss: 4.873965740203857 \n",
      "Epoch 2: >>>>>>> loss: 4.776188373565674 \n",
      "Epoch 3: >>>>>>> loss: 4.776790618896484 \n",
      "Epoch 4: >>>>>>> loss: 4.746586799621582 \n",
      "Epoch 5: >>>>>>> loss: 4.709256649017334 \n",
      "Epoch 6: >>>>>>> loss: 4.6465911865234375 \n",
      "Epoch 7: >>>>>>> loss: 4.63023042678833 \n",
      "Epoch 8: >>>>>>> loss: 4.568632125854492 \n",
      "Epoch 9: >>>>>>> loss: 4.54149055480957 \n",
      "Epoch 10: >>>>>>> loss: 4.521923065185547 \n",
      "Epoch 11: >>>>>>> loss: 4.435563087463379 \n",
      "Epoch 12: >>>>>>> loss: 4.402491092681885 \n",
      "Epoch 13: >>>>>>> loss: 4.374080181121826 \n",
      "Epoch 14: >>>>>>> loss: 4.263550281524658 \n",
      "Epoch 15: >>>>>>> loss: 4.312619209289551 \n",
      "Epoch 16: >>>>>>> loss: 4.114223957061768 \n",
      "Epoch 17: >>>>>>> loss: 4.1044135093688965 \n",
      "Epoch 18: >>>>>>> loss: 4.12396240234375 \n",
      "Epoch 19: >>>>>>> loss: 4.100536346435547 \n",
      "Epoch 20: >>>>>>> loss: 4.057999134063721 \n",
      "Epoch 21: >>>>>>> loss: 3.9668023586273193 \n",
      "Epoch 22: >>>>>>> loss: 3.9496145248413086 \n",
      "Epoch 23: >>>>>>> loss: 3.9989218711853027 \n",
      "Epoch 24: >>>>>>> loss: 3.8581857681274414 \n",
      "Epoch 25: >>>>>>> loss: 3.9927854537963867 \n",
      "Epoch 26: >>>>>>> loss: 3.967367649078369 \n",
      "Epoch 27: >>>>>>> loss: 4.0600266456604 \n",
      "Epoch 28: >>>>>>> loss: 3.7198703289031982 \n",
      "Epoch 29: >>>>>>> loss: 3.908046007156372 \n",
      "Epoch 30: >>>>>>> loss: 3.736417770385742 \n",
      "Epoch 31: >>>>>>> loss: 3.5245866775512695 \n",
      "Epoch 32: >>>>>>> loss: 3.8329453468322754 \n",
      "Epoch 33: >>>>>>> loss: 4.563309192657471 \n",
      "Epoch 34: >>>>>>> loss: 3.339230537414551 \n",
      "Epoch 35: >>>>>>> loss: 3.948408603668213 \n",
      "Epoch 36: >>>>>>> loss: 3.3303329944610596 \n",
      "Epoch 37: >>>>>>> loss: 3.608299970626831 \n",
      "Epoch 38: >>>>>>> loss: 3.407862901687622 \n",
      "Epoch 39: >>>>>>> loss: 3.271136522293091 \n",
      "Epoch 40: >>>>>>> loss: 4.441435813903809 \n",
      "Epoch 41: >>>>>>> loss: 3.193636894226074 \n",
      "Epoch 42: >>>>>>> loss: 4.263623237609863 \n",
      "Epoch 43: >>>>>>> loss: 3.084412097930908 \n",
      "Epoch 44: >>>>>>> loss: 4.233483791351318 \n",
      "Epoch 45: >>>>>>> loss: 2.883535385131836 \n",
      "Epoch 46: >>>>>>> loss: 5.644631862640381 \n",
      "Epoch 47: >>>>>>> loss: 2.6730685234069824 \n",
      "Epoch 48: >>>>>>> loss: 4.456677436828613 \n",
      "Epoch 49: >>>>>>> loss: 2.7136077880859375 \n",
      "Epoch 50: >>>>>>> loss: 5.149930953979492 \n",
      "Epoch 51: >>>>>>> loss: 2.456879138946533 \n",
      "Epoch 52: >>>>>>> loss: 5.347750186920166 \n",
      "Epoch 53: >>>>>>> loss: 2.4935078620910645 \n",
      "Epoch 54: >>>>>>> loss: 5.811324596405029 \n",
      "Epoch 55: >>>>>>> loss: 2.2666289806365967 \n",
      "Epoch 56: >>>>>>> loss: 4.48969841003418 \n",
      "Epoch 57: >>>>>>> loss: 2.274348735809326 \n",
      "Epoch 58: >>>>>>> loss: 4.190578460693359 \n",
      "Epoch 59: >>>>>>> loss: 2.2802789211273193 \n",
      "Epoch 60: >>>>>>> loss: 2.2920987606048584 \n",
      "Epoch 61: >>>>>>> loss: 3.230334758758545 \n",
      "Epoch 62: >>>>>>> loss: 2.0705811977386475 \n",
      "Epoch 63: >>>>>>> loss: 3.719447612762451 \n",
      "Epoch 64: >>>>>>> loss: 2.3157010078430176 \n",
      "Epoch 65: >>>>>>> loss: 6.130021572113037 \n",
      "Epoch 66: >>>>>>> loss: 2.152453899383545 \n",
      "Epoch 67: >>>>>>> loss: 3.021618127822876 \n",
      "Epoch 68: >>>>>>> loss: 2.2851152420043945 \n",
      "Epoch 69: >>>>>>> loss: 1.6412522792816162 \n",
      "Epoch 70: >>>>>>> loss: 5.3387322425842285 \n",
      "Epoch 71: >>>>>>> loss: 1.9374043941497803 \n",
      "Epoch 72: >>>>>>> loss: 1.5534712076187134 \n",
      "Epoch 73: >>>>>>> loss: 4.768887996673584 \n",
      "Epoch 74: >>>>>>> loss: 2.4307119846343994 \n",
      "Epoch 75: >>>>>>> loss: 1.2919718027114868 \n",
      "Epoch 76: >>>>>>> loss: 1.4378129243850708 \n",
      "Epoch 77: >>>>>>> loss: 5.494770050048828 \n",
      "Epoch 78: >>>>>>> loss: 1.5260372161865234 \n",
      "Epoch 79: >>>>>>> loss: 1.4038641452789307 \n",
      "Epoch 80: >>>>>>> loss: 1.4733293056488037 \n",
      "Epoch 81: >>>>>>> loss: 6.024144172668457 \n",
      "Epoch 82: >>>>>>> loss: 1.1907086372375488 \n",
      "Epoch 83: >>>>>>> loss: 0.9537434577941895 \n",
      "Epoch 84: >>>>>>> loss: 0.9351985454559326 \n",
      "Epoch 85: >>>>>>> loss: 0.92304927110672 \n",
      "Epoch 86: >>>>>>> loss: 1.2429461479187012 \n",
      "Epoch 87: >>>>>>> loss: 8.305231094360352 \n",
      "Epoch 88: >>>>>>> loss: 1.482470989227295 \n",
      "Epoch 89: >>>>>>> loss: 0.8876956701278687 \n",
      "Epoch 90: >>>>>>>"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a9aa7d086dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m                               \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                               \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrain_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                               device=device)\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mrecord_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_outpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/irs_toolbox/contrastive_learning.py\u001b[0m in \u001b[0;36mpretrain\u001b[0;34m(model, train_loader, criterion, optimizer, end, start, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Project/irs_toolbox/losses/__init__.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z_i, z_j)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         )\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mnegative_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################################ Lab-Pretrain-phase ################################\n",
    "\n",
    "# create dataloader for pretraining  \n",
    "pretrain_loader = create_dataloader(X1_train,X2_train,batch_size=pre_batch_size,num_workers=num_workers)\n",
    "print('X1_train: ',X1_train.shape,'\\tX2_train: ',X2_train.shape)\n",
    "\n",
    "# create enocder  \n",
    "encoder, outsize = create_encoder(network,'nuc2')\n",
    "encoder2, outsize2 = None,None\n",
    "\n",
    "# the pairing data use the same encoder if 'joint', else we use two encoders\n",
    "if joint == 'joint':\n",
    "    simclr = add_SimCLR(encoder, outsize)\n",
    "else:\n",
    "    encoder2, outsize2 = create_encoder(network,pairing)\n",
    "    simclr = add_SimCLR_multi(enc1=encoder,enc2=encoder2,out_size1=outsize,out_size2=outsize2)\n",
    "    \n",
    "# (optional) pretraining with simclr  \n",
    "phase = 'pretrain'\n",
    "if trainmode == 'simclr':\n",
    "    criterion = NT_Xent(pre_batch_size, temperature, world_size=1)\n",
    "    optimizer = torch.optim.SGD(list(simclr.parameters()), lr=0.0005)\n",
    "    simclr, record = pretrain(model=simclr,\n",
    "                              train_loader=pretrain_loader,\n",
    "                              criterion=criterion,\n",
    "                              optimizer=optimizer,\n",
    "                              end=pretrain_epochs,\n",
    "                              device=device)\n",
    "    record_log(record_outpath,exp_name,phase,record=record)\n",
    "    \n",
    "# save\n",
    "encoder_fp = save_model(model_outpath,exp_name,phase,simclr.encoder)\n",
    "del simclr,encoder,outsize, encoder2,outsize2\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "sampling:  1 \n",
      "\n",
      "X_train:  (6, 1, 65, 501) \ty_train:  (6,) \tX_test:  (120, 1, 65, 501) \ty_test:  (120,)\n",
      "class:  ['lay' 'pickup' 'sit' 'stand' 'standff' 'walk']\n",
      "class_size:  tensor([0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'encoder_fp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-d12cbf3c368c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# load the encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_fp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;31m# add a classifier on top of the encoder to form the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder_fp' is not defined"
     ]
    }
   ],
   "source": [
    "################################ Lab-Finetuning-phase ################################\n",
    "\n",
    "# sampling condition\n",
    "samplings = [1,5,10,'weight','undersampling','oversampling',]\n",
    "\n",
    "\n",
    "\n",
    "inital = {'lab':True} # for initial record\n",
    "for sampling in samplings:\n",
    "    \n",
    "    print('\\n\\nsampling: ',sampling,'\\n')\n",
    "    \n",
    "    ### Sampling data ###\n",
    "    \n",
    "    # decided whether the dataset should be joined together\n",
    "    X_train, X_test, y_train, y_test = select_train_test_dataset(X1_train, X1_test, X2_train, X2_test, y_train_, y_test_, joint)\n",
    "    # filtering activities and label-encode the remain activities\n",
    "    X_train, X_test, y_train, y_test, lb = filtering_activities_and_label_encoding(X_train, X_test, y_train, y_test, activities)\n",
    "    # create dataloader class \n",
    "    lab_finetune_loader, lab_validatn_loader, class_weight = combine1(X_train, X_test, y_train, y_test, \n",
    "                                                                      sampling, lb, batch_size, num_workers, \n",
    "                                                                      y_sampling=y_sampling)\n",
    "    print(\"class: \",lb.classes_)\n",
    "    print(\"class_size: \",1-class_weight)\n",
    "    \n",
    "\n",
    "    # load the encoder\n",
    "    encoder, outsize = load_encoder(network, encoder_fp)\n",
    "    # add a classifier on top of the encoder to form the model\n",
    "    model = add_classifier(encoder,in_size=outsize,out_size=len(lb.classes_),freeze=freeze)\n",
    "    # (optional) simpleCNN from previous paper; model = SimpleCNN('1st',num_classes=7)\n",
    "    \n",
    "    \n",
    "    # evaluate the untrained network  \n",
    "    phase = 'lab-initial'\n",
    "    if inital['lab']:\n",
    "        cmtx,cls = evaluation(model,lab_finetune_loader,label_encoder=lb)\n",
    "        record_log(record_outpath,exp_name,phase,cmtx=cmtx,cls=cls)\n",
    "        inital['lab'] = False\n",
    "    \n",
    "    # finetuning \n",
    "    phase = 'lab-finetune'+'-'+str(sampling)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weight).to(device)\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=0.0005)\n",
    "    model, record = train(model=model,\n",
    "                          train_loader= lab_finetune_loader,\n",
    "                          criterion=criterion,\n",
    "                          optimizer=optimizer,\n",
    "                          end= lab_finetune_epochs,\n",
    "                          test_loader = lab_validatn_loader,\n",
    "                          device = device,\n",
    "                          regularize = regularize)\n",
    "    \n",
    "    # test and record\n",
    "    cmtx,cls = evaluation(model,lab_validatn_loader,label_encoder=lb)\n",
    "    record_log(record_outpath,exp_name,phase,record=record,cmtx=cmtx,cls=cls,acc_rec=True)\n",
    "    \n",
    "    \n",
    "    # (obtional) extrafinetuning: defreeze the encoder \n",
    "    if sampling == 'extrafine':\n",
    "        phase = 'lab-extrafinetune'+'-'+str(sampling)\n",
    "        model = unfreeze_network(model)\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()), lr=0.0001)\n",
    "        model, record = train(model=model,\n",
    "                              train_loader= lab_finetune_loader,\n",
    "                              criterion=criterion,\n",
    "                              optimizer=optimizer,\n",
    "                              end= lab_finetune_epochs,\n",
    "                              test_loader = lab_validatn_loader,\n",
    "                              device = device,\n",
    "                              regularize = regularize)\n",
    "        \n",
    "        cmtx,cls = evaluation(model,lab_validatn_loader,label_encoder=lb)\n",
    "        record_log(record_outpath,exp_name,phase,record=record,cmtx=cmtx,cls=cls,acc_rec=True)\n",
    "        \n",
    "    \n",
    "    # every loop the model (including encoder),criterion,optimizer are to be deleted \n",
    "    if sampling != 'weight':\n",
    "\n",
    "        del encoder,model,criterion,optimizer# ,record,cmtx,cls\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    elif sampling == 'weight':\n",
    "        \n",
    "        model_fp = save_model(model_outpath,exp_name,phase,model)\n",
    "        del encoder,model,criterion,optimizer# ,record,cmtx,cls\n",
    "        torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
