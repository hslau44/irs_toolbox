{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn  # for heatmaps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import loadmat\n",
    "\n",
    "# link = r\"D:\\external_data\\Experiment4\"\n",
    "# filename = r\"\\Dataset_PWR_WiFi.mat\"\n",
    "# directory = link + filename\n",
    "\n",
    "# mat = loadmat(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import experimental dataset 2\n",
    "# folderpath2 = \"D:/external_data/Experiment3/csv_files/exp_2\"  # CHANGE THIS IF THE PATH CHANGED\n",
    "# df_exp2 = import_clean_data('exp2',folderpath2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evalaute(model, test_loader):\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         for X_test, y_test in test_loader:\n",
    "            \n",
    "#             y_val = model(X_test)\n",
    "#             predicted = torch.max(y_val,1)[1] \n",
    "    \n",
    "#     arr = confusion_matrix(y_test.view(-1), predicted.view(-1))\n",
    "#     return arr\n",
    "\n",
    "# arr = evalaute(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.import_data import import_experimental_data\n",
    "from data.process_data import DatasetObject\n",
    "from models.train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folderpath1 = \"D:/external_data/Experiment3/csv_files/exp_1\"  # CHANGE THIS IF THE PATH CHANGED\n",
    "df_exp1 = import_experimental_data(folderpath1) # import_clean_data('exp1',\n",
    "df_exp1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process data\n",
    "X_ls, y_ls = seperate_dataframes(df_exp1)\n",
    "del df_exp1\n",
    "# DatasetObject\n",
    "exp_1 = create_datasetobject(X_ls, y_ls)\n",
    "del X_ls, y_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data\n",
    "X,y,_ = exp_1.__getitem__([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_M = (18,10)\n",
    "FIG_S = (10,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "section = 0\n",
    "u = 'user2'\n",
    "\n",
    "img = df_exp1[(df_exp1['user']==u)].iloc[section*6999:(section+1)*6999,:]\n",
    "print(img['label'].unique())\n",
    "img = img.iloc[:,0:90].values\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "plt.figure(figsize=FIG_S)\n",
    "plt.plot(img[:,::30])\n",
    "plt.show()\n",
    "plt.figure(figsize=FIG_S)\n",
    "plt.plot(img[:,:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = img[:,30]\n",
    "n = len(line)\n",
    "f_hat = np.fft.fft(line,n)\n",
    "f_hat\n",
    "psd = f_hat*np.conj(f_hat)/n\n",
    "psd\n",
    "freq = (1/n)*np.arange(n)\n",
    "freq\n",
    "L = np.arange(1,np.floor(n/2),dtype='int')\n",
    "L\n",
    "\n",
    "fig,axs = plt.subplots(2,1)\n",
    "\n",
    "plt.sca(axs[0])\n",
    "plt.plot(line)\n",
    "\n",
    "plt.sca(axs[1])\n",
    "plt.plot(freq[L],psd[L])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise spectrogram\n",
    "img = img.transpose()\n",
    "img = img[:,::10]\n",
    "plt.figure(figsize = (15,50))\n",
    "plt.imshow(img,cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data.process_data import stacking\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def visual_spectrogram(img,title='spectrogram'):\n",
    "    \n",
    "    if len(img.shape) == 3:\n",
    "        \n",
    "        img.squeeze()\n",
    "    \n",
    "    img = MinMaxScaler(feature_range=(0,1)).fit_transform(img)\n",
    "    \n",
    "#     img = img.reshape(*img.shape[:-1],1)\n",
    "    \n",
    "#     img = np.concatenate((img,img,img),axis=2)\n",
    "    \n",
    "    plt.figure(figsize = (15,50))\n",
    "    plt.imshow(img,cmap='jet')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "        \n",
    "#visualize weights for alexnet - first conv layer\n",
    "alexnet = models.alexnet(pretrained=False)\n",
    "\n",
    "# Store all conv layer in alexnet\n",
    "k = []\n",
    "for m in alexnet.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m.weight.data.shape)\n",
    "        k.append(m)\n",
    "    else:  \n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_single_filter(weight):\n",
    "    \n",
    "    img = MinMaxScaler(feature_range=(0,1)).fit_transform(weight)\n",
    "    \n",
    "#     img = img.reshape(*img.shape[:-1],1)\n",
    "    \n",
    "#     img = np.concatenate((img,img,img),axis=2)\n",
    "    \n",
    "#     plt.figure(figsize = (15,10))\n",
    "    plt.imshow(weight,cmap='jet')\n",
    "#     plt.show()\n",
    "    \n",
    "    return \n",
    "\n",
    "def visualise_filters(layer,figsize=(20,15)):\n",
    "    \n",
    "    weights = layer.weight.data\n",
    "\n",
    "    if weights.shape[0] > weights.shape[1]:\n",
    "        \n",
    "        weights = weights.transpose(1,0)\n",
    "        \n",
    "    rows = weights.shape[0]\n",
    "    \n",
    "    cols = weights.shape[1]\n",
    "    \n",
    "    axes = []\n",
    "    \n",
    "    fig= plt.figure()\n",
    "\n",
    "    for n in range(rows):\n",
    "\n",
    "        for c in range(cols):\n",
    "            \n",
    "            axes.append(fig.add_subplot(rows, cols, n*cols+c+1))\n",
    "\n",
    "            single_filter = weights[n,c]\n",
    "        \n",
    "            visual_single_filter(single_filter)\n",
    "            \n",
    "#     fig.tight_layout()    \n",
    "    \n",
    "    plt.show()     \n",
    "    \n",
    "    return \n",
    "\n",
    "visualise_filters(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "# a = time.time()\n",
    "# model  = train(model, train_loader, criterion, optimizer, 48)\n",
    "# print(time.time() - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.CNN import Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_LSTM_OneToOne(nn.Module):\n",
    "    def __init__(self,seq_size):\n",
    "        \"\"\"\n",
    "        CNN-LSTM model\n",
    "\n",
    "        attr:\n",
    "        seq_size: length of the sequence\n",
    "        feature_size: feature size of each interval in the sequence\n",
    "\n",
    "        \"\"\"\n",
    "        super(CNN_LSTM_OneToOne, self).__init__()\n",
    "        self.split = Split_Channel(unsqueeze=False)\n",
    "        self.cnn = CNN_MultiStream()\n",
    "        self.lstm = LSTM_Baseline(seq_size=seq_size, feature_size=3*128)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = self.split(X)\n",
    "        X = [self.cnn(X[i]).unsqueeze(1) for i in range(len(X))]\n",
    "        X = torch.cat(X,1)\n",
    "        X = self.lstm(X)\n",
    "        return X\n",
    "    \n",
    "class Split_Channel(nn.Module):\n",
    "    def __init__(self,unsqueeze=True):\n",
    "        super(Split_Channel, self).__init__()\n",
    "        self.unsqueeze = unsqueeze\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.split_channel(x)\n",
    "\n",
    "    def split_channel(self, x):\n",
    "        if self.unsqueeze == True:\n",
    "            return [x[:,i].unsqueeze(1) for i in range(x.shape[1])]\n",
    "        else:\n",
    "            return [x[:,i] for i in range(x.shape[1])]\n",
    "\n",
    "\n",
    "class CNN_SingleStream(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_SingleStream, self).__init__()\n",
    "        ### 1st ###\n",
    "        self.conv1 = nn.Conv2d(1,32,kernel_size=3,stride=2)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.norm1 = Lambda(lambda x:x)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2),stride=(1,1))\n",
    "        ### 2nd ###\n",
    "        self.conv2 = nn.Conv2d(32,64,kernel_size=3,stride=2)\n",
    "        self.actv2 = nn.ReLU()\n",
    "        self.norm2 = Lambda(lambda x:x)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2),stride=(1,1))\n",
    "        ### 3rd ###\n",
    "        self.conv3 = nn.Conv2d(64,128,kernel_size=3,stride=2)\n",
    "        self.actv3 = nn.ReLU()\n",
    "        self.norm3 = Lambda(lambda x:x)\n",
    "        self.pool3 = Lambda(lambda x:x)\n",
    "        ### Global_pooling ###\n",
    "        self.final = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = self.pool1(self.norm1(self.actv1(self.conv1(X))))\n",
    "        X = self.pool2(self.norm2(self.actv2(self.conv2(X))))\n",
    "        X = self.pool3(self.norm3(self.actv3(self.conv3(X))))\n",
    "        X = self.final(X)\n",
    "        X = torch.flatten(X,1)\n",
    "        return X\n",
    "\n",
    "class CNN_MultiStream(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN_MultiStream, self).__init__()\n",
    "        self.split = Split_Channel()\n",
    "        self.convnet_c1 = CNN_SingleStream()\n",
    "        self.convnet_c2 = CNN_SingleStream()\n",
    "        self.convnet_c3 = CNN_SingleStream()\n",
    "        self.concat = Lambda(lambda a,b,c: torch.cat((a,b,c),-1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        a,b,c = self.split(x)\n",
    "        a = self.convnet_c1(a)\n",
    "        b = self.convnet_c2(b)\n",
    "        c = self.convnet_c2(c)\n",
    "        x = torch.cat((a,b,c),-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1024)\n",
    "torch.manual_seed(1024)\n",
    "torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from data.import_data import import_experimental_data\n",
    "from models.train import * \n",
    "from models.CNN import CNN_module, Encoder, Classifier\n",
    "# from models.self_supervised import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val, 1)[1]\n",
    "    print(classification_report(y_test.view(-1), predicted.view(-1)))\n",
    "    arr = confusion_matrix(y_test.view(-1), predicted.view(-1))\n",
    "    return arr\n",
    "\n",
    "def make_directory(name, epoch=None, filepath='./models/saved_models/'):\n",
    "    time = strftime(\"%Y_%m_%d_%H_%M\", gmtime())\n",
    "    directory = filepath + name + '_checkpoint_' + str(epoch) + '__' + time\n",
    "    return directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, end, start = 1, evaluation = None, auto_save = None, **kwargs):\n",
    "    print('Start Training')\n",
    "    i = start\n",
    "    while i <= end:\n",
    "        print(f\"Epoch {i}: \", end='')\n",
    "        for b, (X_train, y_train) in enumerate(train_loader):\n",
    "            print(f\">\", end='')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_train)\n",
    "            loss   = criterion(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f' loss: {loss.tolist()}')\n",
    "        if i%100 == 0:\n",
    "            if evaluation == True: \n",
    "                array = evaluate(model,test_loader=kwargs['test_loader'])\n",
    "                print('Evaluation:')\n",
    "                print(array)\n",
    "            if auto_save == True:\n",
    "                directory = make_directory(name=kwargs['name'],epoch=i)\n",
    "                save_checkpoint(model,optimizer,i,directory)\n",
    "        i += 1\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 files.\n",
      "input_user10_bendfwd.csv annotation_user10_bendfwd.csv user10\n",
      "input_user10_kneel.csv annotation_user10_kneel.csv user10\n",
      "input_user10_lie.csv annotation_user10_lie.csv user10\n",
      "input_user10_sit.csv annotation_user10_sit.csv user10\n",
      "input_user10_sitrotate.csv annotation_user10_sitrotate.csv user10\n",
      "input_user10_stand.csv annotation_user10_stand.csv user10\n",
      "input_user10_standrotate.csv annotation_user10_standrotate.csv user10\n",
      "input_user10_walking.csv annotation_user10_walking.csv user10\n",
      "input_user1_bendfwd.csv annotation_user1_bendfwd.csv user1\n",
      "input_user1_kneel.csv annotation_user1_kneel.csv user1\n",
      "input_user1_lie.csv annotation_user1_lie.csv user1\n",
      "input_user1_sit.csv annotation_user1_sit.csv user1\n",
      "input_user1_sitrotate.csv annotation_user1_sitrotate.csv user1\n",
      "input_user1_stand.csv annotation_user1_stand.csv user1\n",
      "input_user1_standrotate.csv annotation_user1_standrotate.csv user1\n",
      "input_user1_walking.csv annotation_user1_walking.csv user1\n",
      "input_user2_bendfwd.csv annotation_user2_bendfwd.csv user2\n",
      "input_user2_kneel.csv annotation_user2_kneel.csv user2\n",
      "input_user2_lie.csv annotation_user2_lie.csv user2\n",
      "input_user2_sit.csv annotation_user2_sit.csv user2\n",
      "input_user2_sitrotate.csv annotation_user2_sitrotate.csv user2\n",
      "input_user2_stand.csv annotation_user2_stand.csv user2\n",
      "input_user2_standrotate.csv annotation_user2_standrotate.csv user2\n",
      "input_user2_walking.csv annotation_user2_walking.csv user2\n",
      "input_user3_bendfwd.csv annotation_user3_bendfwd.csv user3\n",
      "input_user3_kneel.csv annotation_user3_kneel.csv user3\n",
      "input_user3_lie.csv annotation_user3_lie.csv user3\n",
      "input_user3_sit.csv annotation_user3_sit.csv user3\n",
      "input_user3_sitrotate.csv annotation_user3_sitrotate.csv user3\n",
      "input_user3_stand.csv annotation_user3_stand.csv user3\n",
      "input_user3_standrotate.csv annotation_user3_standrotate.csv user3\n",
      "input_user3_walking.csv annotation_user3_walking.csv user3\n",
      "input_user4_bendfwd.csv annotation_user4_bendfwd.csv user4\n",
      "input_user4_kneel.csv annotation_user4_kneel.csv user4\n",
      "input_user4_lie.csv annotation_user4_lie.csv user4\n",
      "input_user4_sit.csv annotation_user4_sit.csv user4\n",
      "input_user4_sitrotate.csv annotation_user4_sitrotate.csv user4\n",
      "input_user4_stand.csv annotation_user4_stand.csv user4\n",
      "input_user4_standrotate.csv annotation_user4_standrotate.csv user4\n",
      "input_user4_walking.csv annotation_user4_walking.csv user4\n",
      "input_user5_bendfwd.csv annotation_user5_bendfwd.csv user5\n",
      "input_user5_kneel.csv annotation_user5_kneel.csv user5\n",
      "input_user5_lie.csv annotation_user5_lie.csv user5\n",
      "input_user5_sit.csv annotation_user5_sit.csv user5\n",
      "input_user5_sitrotate.csv annotation_user5_sitrotate.csv user5\n",
      "input_user5_stand.csv annotation_user5_stand.csv user5\n",
      "input_user5_standrotate.csv annotation_user5_standrotate.csv user5\n",
      "input_user5_walking.csv annotation_user5_walking.csv user5\n",
      "input_user6_bendfwd.csv annotation_user6_bendfwd.csv user6\n",
      "input_user6_kneel.csv annotation_user6_kneel.csv user6\n",
      "input_user6_lie.csv annotation_user6_lie.csv user6\n",
      "input_user6_sit.csv annotation_user6_sit.csv user6\n",
      "input_user6_sitrotate.csv annotation_user6_sitrotate.csv user6\n",
      "input_user6_stand.csv annotation_user6_stand.csv user6\n",
      "input_user6_standrotate.csv annotation_user6_standrotate.csv user6\n",
      "input_user6_walking.csv annotation_user6_walking.csv user6\n",
      "input_user7_bendfwd.csv annotation_user7_bendfwd.csv user7\n",
      "input_user7_kneel.csv annotation_user7_kneel.csv user7\n",
      "input_user7_lie.csv annotation_user7_lie.csv user7\n",
      "input_user7_sit.csv annotation_user7_sit.csv user7\n",
      "input_user7_sitrotate.csv annotation_user7_sitrotate.csv user7\n",
      "input_user7_stand.csv annotation_user7_stand.csv user7\n",
      "input_user7_standrotate.csv annotation_user7_standrotate.csv user7\n",
      "input_user7_walking.csv annotation_user7_walking.csv user7\n",
      "input_user8_bendfwd.csv annotation_user8_bendfwd.csv user8\n",
      "input_user8_kneel.csv annotation_user8_kneel.csv user8\n",
      "input_user8_lie.csv annotation_user8_lie.csv user8\n",
      "input_user8_sit.csv annotation_user8_sit.csv user8\n",
      "input_user8_sitrotate.csv annotation_user8_sitrotate.csv user8\n",
      "input_user8_stand.csv annotation_user8_stand.csv user8\n",
      "input_user8_standrotate.csv annotation_user8_standrotate.csv user8\n",
      "input_user8_walking.csv annotation_user8_walking.csv user8\n",
      "input_user9_bendfwd.csv annotation_user9_bendfwd.csv user9\n",
      "input_user9_kneel.csv annotation_user9_kneel.csv user9\n",
      "input_user9_lie.csv annotation_user9_lie.csv user9\n",
      "input_user9_sit.csv annotation_user9_sit.csv user9\n",
      "input_user9_sitrotate.csv annotation_user9_sitrotate.csv user9\n",
      "input_user9_stand.csv annotation_user9_stand.csv user9\n",
      "input_user9_standrotate.csv annotation_user9_standrotate.csv user9\n",
      "input_user9_walking.csv annotation_user9_walking.csv user9\n",
      "index 0 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 1 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 2 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 3 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 4 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 5 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 6 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 7 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 8 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 9 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "size of DatasetObject ------ :  (10, 3)\n",
      "index 0 arrays sizes ------ X:  (192, 900, 90)  Y:  (192,)  Z:  (192, 1)\n",
      "index 1 arrays sizes ------ X:  (208, 900, 90)  Y:  (208,)  Z:  (208, 1)\n",
      "index 2 arrays sizes ------ X:  (216, 900, 90)  Y:  (216,)  Z:  (216, 1)\n",
      "index 3 arrays sizes ------ X:  (184, 900, 90)  Y:  (184,)  Z:  (184, 1)\n",
      "index 4 arrays sizes ------ X:  (192, 900, 90)  Y:  (192,)  Z:  (192, 1)\n",
      "index 5 arrays sizes ------ X:  (176, 900, 90)  Y:  (176,)  Z:  (176, 1)\n",
      "index 6 arrays sizes ------ X:  (192, 900, 90)  Y:  (192,)  Z:  (192, 1)\n",
      "index 7 arrays sizes ------ X:  (176, 900, 90)  Y:  (176,)  Z:  (176, 1)\n",
      "index 8 arrays sizes ------ X:  (184, 900, 90)  Y:  (184,)  Z:  (184, 1)\n",
      "index 9 arrays sizes ------ X:  (200, 900, 90)  Y:  (200,)  Z:  (200, 1)\n",
      "size of DatasetObject ------ :  (10, 3)\n",
      "train set: [1, 2, 3, 4, 5, 6, 7, 8, 9] \ttest set: [0]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import data\n",
    "folderpath1 = \"E:/external_data/Experiment3/csv_files/exp_1\"\n",
    "df_exp1 = import_experimental_data(folderpath1)\n",
    "\n",
    "# 2. Process data\n",
    "X_ls, y_ls = seperate_dataframes(df_exp1)\n",
    "del df_exp1\n",
    "\n",
    "# 3. DatasetObject\n",
    "exp_1 = create_datasetobject(X_ls, y_ls)\n",
    "del X_ls,y_ls\n",
    "\n",
    "# 4. Load data into dataloaders\n",
    "idxs = [0]\n",
    "(X_train, y_train,_),(X_test, y_test,_) = exp_1(idxs,return_train_sets=True)\n",
    "train_loader, test_loader = create_dataloaders(X_train, y_train, X_test, y_test)\n",
    "del X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=list(model.parameters()),lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch 101: >>>>>>>>>>>>> loss: 0.1487976461648941\n",
      "Epoch 102: >>>>>>>>>>>>> loss: 0.1480502188205719\n",
      "Epoch 103: >>>>>>>>>>>>> loss: 0.1482209861278534\n",
      "Epoch 104: >>>>>>>>>>>>> loss: 0.1604386568069458\n",
      "Epoch 105: >>>>>>>>>>>>> loss: 0.1296694427728653\n",
      "Epoch 106: >>>>>>>>>>>>> loss: 0.13714203238487244\n",
      "Epoch 107: >>>>>>>>>>>>> loss: 0.12516359984874725\n",
      "Epoch 108: >>>>>>>>>>>>> loss: 0.12973427772521973\n",
      "Epoch 109: >>>>>>>>>>>>> loss: 0.12401897460222244\n",
      "Epoch 110: >>>>>>>>>>>>> loss: 0.1261138767004013\n",
      "Epoch 111: >>>>>>>>>>>>> loss: 0.1240839883685112\n",
      "Epoch 112: >>>>>>>>>>>>> loss: 0.13555896282196045\n",
      "Epoch 113: >>>>>>>>>>>>> loss: 0.11251303553581238\n",
      "Epoch 114: >>>>>>>>>>>>> loss: 0.11354794353246689\n",
      "Epoch 115: >>>>>>>>>>>>> loss: 0.10430668294429779\n",
      "Epoch 116: >>>>>>>>>>>>> loss: 0.11245805025100708\n",
      "Epoch 117: >>>>>>>>>>>>> loss: 0.12060561031103134\n",
      "Epoch 118: >>>>>>>>>>>>> loss: 0.10425747185945511\n",
      "Epoch 119: >>>>>>>>>>>>> loss: 0.13924752175807953\n",
      "Epoch 120: >>>>>>>>>>>>> loss: 0.09882576763629913\n",
      "Epoch 121: >>>>>>>>>>>>> loss: 0.09503888338804245\n",
      "Epoch 122: >>>>>>>>>>>>> loss: 0.09795360267162323\n",
      "Epoch 123: >>>>>>>>>>>>> loss: 0.08810845762491226\n",
      "Epoch 124: >>>>>>>>>>>>> loss: 0.09324026852846146\n",
      "Epoch 125: >>>>>>>>>>>>> loss: 0.10588252544403076\n",
      "Epoch 126: >>>>>>>>>>>>> loss: 0.0817791074514389\n",
      "Epoch 127: >>>>>>>>>>>>> loss: 0.0808505117893219\n",
      "Epoch 128: >>>>>>>>>>>>> loss: 0.09004352241754532\n",
      "Epoch 129: >>>>>>>>>>>>> loss: 0.09636419266462326\n",
      "Epoch 130: >>>>>>>>>>>>> loss: 0.0782792940735817\n",
      "Epoch 131: >>>>>>>>>>>>> loss: 0.10360176116228104\n",
      "Epoch 132: >>>>>>>>>>>>> loss: 0.07554519176483154\n",
      "Epoch 133: >>>>>>>>>>>>> loss: 0.08449224382638931\n",
      "Epoch 134: >>>>>>>>>>>>> loss: 0.08082427084445953\n",
      "Epoch 135: >>>>>>>>>>>>> loss: 0.08025836944580078\n",
      "Epoch 136: >>>>>>>>>>>>> loss: 0.07658010721206665\n",
      "Epoch 137: >>>>>>>>>>>>> loss: 0.07030981779098511\n",
      "Epoch 138: >>>>>>>>>>>>> loss: 0.07462375611066818\n",
      "Epoch 139: >>>>>>>>>>>>> loss: 0.06934867799282074\n",
      "Epoch 140: >>>>>>>>>>>>> loss: 0.07111281901597977\n",
      "Epoch 141: >>>>>>>>>>>>> loss: 0.07198236137628555\n",
      "Epoch 142: >>>>>>>>>>>>> loss: 0.05819818377494812\n",
      "Epoch 143: >>>>>>>>>>>>> loss: 0.05822144076228142\n",
      "Epoch 144: >>>>>>>>>>>>> loss: 0.06140625849366188\n",
      "Epoch 145: >>>>>>>>>>>>> loss: 0.05390072613954544\n",
      "Epoch 146: >>>>>>>>>>>>> loss: 0.053253188729286194\n",
      "Epoch 147: >>>>>>>>>>>>> loss: 0.060056913644075394\n",
      "Epoch 148: >>>>>>>>>>>>> loss: 0.052531249821186066\n",
      "Epoch 149: >>>>>>>>>>>>> loss: 0.05550551787018776\n",
      "Epoch 150: >>>>>>>>>>>>> loss: 0.05562132969498634\n",
      "Epoch 151: >>>>>>>>>>>>> loss: 0.057637330144643784\n",
      "Epoch 152: >>>>>>>>>>>>> loss: 0.059878554195165634\n",
      "Epoch 153: >>>>>>>>>>>>> loss: 0.0531437024474144\n",
      "Epoch 154: >>>>>>>>>>>>> loss: 0.07499691843986511\n",
      "Epoch 155: >>>>>>>>>>>>> loss: 0.04199184104800224\n",
      "Epoch 156: >>>>>>>>>>>>> loss: 0.042491987347602844\n",
      "Epoch 157: >>>>>>>>>>>>> loss: 0.05132223293185234\n",
      "Epoch 158: >>>>>>>>>>>>> loss: 0.07429828494787216\n",
      "Epoch 159: >>>>>>>>>>>>> loss: 0.06598135828971863\n",
      "Epoch 160: >>>>>>>>>>>>> loss: 0.06905094534158707\n",
      "Epoch 161: >>>>>>>>>>>>> loss: 0.046124789863824844\n",
      "Epoch 162: >>>>>>>>>>>>> loss: 0.03810684755444527\n",
      "Epoch 163: >>>>>>>>>>>>> loss: 0.03955439105629921\n",
      "Epoch 164: >>>>>>>>>>>>> loss: 0.04850710555911064\n",
      "Epoch 165: >>>>>>>>>>>>> loss: 0.03489716723561287\n",
      "Epoch 166: >>>>>>>>>>>>> loss: 0.047147512435913086\n",
      "Epoch 167: >>>>>>>>>>>>> loss: 0.04079166054725647\n",
      "Epoch 168: >>>>>>>>>>>>> loss: 0.06090840324759483\n",
      "Epoch 169: >>>>>>>>>>>>> loss: 0.038881879299879074\n",
      "Epoch 170: >>>>>>>>>>>>> loss: 0.04629330337047577\n",
      "Epoch 171: >>>>>>>>>>>>> loss: 0.03442825376987457\n",
      "Epoch 172: >>>>>>>>>>>>> loss: 0.03396250680088997\n",
      "Epoch 173: >>>>>>>>>>>>> loss: 0.038027144968509674\n",
      "Epoch 174: >>>>>>>>>>>>> loss: 0.03444792330265045\n",
      "Epoch 175: >>>>>>>>>>>>> loss: 0.04540175199508667\n",
      "Epoch 176: >>>>>>>>>>>>> loss: 0.05521167442202568\n",
      "Epoch 177: >>>>>>>>>>>>> loss: 0.03992133587598801\n",
      "Epoch 178: >>>>>>>>>>>>> loss: 0.033846814185380936\n",
      "Epoch 179: >>>>>>>>>>>>> loss: 0.053141999989748\n",
      "Epoch 180: >>>>>>>>>>>>> loss: 0.03501025587320328\n",
      "Epoch 181: >>>>>>>>>>>>> loss: 0.0349026694893837\n",
      "Epoch 182: >>>>>>>>>>>>> loss: 0.034572746604681015\n",
      "Epoch 183: >>>>>>>>>>>>> loss: 0.029400816187262535\n",
      "Epoch 184: >>>>>>>>>>>>> loss: 0.052285704761743546\n",
      "Epoch 185: >>>>>>>>>>>>> loss: 0.026845823973417282\n",
      "Epoch 186: >>>>>>>>>>>>> loss: 0.049388185143470764\n",
      "Epoch 187: >>>>>>>>>>>>> loss: 0.02652929164469242\n",
      "Epoch 188: >>>>>>>>>>>>> loss: 0.02708684839308262\n",
      "Epoch 189: >>>>>>>>>>>>> loss: 0.054693859070539474\n",
      "Epoch 190: >>>>>>>>>>>>> loss: 0.044272761791944504\n",
      "Epoch 191: >>>>>>>>>>>>> loss: 0.024528788402676582\n",
      "Epoch 192: >>>>>>>>>>>>> loss: 0.025419829413294792\n",
      "Epoch 193: >>>>>>>>>>>>> loss: 0.026964915916323662\n",
      "Epoch 194: >>>>>>>>>>>>> loss: 0.029619760811328888\n",
      "Epoch 195: >>>>>>>>>>>>> loss: 0.02317175641655922\n",
      "Epoch 196: >>>>>>>>>>>>> loss: 0.023649975657463074\n",
      "Epoch 197: >>>>>>>>>>>>> loss: 0.024962307885289192\n",
      "Epoch 198: >>>>>>>>>>>>> loss: 0.022966355085372925\n",
      "Epoch 199: >>>>>>>>>>>>> loss: 0.03060721606016159\n",
      "Epoch 200: >>>>>>>>>>>>> loss: 0.02694614976644516\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.17      0.26        24\n",
      "           1       0.74      0.96      0.84        24\n",
      "           2       0.50      0.67      0.57        24\n",
      "           3       1.00      0.25      0.40        24\n",
      "           4       0.43      0.92      0.59        24\n",
      "           5       0.43      0.25      0.32        24\n",
      "           6       0.57      1.00      0.73        24\n",
      "           7       0.00      0.00      0.00        24\n",
      "\n",
      "    accuracy                           0.53       192\n",
      "   macro avg       0.53      0.53      0.46       192\n",
      "weighted avg       0.53      0.53      0.46       192\n",
      "\n",
      "Evaluation:\n",
      "[[ 4  0 12  0  0  8  0  0]\n",
      " [ 0 23  1  0  0  0  0  0]\n",
      " [ 0  8 16  0  0  0  0  0]\n",
      " [ 0  0  0  6 18  0  0  0]\n",
      " [ 0  0  0  0 22  0  0  2]\n",
      " [ 0  0  0  0 11  6  0  7]\n",
      " [ 0  0  0  0  0  0 24  0]\n",
      " [ 3  0  3  0  0  0 18  0]]\n",
      "save checkpoint in : ./models/saved_models/CnnLstm_v2_checkpoint_200__2021_01_14_14_23\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "model  = train(model=model, train_loader=train_loader, criterion=criterion, \n",
    "                 optimizer=optimizer, end=epochs, start=101, evaluation=True, auto_save=True,\n",
    "                 name='CnnLstm_v2',test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch 1: >>>>>>>>>>>>> loss: 3.0878446102142334\n",
      "Epoch 2: >>>>>>>>>>>>> loss: 2.752030611038208\n",
      "Epoch 3: >>>>>>>>>>>>> loss: 3.13301682472229\n",
      "Epoch 4: >>>>>>>>>>>>> loss: 3.130406379699707\n",
      "Epoch 5: >>>>>>>>>>>>> loss: 3.243772029876709\n",
      "Epoch 6: >>>>>>>>>>>>> loss: 3.0399789810180664\n",
      "Epoch 7: >>>>>>>>>>>>> loss: 3.3295552730560303\n",
      "Epoch 8: >>>>>>>>>>>>> loss: 2.9830477237701416\n",
      "Epoch 9: >>>>>>>>>>>>> loss: 3.184230327606201\n",
      "Epoch 10: >>>>>>>>>>>>> loss: 3.1230194568634033\n",
      "Epoch 11: >>>>>>>>>>>>> loss: 3.1196846961975098\n",
      "Epoch 12: >>>>>>>>>>>>> loss: 3.275430679321289\n",
      "Epoch 13: >>>>>>>>>>>>> loss: 2.906364679336548\n",
      "Epoch 14: >>>>>>>>>>>>> loss: 2.8974521160125732\n",
      "Epoch 15: >>>>>>>>>>>>> loss: 2.918517827987671\n",
      "Epoch 16: >>>>>>>>>>>>> loss: 2.857963800430298\n",
      "Epoch 17: >>>>>>>>>>>>> loss: 2.991358995437622\n",
      "Epoch 18: >>>>>>>>>>>>> loss: 3.1054749488830566\n",
      "Epoch 19: >>>>>>>>>>>>> loss: 2.9762425422668457\n",
      "Epoch 20: >>>>>>>>>>>>> loss: 3.0132040977478027\n",
      "Epoch 21: >>>>>>>>>>>>> loss: 3.104688882827759\n",
      "Epoch 22: >>>>>>>>>>>>> loss: 2.956728219985962\n",
      "Epoch 23: >>>>>>>>>>>>> loss: 2.7569215297698975\n",
      "Epoch 24: >>>>>>>>>>>>> loss: 2.826502561569214\n",
      "Epoch 25: >>>>>>>>>>>>> loss: 3.201997756958008\n",
      "Epoch 26: >>>>>>>>>>>>> loss: 2.9136526584625244\n",
      "Epoch 27: >>>>>>>>>>>>> loss: 3.1330134868621826\n",
      "Epoch 28: >>>>>>>>>>>>> loss: 2.883283853530884\n",
      "Epoch 29: >>>>>>>>>>>>> loss: 3.0354018211364746\n",
      "Epoch 30: >>>>>>>>>>>>> loss: 2.800936460494995\n",
      "Epoch 31: >>>>>>>>>>>>> loss: 3.1300644874572754\n",
      "Epoch 32: >>>>>>>>>>>>> loss: 3.05354905128479\n",
      "Epoch 33: >>>>>>>>>>>>> loss: 3.1025969982147217\n",
      "Epoch 34: >>>>>>>>>>>>> loss: 2.918687582015991\n",
      "Epoch 35: >>>>>>>>>>>>> loss: 3.0227103233337402\n",
      "Epoch 36: >>>>>>>>>>>>> loss: 2.8004565238952637\n",
      "Epoch 37: >>>>>>>>>>>>> loss: 2.97813081741333\n",
      "Epoch 38: >>>>>>>>>>>>> loss: 2.927912950515747\n",
      "Epoch 39: >>>>>>>>>>>>> loss: 2.8635215759277344\n",
      "Epoch 40: >>>>>>>>>>>>> loss: 2.9671740531921387\n",
      "Epoch 41: >>>>>>>>>>>>> loss: 2.7755744457244873\n",
      "Epoch 42: >>>>>>>>>>>>> loss: 2.935103178024292\n",
      "Epoch 43: >>>>>>>>>>>>> loss: 2.9596049785614014\n",
      "Epoch 44: >>>>>>>>>>>>> loss: 3.1682653427124023\n",
      "Epoch 45: >>>>>>>>>>>>> loss: 3.223031520843506\n",
      "Epoch 46: >>>>>>>>>>>>> loss: 3.0842859745025635\n",
      "Epoch 47: >>>>>>>>>>>>> loss: 2.9869749546051025\n",
      "Epoch 48: >>>>>>>>>>>>> loss: 3.02551007270813\n",
      "Epoch 49: >>>>>>>>>>>>> loss: 3.111614465713501\n",
      "Epoch 50: >>>>>>>>>>>>> loss: 2.9554145336151123\n",
      "Epoch 51: >>>>>>>>>>>>> loss: 3.129147529602051\n",
      "Epoch 52: >>>>>>>>>>>>> loss: 2.963541030883789\n",
      "Epoch 53: >>>>>>>>>>>>> loss: 3.181347370147705\n",
      "Epoch 54: >>>>>>>>>>>>> loss: 3.119673013687134\n",
      "Epoch 55: >>>>>>>>>>>>> loss: 3.379295825958252\n",
      "Epoch 56: >>>>>>>>>>>>> loss: 2.8608767986297607\n",
      "Epoch 57: >>>>>>>>>>>>> loss: 3.0069000720977783\n",
      "Epoch 58: >>>>>>>>>>>>> loss: 3.035977602005005\n",
      "Epoch 59: >>>>>>>>>>>>> loss: 2.9930877685546875\n",
      "Epoch 60: >>>>>>>>>>>>> loss: 3.0027735233306885\n",
      "Epoch 61: >>>>>>>>>>>>> loss: 2.896322011947632\n",
      "Epoch 62: >>>>>>>>>>>>> loss: 2.864677667617798\n",
      "Epoch 63: >>>>>>>>>>>>> loss: 3.3362176418304443\n",
      "Epoch 64: >>>>>>>>>>>>> loss: 3.0962328910827637\n",
      "Epoch 65: >>>>>>>>>>>>> loss: 2.971330165863037\n",
      "Epoch 66: >>>>>>>>>>>>> loss: 3.1537439823150635\n",
      "Epoch 67: >>>>>>>>>>>>> loss: 3.078763961791992\n",
      "Epoch 68: >>>>>>>>>>>>> loss: 2.736253023147583\n",
      "Epoch 69: >>>>>>>>>>>>> loss: 3.1825859546661377\n",
      "Epoch 70: >>>>>>>>>>>>> loss: 3.0241053104400635\n",
      "Epoch 71: >>>>>>>>>>>>> loss: 3.0318443775177\n",
      "Epoch 72: >>>>>>>>>>>>> loss: 2.9577457904815674\n",
      "Epoch 73: >>>>>>>>>>>>> loss: 2.9609971046447754\n",
      "Epoch 74: >>>>>>>>>>>>> loss: 2.850727081298828\n",
      "Epoch 75: >>>>>>>>>>>>> loss: 2.8077609539031982\n",
      "Epoch 76: >>>>>>>>>>>>> loss: 3.1861891746520996\n",
      "Epoch 77: >>>>>>>>>>>>> loss: 2.8342623710632324\n",
      "Epoch 78: >>>>>>>>>>>>> loss: 2.8325729370117188\n",
      "Epoch 79: >>>>>>>>>>>>> loss: 2.936110496520996\n",
      "Epoch 80: >>>>>>>>>>>>> loss: 3.0970265865325928\n",
      "Epoch 81: >>>>>>>>>>>>> loss: 2.9914557933807373\n",
      "Epoch 82: >>>>>>>>>>>>> loss: 3.101821184158325\n",
      "Epoch 83: >>>>>>>>>>>>> loss: 3.1431877613067627\n",
      "Epoch 84: >>>>>>>>>>>>> loss: 3.007298707962036\n",
      "Epoch 85: >>>>>>>>>>>>> loss: 3.094482183456421\n",
      "Epoch 86: >>>>>>>>>>>>> loss: 3.0513691902160645\n",
      "Epoch 87: >>>>>>>>>>>>> loss: 3.0779712200164795\n",
      "Epoch 88: >>>>>>>>>>>>> loss: 3.1562538146972656\n",
      "Epoch 89: >>>>>>>>>>>>> loss: 2.7905070781707764\n",
      "Epoch 90: >>>>>>>>>>>>> loss: 3.1393113136291504\n",
      "Epoch 91: >>>>>>>>>>>>> loss: 3.0550613403320312\n",
      "Epoch 92: >>>>>>>>>>>>> loss: 3.130885601043701\n",
      "Epoch 93: >>>>>>>>>>>>> loss: 3.2416651248931885\n",
      "Epoch 94: >>>>>>>>>>>>> loss: 2.988450527191162\n",
      "Epoch 95: >>>>>>>>>>>>> loss: 3.2596023082733154\n",
      "Epoch 96: >>>>>>>>>>>>> loss: 2.9378063678741455\n",
      "Epoch 97: >>>>>>>>>>>>> loss: 3.002021551132202\n",
      "Epoch 98: >>>>>>>>>>>>> loss: 2.8904783725738525\n",
      "Epoch 99: >>>>>>>>>>>>> loss: 3.135085344314575\n",
      "Epoch 100: >>>>>>>>>>>>> loss: 2.8764402866363525\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.12      0.12        25\n",
      "           1       0.18      0.12      0.14        25\n",
      "           2       0.06      0.04      0.05        25\n",
      "           3       0.10      0.12      0.11        25\n",
      "           4       0.00      0.00      0.00        25\n",
      "           5       0.15      0.16      0.15        25\n",
      "           6       0.21      0.24      0.23        25\n",
      "           7       0.12      0.16      0.14        25\n",
      "\n",
      "    accuracy                           0.12       200\n",
      "   macro avg       0.12      0.12      0.12       200\n",
      "weighted avg       0.12      0.12      0.12       200\n",
      "\n",
      "Evaluation:\n",
      "None\n",
      "save checkpoint in : ./models/saved_models/Baseline_checkpoint_100__2021_01_12_19_10\n",
      "Epoch 101: >>>>>>>>>>>>> loss: 3.171443462371826\n",
      "Epoch 102: >>>>>>>>>>>>> loss: 3.0130791664123535\n",
      "Epoch 103: >>>>>>>>>>>>> loss: 3.140843391418457\n",
      "Epoch 104: >>>>>>>>>>>>> loss: 3.043334484100342\n",
      "Epoch 105: >>>>>>>>>>>>> loss: 3.200587749481201\n",
      "Epoch 106: >>>>>>>>>>>>> loss: 3.0551843643188477\n",
      "Epoch 107: >>>>>>>>>>>>> loss: 2.9091365337371826\n",
      "Epoch 108: >>>>>>>>>>>>> loss: 3.2158305644989014\n",
      "Epoch 109: >>>>>>>>>>>>> loss: 2.8778231143951416\n",
      "Epoch 110: >>>>>>>>>>>>> loss: 3.00132417678833\n",
      "Epoch 111: >>>>>>>>>>>>> loss: 2.9425528049468994\n",
      "Epoch 112: >>>>>>>>>>>>> loss: 3.1923625469207764\n",
      "Epoch 113: >>>>>>>>>>>>> loss: 2.960848331451416\n",
      "Epoch 114: >>>>>>>>>>>>> loss: 3.0310580730438232\n",
      "Epoch 115: >>>>>>>>>>>>> loss: 3.1135635375976562\n",
      "Epoch 116: >>>>>>>>>>>>> loss: 3.0087740421295166\n",
      "Epoch 117: >>>>>>>>>>>>> loss: 3.1898834705352783\n",
      "Epoch 118: >>>>>>>>>>>>> loss: 3.060816526412964\n",
      "Epoch 119: >>>>>>>>>>>>> loss: 3.1055455207824707\n",
      "Epoch 120: >>>>>>>>>>>>> loss: 3.165496587753296\n",
      "Epoch 121: >>>>>>>>>>>>> loss: 3.16249418258667\n",
      "Epoch 122: >>>>>>>>>>>>> loss: 3.2595648765563965\n",
      "Epoch 123: >>>>>>>>>>>>> loss: 3.07033109664917\n",
      "Epoch 124: >>>>>>>>>>>>> loss: 2.9531617164611816\n",
      "Epoch 125: >>>>>>>>>>>>> loss: 3.2059783935546875\n",
      "Epoch 126: >>>>>>>>>>>>> loss: 3.15291428565979\n",
      "Epoch 127: >>>>>>>>>>>>> loss: 3.031497001647949\n",
      "Epoch 128: >>>>>>>>>>>>> loss: 3.0111310482025146\n",
      "Epoch 129: >>>>>>>>>>>>> loss: 2.9514126777648926\n",
      "Epoch 130: >>>>>>>>>>>>> loss: 2.7570621967315674\n",
      "Epoch 131: >>>>>>>>>>>>> loss: 2.797525405883789\n",
      "Epoch 132: >>>>>>>>>>>>> loss: 2.9849021434783936\n",
      "Epoch 133: >>>>>>>>>>>>> loss: 3.0497684478759766\n",
      "Epoch 134: >>>>>>>>>>>>> loss: 2.882444143295288\n",
      "Epoch 135: >>>>>>>>>>>>> loss: 3.0189950466156006\n",
      "Epoch 136: >>>>>>>>>>>>> loss: 3.028284788131714\n",
      "Epoch 137: >>>>>>>>>>>>> loss: 3.2105355262756348\n",
      "Epoch 138: >>>>>>>>>>>>> loss: 2.8489010334014893\n",
      "Epoch 139: >>>>>>>>>>>>> loss: 2.855510950088501\n",
      "Epoch 140: >>>>>>>>>>>>> loss: 2.797168254852295\n",
      "Epoch 141: >>>>>>>>>>>>> loss: 2.953049659729004\n",
      "Epoch 142: >>>>>>>>>>>>> loss: 2.909369707107544\n",
      "Epoch 143: >>>>>>>>>>>>> loss: 3.1879734992980957\n",
      "Epoch 144: >>>>>>>>>>>>> loss: 2.817657709121704\n",
      "Epoch 145: >>>>>>>>>>>>> loss: 3.0177040100097656\n",
      "Epoch 146: >>>>>>>>>>>>> loss: 3.376490831375122\n",
      "Epoch 147: >>>>>>>>>>>>> loss: 3.2613871097564697\n",
      "Epoch 148: >>>>>>>>>>>>> loss: 2.952495574951172\n",
      "Epoch 149: >>>>>>>>>>>>> loss: 2.840205192565918\n",
      "Epoch 150: >>>>>>>>>>>>> loss: 3.3383116722106934\n",
      "Epoch 151: >>>>>>>>>>>>> loss: 3.006347179412842\n",
      "Epoch 152: >>>>>>>>>>>>> loss: 3.0495595932006836\n",
      "Epoch 153: >>>>>>>>>>>>> loss: 3.0955233573913574\n",
      "Epoch 154: >>>>>>>>>>>>> loss: 3.146001100540161\n",
      "Epoch 155: >>>>>>>>>>>>> loss: 2.780116319656372\n",
      "Epoch 156: >>>>>>>>>>>>> loss: 3.2629711627960205\n",
      "Epoch 157: >>>>>>>>>>>>> loss: 3.1313047409057617\n",
      "Epoch 158: >>>>>>>>>>>>> loss: 2.9459729194641113\n",
      "Epoch 159: >>>>>>>>>>>>> loss: 2.9656519889831543\n",
      "Epoch 160: >>>>>>>>>>>>> loss: 2.902775764465332\n",
      "Epoch 161: >>>>>>>>>>>>> loss: 3.22898006439209\n",
      "Epoch 162: >>>>>>>>>>>>> loss: 3.073711633682251\n",
      "Epoch 163: >>>>>>>>>>>>> loss: 3.063356637954712\n",
      "Epoch 164: >>>>>>>>>>>>> loss: 3.044023275375366\n",
      "Epoch 165: >>>>>>>>>>>>> loss: 3.0399701595306396\n",
      "Epoch 166: >>>>>>>>>>>>> loss: 2.973405361175537\n",
      "Epoch 167: >>>>>>>>>>>>> loss: 2.79913067817688\n",
      "Epoch 168: >>>>>>>>>>>>> loss: 3.080512762069702\n",
      "Epoch 169: >>>>>>>>>>>>> loss: 2.6695127487182617\n",
      "Epoch 170: >>>>>>>>>>>>> loss: 3.041332960128784\n",
      "Epoch 171: >>>>>>>>>>>>> loss: 2.8815512657165527\n",
      "Epoch 172: >>>>>>>>>>>>> loss: 3.0730035305023193\n",
      "Epoch 173: >>>>>>>>>>>>> loss: 2.7651751041412354\n",
      "Epoch 174: >>>>>>>>>>>>> loss: 3.0732665061950684\n",
      "Epoch 175: >>>>>>>>>>>>> loss: 2.7885191440582275\n",
      "Epoch 176: >>>>>>>>>>>>> loss: 3.178368330001831\n",
      "Epoch 177: >>>>>>>>>>>>> loss: 2.9655299186706543\n",
      "Epoch 178: >>>>>>>>>>>>> loss: 3.106254816055298\n",
      "Epoch 179: >>>>>>>>>>>>> loss: 2.721958637237549\n",
      "Epoch 180: >>>>>>>>>>>>> loss: 2.939643383026123\n",
      "Epoch 181: >>>>>>>>>>>>> loss: 3.215432643890381\n",
      "Epoch 182: >>>>>>>>>>>>> loss: 3.1550822257995605\n",
      "Epoch 183: >>>>>>>>>>>>> loss: 3.2813830375671387\n",
      "Epoch 184: >>>>>>>>>>>>> loss: 3.0717644691467285\n",
      "Epoch 185: >>>>>>>>>>>>> loss: 2.8900434970855713\n",
      "Epoch 186: >>>>>>>>>>>>> loss: 3.100370168685913\n",
      "Epoch 187: >>>>>>>>>>>>> loss: 2.8758115768432617\n",
      "Epoch 188: >>>>>>>>>>>>> loss: 3.1233584880828857\n",
      "Epoch 189: >>>>>>>>>>>>> loss: 2.956411838531494\n",
      "Epoch 190: >>>>>>>>>>>>> loss: 3.046215772628784\n",
      "Epoch 191: >>>>>>>>>>>>> loss: 3.0396625995635986\n",
      "Epoch 192: >>>>>>>>>>>>> loss: 2.8999111652374268\n",
      "Epoch 193: >>>>>>>>>>>>> loss: 3.2452943325042725\n",
      "Epoch 194: >>>>>>>>>>>>> loss: 2.971694231033325\n",
      "Epoch 195: >>>>>>>>>>>>> loss: 2.9440629482269287\n",
      "Epoch 196: >>>>>>>>>>>>> loss: 3.0312891006469727\n",
      "Epoch 197: >>>>>>>>>>>>> loss: 2.896026849746704\n",
      "Epoch 198: >>>>>>>>>>>>> loss: 2.9495208263397217\n",
      "Epoch 199: >>>>>>>>>>>>> loss: 2.971513271331787\n",
      "Epoch 200: >>>>>>>>>>>>> loss: 2.8379640579223633\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.12      0.12        25\n",
      "           1       0.14      0.12      0.13        25\n",
      "           2       0.10      0.08      0.09        25\n",
      "           3       0.19      0.28      0.23        25\n",
      "           4       0.00      0.00      0.00        25\n",
      "           5       0.28      0.28      0.28        25\n",
      "           6       0.13      0.20      0.16        25\n",
      "           7       0.18      0.16      0.17        25\n",
      "\n",
      "    accuracy                           0.15       200\n",
      "   macro avg       0.14      0.16      0.15       200\n",
      "weighted avg       0.14      0.15      0.15       200\n",
      "\n",
      "Evaluation:\n",
      "None\n",
      "save checkpoint in : ./models/saved_models/Baseline_checkpoint_200__2021_01_12_19_44\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Train\n",
    "epochs = 200\n",
    "model  = train__(model=cnn, train_loader=train_loader, criterion=criterion, \n",
    "                 optimizer=optimizer, epochs=epochs, start=0, evaluation=True, auto_save=True,\n",
    "                 name='Baseline',test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
