{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import gmtime, strftime\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import poutyne\n",
    "from poutyne import Model,Experiment\n",
    "import transformers\n",
    "\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.custom_data import filepath_dataframe,nucPaired_fpDataframe\n",
    "from data.selection import Selection,SelectionSet_1\n",
    "from data.torchData import DataLoading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.transformer_pretraining import Wav2VecPreTraining\n",
    "from models.hf_transformers import Sig2VecConfig, Sig2VecForPreTraining, Sig2VecForSequenceClassificationPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name:  Sig2Vec_Pretraining_SelectionSet1_Comment-TestTrainer\n",
      "Cuda Availability:  False\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################################\n",
    "\n",
    "# random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# gpu setting\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.set_device(DEVICE)\n",
    "device = DEVICE\n",
    "\n",
    "## data directory\n",
    "data_dir  = '/root/npy_format'\n",
    "readtype = 'npy'\n",
    "splitchar = '/'\n",
    "fpath = '.'\n",
    "\n",
    "# data selection\n",
    "data_selection = SelectionSet_1()\n",
    "dataselection_name = 'SelectionSet1'\n",
    "\n",
    "# data loading\n",
    "transform = None\n",
    "batch_size = 32\n",
    "num_workers = 0\n",
    "\n",
    "# training\n",
    "optim = torch.optim.SGD\n",
    "lr = 0.0005\n",
    "mask_prob= 0.2\n",
    "mask_length =4\n",
    "num_negative =4 \n",
    "pretrain_epochs = 1\n",
    "finetune_epochs = 1\n",
    "supervision = None\n",
    "temperature = 0.5\n",
    "\n",
    "# model\n",
    "in_channels = 70\n",
    "embed_size = 512\n",
    "num_hidden_layers = 2\n",
    "num_attention_heads = 16\n",
    "intermediate_size = 4*embed_size\n",
    "num_codevector_groups = 32\n",
    "num_codevectors_per_group = embed_size//num_codevector_groups\n",
    "codevector_dim = 512//4\n",
    "proj_codevector_dim = 512//4\n",
    "total_time = 1600\n",
    "num_frame = 25\n",
    "frame_len = total_time//num_frame\n",
    "n_classes = 6\n",
    "network_name = 'Sig2Vec'\n",
    "\n",
    "# Experiment Name\n",
    "comment = 'TestTrainer'\n",
    "exp_name = f'{network_name}_Pretraining_{dataselection_name}_Comment-{comment}'\n",
    "\n",
    "# auto\n",
    "\n",
    "model_dir = os.path.join(fpath,'saved_model')\n",
    "model_fname = os.path.join(model_dir,f'{exp_name}')\n",
    "record_dir = os.path.join(fpath,'record')\n",
    "record_fname = os.path.join(record_dir,f'{exp_name}.csv')\n",
    "print('Experiment Name: ',exp_name)\n",
    "print('Cuda Availability: ',torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Sig2VecConfig(\n",
    "    in_channels = in_channels,\n",
    "    vocab_size = embed_size,\n",
    "    hidden_size = embed_size,\n",
    "    num_hidden_layers = num_hidden_layers,\n",
    "    num_attention_heads = num_attention_heads,\n",
    "    intermediate_size = intermediate_size,\n",
    "    hidden_act = 'gelu',\n",
    "    num_codevector_groups = num_codevector_groups, \n",
    "    num_codevectors_per_group = num_codevectors_per_group,\n",
    "    codevector_dim = codevector_dim,\n",
    "    proj_codevector_dim = proj_codevector_dim,\n",
    "    conv_dim = (embed_size,),\n",
    "    conv_stride = (frame_len,), \n",
    "    conv_kernel = (frame_len,)\n",
    ")\n",
    "\n",
    "module = Sig2VecForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data preparation\n",
    "df = filepath_dataframe(data_dir,splitchar)\n",
    "# df = nucPaired_fpDataframe(df)\n",
    "df_train,df_val,df_test = data_selection(df)\n",
    "df_train = pd.concat([df_train,df_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_loading = DataLoading(transform=transform,\n",
    "                               batch_size=batch_size,\n",
    "                               readtype=readtype,\n",
    "                               num_workers=num_workers,\n",
    "                               drop_last=True)\n",
    "\n",
    "\n",
    "pretrain_loader = pretrain_loading(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Wav2VecPreTraining(module,optim,lr,mask_prob,mask_length,num_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [2021-11-30 22:11:59.428 pytorch-1-6-cpu-py36--ml-t3-medium-4350395e6b439bff0bb751aa914e:497 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-11-30 22:11:59.491 pytorch-1-6-cpu-py36--ml-t3-medium-4350395e6b439bff0bb751aa914e:497 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> loss: 703.4725952148438\n"
     ]
    }
   ],
   "source": [
    "history = trainer.train(pretrain_loader,epochs=pretrain_epochs,verbose=True,rtn_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del module, trainer, pretrain_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FINE-TUNING #####\n",
    "\n",
    "# data loading\n",
    "data_loading = DataLoading(transform=transform,batch_size=batch_size,readtype=readtype,\n",
    "                           num_workers=num_workers,drop_last=True)\n",
    "test_loading = DataLoading(transform=transform,batch_size=len(df_test),readtype=readtype,\n",
    "                           num_workers=num_workers,drop_last=True)\n",
    "\n",
    "df_train = df_train.rename({'fullpath_x':'fullpath'},axis=1)\n",
    "df_val = df_val.rename({'fullpath_x':'fullpath'},axis=1)\n",
    "df_test = df_test.rename({'fullpath_x':'fullpath'},axis=1)\n",
    "\n",
    "train_loader = data_loading(df_train)\n",
    "val_loader   = data_loading(df_val)\n",
    "test_loader  = test_loading(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./saved_model/Sig2Vec_Pretraining_SelectionSet1_Comment-TestTrainer were not used when initializing Sig2VecForSequenceClassificationPT: ['quantizer.codevectors', 'project_q.bias', 'project_q.weight', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_hid.bias', 'quantizer.weight_proj.bias']\n",
      "- This IS expected if you are initializing Sig2VecForSequenceClassificationPT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Sig2VecForSequenceClassificationPT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Sig2VecForSequenceClassificationPT were not initialized from the model checkpoint at ./saved_model/Sig2Vec_Pretraining_SelectionSet1_Comment-TestTrainer and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load and create model\n",
    "model = Sig2VecForSequenceClassificationPT.from_pretrained(model_fname, num_labels=n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35mEpoch: \u001b[36m1/1 \u001b[35mTrain steps: \u001b[36m50 \u001b[35mVal steps: \u001b[36m1 \u001b[32m5m22.27s \u001b[35mloss:\u001b[94m 2.789990\u001b[35m acc:\u001b[94m 51.812500\u001b[35m fscore_micro:\u001b[94m 0.518125\u001b[35m fscore_macro:\u001b[94m 0.415889\u001b[35m val_loss:\u001b[94m 3.241449\u001b[35m val_acc:\u001b[94m 16.666668\u001b[35m val_fscore_micro:\u001b[94m 0.166667\u001b[35m val_fscore_macro:\u001b[94m 0.047619\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# train with poutyne\n",
    "mdl = Model(model,'adam','cross_entropy',\n",
    "            batch_metrics=['accuracy'],\n",
    "            epoch_metrics=[poutyne.F1('micro'),poutyne.F1('macro')]).to(device)\n",
    "history = mdl.fit_generator(train_generator=train_loader,valid_generator=test_loader,epochs=finetune_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history).to_csv(record_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.6-cpu-py36-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
