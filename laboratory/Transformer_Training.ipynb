{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from time import gmtime, strftime\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import poutyne\n",
    "from poutyne import Model,Experiment\n",
    "import transformers\n",
    "\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.custom_data import filepath_dataframe,nucPaired_fpDataframe\n",
    "from data.selection import Selection,SelectionSet_1\n",
    "from data.torchData import DataLoading\n",
    "from data.transformation import ReduceRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from training.transformer_pretraining import Wav2VecPreTraining\n",
    "from models.hf_transformers import Sig2VecConfig, Sig2VecForPreTraining, Sig2VecForSequenceClassificationPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment Name:  Sig2Vec_Pretraining_SelectionSet1_Comment-TestTrainer\n",
      "Cuda Availability:  True\n"
     ]
    }
   ],
   "source": [
    "#####################################################################################################################\n",
    "\n",
    "# random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "# gpu setting\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.set_device(DEVICE)\n",
    "device = DEVICE\n",
    "\n",
    "## data directory\n",
    "data_dir  = '/root/npy_format'\n",
    "readtype = 'npy'\n",
    "splitchar = '/'\n",
    "fpath = '.'\n",
    "\n",
    "# data selection\n",
    "data_selection = SelectionSet_1()\n",
    "dataselection_name = 'SelectionSet1'\n",
    "\n",
    "# data loading\n",
    "transform = ReduceRes()\n",
    "batch_size = 128\n",
    "num_workers = 0\n",
    "\n",
    "# model\n",
    "in_channels = 70\n",
    "embed_size = 768\n",
    "num_hidden_layers = 10\n",
    "num_attention_heads = 16\n",
    "intermediate_size = 4*embed_size\n",
    "total_time = 1600\n",
    "num_frame = 25\n",
    "frame_len = total_time//num_frame\n",
    "classifier_proj_size = 64\n",
    "num_labels = 6 \n",
    "num_conv_pos_embeddings = 128\n",
    "network_name = 'Sig2Vec'\n",
    "\n",
    "# training\n",
    "optim = torch.optim.Adam\n",
    "lr = 0.001\n",
    "pretrain_epochs = 100\n",
    "finetune_epochs = 10\n",
    "apply_spec_augment = True\n",
    "mask_time_prob = 0.2\n",
    "mask_time_length = 5\n",
    "mask_feature_prob = 0\n",
    "mask_feature_length = 0\n",
    "num_negatives = 2\n",
    "num_codevector_groups = 16  \n",
    "num_codevectors_per_group = embed_size // num_codevector_groups  \n",
    "codevector_dim = 128\n",
    "proj_codevector_dim = 128\n",
    "contrastive_logits_temperature = 0.1\n",
    "\n",
    "\n",
    "# Experiment Name\n",
    "comment = 'TestTrainer'\n",
    "exp_name = f'{network_name}_Pretraining_{dataselection_name}_Comment-{comment}'\n",
    "\n",
    "# auto\n",
    "\n",
    "model_dir = os.path.join(fpath,'saved_model')\n",
    "model_fname = os.path.join(model_dir,f'{exp_name}')\n",
    "record_dir = os.path.join(fpath,'record')\n",
    "record_fname = os.path.join(record_dir,f'{exp_name}.csv')\n",
    "print('Experiment Name: ',exp_name)\n",
    "print('Cuda Availability: ',torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Sig2VecConfig(\n",
    "    in_channels = in_channels,\n",
    "    vocab_size = embed_size,\n",
    "    hidden_size = embed_size,\n",
    "    num_hidden_layers = num_hidden_layers,\n",
    "    num_attention_heads = num_attention_heads,\n",
    "    intermediate_size = intermediate_size,\n",
    "    hidden_act = 'gelu',\n",
    "    conv_dim = (embed_size,),\n",
    "    conv_stride = (frame_len,), \n",
    "    conv_kernel = (frame_len,),\n",
    "    apply_spec_augment = True,    \n",
    "    mask_time_prob = mask_time_prob,\n",
    "    mask_time_length = mask_time_length,\n",
    "    mask_feature_prob = mask_feature_prob,\n",
    "    mask_feature_length = mask_feature_length,\n",
    "    num_negatives = num_negatives,\n",
    "    num_codevector_groups = num_codevector_groups, \n",
    "    num_codevectors_per_group = num_codevectors_per_group,\n",
    "    codevector_dim = codevector_dim, \n",
    "    proj_codevector_dim = proj_codevector_dim,\n",
    "    contrastive_logits_temperature = contrastive_logits_temperature,\n",
    ")\n",
    "\n",
    "module = Sig2VecForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data preparation\n",
    "df = filepath_dataframe(data_dir,splitchar)\n",
    "# df = nucPaired_fpDataframe(df)\n",
    "df_train,df_val,df_test = data_selection(df)\n",
    "df_train = pd.concat([df_train,df_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_loading = DataLoading(transform=transform,\n",
    "                               batch_size=batch_size,\n",
    "                               readtype=readtype,\n",
    "                               num_workers=num_workers,\n",
    "                               drop_last=True)\n",
    "\n",
    "\n",
    "pretrain_loader = pretrain_loading(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Wav2VecPreTraining(module,optim,lr, mask_time_prob, mask_time_length, num_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [2021-12-15 23:23:06.072 pytorch-1-6-gpu-py3-ml-g4dn-xlarge-2a0e2554e33379262f4fea063cf1:256 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2021-12-15 23:23:06.095 pytorch-1-6-gpu-py3-ml-g4dn-xlarge-2a0e2554e33379262f4fea063cf1:256 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-7495d8f2bb45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrain_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrtn_history\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/irs_toolbox/training/transformer_pretraining.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_loader, epochs, verbose, rtn_history, device)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "history = trainer.train(pretrain_loader,epochs=pretrain_epochs,verbose=True,rtn_history=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save(model_fname)\n",
    "record_fname = os.path.join(record_dir,f'{exp_name}_pretrain.csv')\n",
    "pd.DataFrame(history).to_csv(record_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del module, trainer, pretrain_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FINE-TUNING #####\n",
    "\n",
    "# data loading\n",
    "data_loading = DataLoading(transform=transform,batch_size=batch_size,readtype=readtype,\n",
    "                           num_workers=num_workers,drop_last=True)\n",
    "test_loading = DataLoading(transform=transform,batch_size=len(df_test),readtype=readtype,\n",
    "                           num_workers=num_workers,drop_last=True)\n",
    "\n",
    "df_train = df_train.rename({'fullpath_x':'fullpath'},axis=1)\n",
    "df_val = df_val.rename({'fullpath_x':'fullpath'},axis=1)\n",
    "df_test = df_test.rename({'fullpath_x':'fullpath'},axis=1)\n",
    "\n",
    "train_loader = data_loading(df_train)\n",
    "val_loader   = data_loading(df_val)\n",
    "test_loader  = test_loading(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric\n",
    "from data.torchData.utils import TransformersDataset, DatasetObject\n",
    "from models.hf_transformers import Sig2VecForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and create model\n",
    "model = Sig2VecForSequenceClassificationPT.from_pretrained(model_fname, num_labels=num_labels)\n",
    "\n",
    "# train with poutyne\n",
    "mdl = Model(model,'adam','cross_entropy',\n",
    "            batch_metrics=['accuracy'],\n",
    "            epoch_metrics=[poutyne.F1('micro'),poutyne.F1('macro')]).to(device)\n",
    "history = mdl.fit_generator(train_generator=train_loader,valid_generator=test_loader,epochs=finetune_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_fname = os.path.join(record_dir,f'{exp_name}_finetuned.csv')\n",
    "pd.DataFrame(history).to_csv(record_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersDataset(DatasetObject):\n",
    "\n",
    "    def __init__(self,filepaths,label=None,transform=None,readtype='npy'):\n",
    "        super().__init__(filepaths=filepaths,\n",
    "                         label=label,\n",
    "                         transform=transform,\n",
    "                         readtype=readtype)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        items = super().__getitem__(idx)\n",
    "        dic = {}\n",
    "        if isinstance(self.label,np.ndarray):\n",
    "            X,y = items\n",
    "            dic['input_values'] = X\n",
    "            dic['label'] = y\n",
    "        else:\n",
    "            X = items\n",
    "            dic['input_values'] = X\n",
    "        return dic\n",
    "\n",
    "    def load_data(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "class create_TransformersDataset(object):\n",
    "    \n",
    "    def __init__(self,transform,readtype='npy'):\n",
    "        self.transform = transform\n",
    "        self.readtype = readtype\n",
    "\n",
    "    def __call__(self,df):\n",
    "        datasetobj = TransformersDataset(filepaths=df['fullpath'].to_numpy(),\n",
    "                                         label=df['activity'].to_numpy(),\n",
    "                                         transform=self.transform,\n",
    "                                         readtype=self.readtype)\n",
    "        return datasetobj\n",
    "\n",
    "def create_compute_metrics():\n",
    "    metric = load_metric(\"accuracy\",\"f1\")\n",
    "    def compute_metrics(eval_pred):\n",
    "        \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "        predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "        return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    return compute_metrics\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \n",
    "        inputs['labels'] = inputs['labels'].long()\n",
    "        \n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "            loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sig2VecForSequenceClassification.from_pretrained(model_fname, num_labels=num_labels)\n",
    "\n",
    "dataset_creator = create_TransformersDataset(transform=transform,readtype=readtype)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=record_dir,\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=finetune_epochs,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "#     logging_steps = 5,\n",
    "#     eval_steps = 5,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_creator(df_train),\n",
    "    eval_dataset=dataset_creator(df_test),\n",
    "    compute_metrics=create_compute_metrics(),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
