{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e94d538310>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random seed\n",
    "np.random.seed(1024)\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "parallel = True\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data setting\n",
    "columns = [f\"col_{i+1}\" for i in range(501)] # 65*501\n",
    "window_size=None\n",
    "slide_size=None\n",
    "\n",
    "enlarge_size = (65,65)\n",
    "\n",
    "# Data main folder\n",
    "dirc = \"E:/external_data/Experiment4/Spectrogram_data_csv_files/CSI_data\"\n",
    "\n",
    "# Saving path\n",
    "PATH = 'C://Users/Creator/Script/Python/Project/irs_toolbox' # '.'\n",
    "\n",
    "# Training setting\n",
    "bsz = 128\n",
    "pre_train_epochs = 500\n",
    "fine_tune_epochs = 300\n",
    "\n",
    "exp_name = 'Encoder_64-128-512-64-7_pretrained_on_exp4csi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `import data` import the csv files and label each file based on its sub-folder. To increase the number sample, each file undergoes data augmentation with `window_size` and `slide_size` . For example, a file with directory: `dirc/label_name/file` and size: (65,501), its form multiple data with label `label_name`.\n",
    "\n",
    "* We take 20% of the entire data as validation and the remained as training set \n",
    "\n",
    "\n",
    "* Previous studies shows the pretraining is benefited from a larger batch size. However, due to GPU Memory limitation, here we are using batch size of 128 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Data\n"
     ]
    }
   ],
   "source": [
    "from data.spectrogram import import_data\n",
    "from data.process_data import label_encode, create_dataloaders\n",
    "\n",
    "def prepare_dataloader():\n",
    "    X,y  = import_data(dirc,columns=columns,window_size=window_size,slide_size=slide_size)\n",
    "    X = X.reshape(*X.shape,1).transpose(0,3,1,2)\n",
    "    y,lb = label_encode(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "    train_loader, test_loader = create_dataloaders(X_train, y_train, X_test, y_test, train_batch_sizes=bsz, test_batch_sizes=200, num_workers=num_workers)\n",
    "    return train_loader, test_loader,lb\n",
    "\n",
    "\n",
    "train_loader, test_loader, lb = prepare_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data augmentation, we only use crop, the data is enlarged back t `enlarge_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.self_supervised import DataAugmentation\n",
    "\n",
    "def create_pipe():\n",
    "    # External libraries required\n",
    "    \n",
    "    pipe = DataAugmentation(enlarge_size)\n",
    "    return pipe\n",
    "\n",
    "\n",
    "data_aug = create_pipe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use 3 layer CNN as the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Lambda\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder for spectrogram (1,65,65), 3 layer\n",
    "    \"\"\"\n",
    "    def __init__(self,num_filters):\n",
    "        super(Encoder, self).__init__()\n",
    "        l1,l2,l3 = num_filters\n",
    "        ### 1st ###\n",
    "        self.conv1 = nn.Conv2d(1,l1,kernel_size=5,stride=2)\n",
    "        self.norm1 = nn.BatchNorm2d(l1)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2))\n",
    "        ### 2nd ###\n",
    "        self.conv2 = nn.Conv2d(l1,l2,kernel_size=3,stride=2)\n",
    "        self.norm2 = nn.BatchNorm2d(l2)\n",
    "        self.actv2 = nn.ReLU()\n",
    "        self.pool2 = Lambda(lambda x:x) \n",
    "        ### 3rd ###\n",
    "        self.conv3 = nn.Conv2d(l2,l3,kernel_size=2,stride=2)\n",
    "        self.norm3 = Lambda(lambda x:x)\n",
    "        self.actv3 = nn.Tanh()\n",
    "        self.pool3 = nn.AvgPool2d(kernel_size=(2,2))\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = self.pool1(self.actv1(self.norm1(self.conv1(X))))\n",
    "        X = self.pool2(self.actv2(self.norm2(self.conv2(X))))\n",
    "        X = self.pool3(self.actv3(self.norm3(self.conv3(X))))\n",
    "        X = torch.flatten(X, 1)\n",
    "        # print(X.shape)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SimCLR implementation, the entire pretrain model consists of an encoder and a projection head. Where the projection head is multilayer perceptron (512>512>128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ED_module\n",
    "from models.self_supervised import Projection_head\n",
    "\n",
    "def create_pretrain_model():\n",
    "    # External libraries required\n",
    "    enc = Encoder([64,128,512])\n",
    "    clf = Projection_head(512,128,head='mlp')\n",
    "    model = ED_module(encoder=enc,decoder=clf)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [256, 64, 31, 31]           1,664\n",
      "       BatchNorm2d-2          [256, 64, 31, 31]             128\n",
      "              ReLU-3          [256, 64, 31, 31]               0\n",
      "         MaxPool2d-4          [256, 64, 15, 15]               0\n",
      "            Conv2d-5           [256, 128, 7, 7]          73,856\n",
      "       BatchNorm2d-6           [256, 128, 7, 7]             256\n",
      "              ReLU-7           [256, 128, 7, 7]               0\n",
      "            Lambda-8           [256, 128, 7, 7]               0\n",
      "            Conv2d-9           [256, 512, 3, 3]         262,656\n",
      "           Lambda-10           [256, 512, 3, 3]               0\n",
      "             Tanh-11           [256, 512, 3, 3]               0\n",
      "        AvgPool2d-12           [256, 512, 1, 1]               0\n",
      "          Encoder-13                 [256, 512]               0\n",
      "           Linear-14                 [256, 512]         262,656\n",
      "             ReLU-15                 [256, 512]               0\n",
      "           Linear-16                 [256, 128]          65,664\n",
      "  Projection_head-17                 [256, 128]               0\n",
      "================================================================\n",
      "Total params: 666,880\n",
      "Trainable params: 666,880\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 4.13\n",
      "Forward/backward pass size (MB): 469.00\n",
      "Params size (MB): 2.54\n",
      "Estimated Total Size (MB): 475.67\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "pretrain_model = create_pretrain_model()\n",
    "summary(pretrain_model,(1,*enlarge_size),batch_size=2*bsz,device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we directly implement **supervised contrastive loss** from [Supervised Contrastive Learning](https://github.com/HobbitLong/SupContrast/tree/6d5a3de39070249a19c62a345eea4acb5f26c0bc), a modification of **NT-Xent(normalized temperature-scaled cross entropy loss)** from SimCLR, where we leveraging the label information for class seperation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import SupConLoss\n",
    "\n",
    "def create_criterion():\n",
    "    # External libraries required\n",
    "    criterion = SupConLoss(temperature=0.1,base_temperature=0.1)\n",
    "    return criterion\n",
    "\n",
    "criterion = create_criterion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pretraining, we use SGD with learning rate of $5\\cdot 10^{-5}$. For finetuning, we use ADAM with learning rate $1\\cdot10^{-5}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(mode,model):\n",
    "    if mode == 'pretrain':\n",
    "        optimizer = torch.optim.SGD(list(model.parameters()), lr=0.0005)\n",
    "    elif mode == 'finetuning':\n",
    "        optimizer = torch.optim.Adam(list(model.parameters()), lr=0.0001)\n",
    "    else:\n",
    "        raise ValueError(\"mode: {'pretrain','finetuning'}\")\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = create_optimizer('pretrain',pretrain_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "Start Training\n",
      "Epoch 1: >>>>> loss: 5.679641246795654 \n",
      "Epoch 2: >>>>> loss: 5.8811140060424805 \n",
      "Epoch 3: >>>>> loss: 5.531131744384766 \n",
      "Epoch 4: >>>>> loss: 5.546865940093994 \n",
      "Epoch 5: >>>>> loss: 5.568600654602051 \n",
      "Epoch 6: >>>>> loss: 5.613371849060059 \n",
      "Epoch 7: >>>>> loss: 5.523183822631836 \n",
      "Epoch 8: >>>>> loss: 5.6177802085876465 \n",
      "Epoch 9: >>>>> loss: 5.570920467376709 \n",
      "Epoch 10: >>>>> loss: 5.486598014831543 \n",
      "Epoch 11: >>>>> loss: 5.478726387023926 \n",
      "Epoch 12: >>>>> loss: 5.480783462524414 \n",
      "Epoch 13: >>>>> loss: 5.5811076164245605 \n",
      "Epoch 14: >>>>> loss: 5.489938735961914 \n",
      "Epoch 15: >>>>> loss: 5.525685787200928 \n",
      "Epoch 16: >>>>> loss: 5.443608283996582 \n",
      "Epoch 17: >>>>> loss: 5.496007442474365 \n",
      "Epoch 18: >>>>> loss: 5.513676643371582 \n",
      "Epoch 19: >>>>> loss: 5.545963287353516 \n",
      "Epoch 20: >>>>> loss: 5.520123481750488 \n",
      "Epoch 21: >>>>> loss: 5.461921691894531 \n",
      "Epoch 22: >>>>> loss: 5.3948774337768555 \n",
      "Epoch 23: >>>>> loss: 5.551749229431152 \n",
      "Epoch 24: >>>>> loss: 5.482010364532471 \n",
      "Epoch 25: >>>>> loss: 5.534126281738281 \n",
      "Epoch 26: >>>>> loss: 5.543308258056641 \n",
      "Epoch 27: >>>>> loss: 5.45632266998291 \n",
      "Epoch 28: >>>>> loss: 5.518325328826904 \n",
      "Epoch 29: >>>>> loss: 5.512081146240234 \n",
      "Epoch 30: >>>>> loss: 5.39261531829834 \n",
      "Epoch 31: >>>>> loss: 5.455897331237793 \n",
      "Epoch 32: >>>>> loss: 5.456388473510742 \n",
      "Epoch 33: >>>>> loss: 5.575577259063721 \n",
      "Epoch 34: >>>>> loss: 5.457196235656738 \n",
      "Epoch 35: >>>>> loss: 5.4310455322265625 \n",
      "Epoch 36: >>>>> loss: 5.496034622192383 \n",
      "Epoch 37: >>>>> loss: 5.450860977172852 \n",
      "Epoch 38: >>>>> loss: 5.461754322052002 \n",
      "Epoch 39: >>>>> loss: 5.519670486450195 \n",
      "Epoch 40: >>>>> loss: 5.440010070800781 \n",
      "Epoch 41: >>>>> loss: 5.421313762664795 \n",
      "Epoch 42: >>>>> loss: 5.450223922729492 \n",
      "Epoch 43: >>>>> loss: 5.476015090942383 \n",
      "Epoch 44: >>>>> loss: 5.386584281921387 \n",
      "Epoch 45: >>>>> loss: 5.465326309204102 \n",
      "Epoch 46: >>>>> loss: 5.453834533691406 \n",
      "Epoch 47: >>>>> loss: 5.498223304748535 \n",
      "Epoch 48: >>>>> loss: 5.428163051605225 \n",
      "Epoch 49: >>>>> loss: 5.415795803070068 \n",
      "Epoch 50: >>>>> loss: 5.479630470275879 \n",
      "Epoch 51: >>>>> loss: 5.395178318023682 \n",
      "Epoch 52: >>>>> loss: 5.400215148925781 \n",
      "Epoch 53: >>>>> loss: 5.464573860168457 \n",
      "Epoch 54: >>>>> loss: 5.411496162414551 \n",
      "Epoch 55: >>>>> loss: 5.517298698425293 \n",
      "Epoch 56: >>>>> loss: 5.472715377807617 \n",
      "Epoch 57: >>>>> loss: 5.497709274291992 \n",
      "Epoch 58: >>>>> loss: 5.411368370056152 \n",
      "Epoch 59: >>>>> loss: 5.403627872467041 \n",
      "Epoch 60: >>>>> loss: 5.425300598144531 \n",
      "Epoch 61: >>>>> loss: 5.45013427734375 \n",
      "Epoch 62: >>>>> loss: 5.4531779289245605 \n",
      "Epoch 63: >>>>> loss: 5.477015495300293 \n",
      "Epoch 64: >>>>> loss: 5.46030330657959 \n",
      "Epoch 65: >>>>> loss: 5.440741062164307 \n",
      "Epoch 66: >>>>> loss: 5.432957172393799 \n",
      "Epoch 67: >>>>> loss: 5.414914131164551 \n",
      "Epoch 68: >>>>> loss: 5.496073246002197 \n",
      "Epoch 69: >>>>> loss: 5.390506744384766 \n",
      "Epoch 70: >>>>> loss: 5.361886024475098 \n",
      "Epoch 71: >>>>> loss: 5.3927412033081055 \n",
      "Epoch 72: >>>>> loss: 5.435142517089844 \n",
      "Epoch 73: >>>>> loss: 5.387293815612793 \n",
      "Epoch 74: >>>>> loss: 5.423749923706055 \n",
      "Epoch 75: >>>>> loss: 5.415948867797852 \n",
      "Epoch 76: >>>>> loss: 5.423209190368652 \n",
      "Epoch 77: >>>>> loss: 5.434425354003906 \n",
      "Epoch 78: >>>>> loss: 5.416534423828125 \n",
      "Epoch 79: >>>>> loss: 5.334709644317627 \n",
      "Epoch 80: >>>>> loss: 5.428956985473633 \n",
      "Epoch 81: >>>>> loss: 5.471033096313477 \n",
      "Epoch 82: >>>>> loss: 5.429651737213135 \n",
      "Epoch 83: >>>>> loss: 5.458118438720703 \n",
      "Epoch 84: >>>>> loss: 5.39605712890625 \n",
      "Epoch 85: >>>>> loss: 5.471790790557861 \n",
      "Epoch 86: >>>>> loss: 5.410820484161377 \n",
      "Epoch 87: >>>>> loss: 5.405794620513916 \n",
      "Epoch 88: >>>>> loss: 5.45089054107666 \n",
      "Epoch 89: >>>>> loss: 5.363800525665283 \n",
      "Epoch 90: >>>>> loss: 5.4308881759643555 \n",
      "Epoch 91: >>>>> loss: 5.399996757507324 \n",
      "Epoch 92: >>>>> loss: 5.3713788986206055 \n",
      "Epoch 93: >>>>> loss: 5.425073623657227 \n",
      "Epoch 94: >>>>> loss: 5.460428237915039 \n",
      "Epoch 95: >>>>> loss: 5.312674045562744 \n",
      "Epoch 96: >>>>> loss: 5.43826961517334 \n",
      "Epoch 97: >>>>> loss: 5.366926193237305 \n",
      "Epoch 98: >>>>> loss: 5.411343097686768 \n",
      "Epoch 99: >>>>> loss: 5.35904598236084 \n",
      "Epoch 100: >>>>> loss: 5.466099262237549 \n",
      "Epoch 101: >>>>> loss: 5.42549991607666 \n",
      "Epoch 102: >>>>> loss: 5.392017364501953 \n",
      "Epoch 103: >>>>> loss: 5.382325649261475 \n",
      "Epoch 104: >>>>> loss: 5.372183799743652 \n",
      "Epoch 105: >>>>> loss: 5.518193244934082 \n",
      "Epoch 106: >>>>> loss: 5.4155988693237305 \n",
      "Epoch 107: >>>>> loss: 5.33725643157959 \n",
      "Epoch 108: >>>>> loss: 5.415896415710449 \n",
      "Epoch 109: >>>>> loss: 5.39734411239624 \n",
      "Epoch 110: >>>>> loss: 5.474922180175781 \n",
      "Epoch 111: >>>>> loss: 5.3386149406433105 \n",
      "Epoch 112: >>>>> loss: 5.490223407745361 \n",
      "Epoch 113: >>>>> loss: 5.3941216468811035 \n",
      "Epoch 114: >>>>> loss: 5.43840217590332 \n",
      "Epoch 115: >>>>> loss: 5.374346733093262 \n",
      "Epoch 116: >>>>> loss: 5.4424214363098145 \n",
      "Epoch 117: >>>>> loss: 5.379243850708008 \n",
      "Epoch 118: >>>>> loss: 5.390134811401367 \n",
      "Epoch 119: >>>>> loss: 5.363188743591309 \n",
      "Epoch 120: >>>>> loss: 5.389533519744873 \n",
      "Epoch 121: >>>>> loss: 5.326759338378906 \n",
      "Epoch 122: >>>>> loss: 5.3436970710754395 \n",
      "Epoch 123: >>>>> loss: 5.412759304046631 \n",
      "Epoch 124: >>>>> loss: 5.353524208068848 \n",
      "Epoch 125: >>>>> loss: 5.409511566162109 \n",
      "Epoch 126: >>>>> loss: 5.424893379211426 \n",
      "Epoch 127: >>>>> loss: 5.420694351196289 \n",
      "Epoch 128: >>>>> loss: 5.377518653869629 \n",
      "Epoch 129: >>>>> loss: 5.334200382232666 \n",
      "Epoch 130: >>>>> loss: 5.453240394592285 \n",
      "Epoch 131: >>>>> loss: 5.397165298461914 \n",
      "Epoch 132: >>>>> loss: 5.383415699005127 \n",
      "Epoch 133: >>>>> loss: 5.4143829345703125 \n",
      "Epoch 134: >>>>> loss: 5.328701019287109 \n",
      "Epoch 135: >>>>> loss: 5.537214756011963 \n",
      "Epoch 136: >>>>> loss: 5.302202224731445 \n",
      "Epoch 137: >>>>> loss: 5.403923034667969 \n",
      "Epoch 138: >>>>> loss: 5.368625640869141 \n",
      "Epoch 139: >>>>> loss: 5.381224632263184 \n",
      "Epoch 140: >>>>> loss: 5.430750846862793 \n",
      "Epoch 141: >>>>> loss: 5.561720848083496 \n",
      "Epoch 142: >>>>> loss: 5.337033271789551 \n",
      "Epoch 143: >>>>> loss: 5.316046714782715 \n",
      "Epoch 144: >>>>> loss: 5.424704551696777 \n",
      "Epoch 145: >>>>> loss: 5.363620758056641 \n",
      "Epoch 146: >>>>> loss: 5.341819763183594 \n",
      "Epoch 147: >>>>> loss: 5.3471999168396 \n",
      "Epoch 148: >>>>> loss: 5.483359336853027 \n",
      "Epoch 149: >>>>> loss: 5.382869243621826 \n",
      "Epoch 150: >>>>> loss: 5.417361259460449 \n",
      "Epoch 151: >>>>> loss: 5.446449279785156 \n",
      "Epoch 152: >>>>> loss: 5.364577293395996 \n",
      "Epoch 153: >>>>> loss: 5.356358528137207 \n",
      "Epoch 154: >>>>> loss: 5.368660926818848 \n",
      "Epoch 155: >>>>> loss: 5.431149005889893 \n",
      "Epoch 156: >>>>> loss: 5.398085117340088 \n",
      "Epoch 157: >>>>> loss: 5.39349365234375 \n",
      "Epoch 158: >>>>> loss: 5.390025615692139 \n",
      "Epoch 159: >>>>> loss: 5.318150520324707 \n",
      "Epoch 160: >>>>> loss: 5.425975322723389 \n",
      "Epoch 161: >>>>> loss: 5.495209217071533 \n",
      "Epoch 162: >>>>> loss: 5.444474697113037 \n",
      "Epoch 163: >>>>> loss: 5.31610107421875 \n",
      "Epoch 164: >>>>> loss: 5.309630393981934 \n",
      "Epoch 165: >>>>> loss: 5.350848197937012 \n",
      "Epoch 166: >>>>> loss: 5.448596477508545 \n",
      "Epoch 167: >>>>> loss: 5.351360321044922 \n",
      "Epoch 168: >>>>> loss: 5.366137981414795 \n",
      "Epoch 169: >>>>> loss: 5.335361003875732 \n",
      "Epoch 170: >>>>> loss: 5.395259857177734 \n",
      "Epoch 171: >>>>> loss: 5.454854965209961 \n",
      "Epoch 172: >>>>> loss: 5.4216814041137695 \n",
      "Epoch 173: >>>>> loss: 5.42579460144043 \n",
      "Epoch 174: >>>>> loss: 5.489029884338379 \n",
      "Epoch 175: >>>>> loss: 5.35307502746582 \n",
      "Epoch 176: >>>>> loss: 5.347850799560547 \n",
      "Epoch 177: >>>>> loss: 5.312796592712402 \n",
      "Epoch 178: >>>>> loss: 5.338391304016113 \n",
      "Epoch 179: >>>>> loss: 5.388819694519043 \n",
      "Epoch 180: >>>>> loss: 5.449267387390137 \n",
      "Epoch 181: >>>>> loss: 5.423703670501709 \n",
      "Epoch 182: >>>>> loss: 5.467480659484863 \n",
      "Epoch 183: >>>>> loss: 5.487425804138184 \n",
      "Epoch 184: >>>>> loss: 5.405675888061523 \n",
      "Epoch 185: >>>>> loss: 5.276802062988281 \n",
      "Epoch 186: >>>>> loss: 5.310805320739746 \n",
      "Epoch 187: >>>>> loss: 5.284457206726074 \n",
      "Epoch 188: >>>>> loss: 5.375916004180908 \n",
      "Epoch 189: >>>>> loss: 5.3413262367248535 \n",
      "Epoch 190: >>>>> loss: 5.417843818664551 \n",
      "Epoch 191: >>>>> loss: 5.519951820373535 \n",
      "Epoch 192: >>>>> loss: 5.288862228393555 \n",
      "Epoch 193: >>>>> loss: 5.385916233062744 \n",
      "Epoch 194: >>>>> loss: 5.400965213775635 \n",
      "Epoch 195: >>>>> loss: 5.435720920562744 \n",
      "Epoch 196: >>>>> loss: 5.392587184906006 \n",
      "Epoch 197: >>>>> loss: 5.297801494598389 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198: >>>>> loss: 5.436718940734863 \n",
      "Epoch 199: >>>>> loss: 5.468601226806641 \n",
      "Epoch 200: >>>>> loss: 5.348794460296631 \n",
      "Epoch 201: >>>>> loss: 5.438653945922852 \n",
      "Epoch 202: >>>>> loss: 5.2752156257629395 \n",
      "Epoch 203: >>>>> loss: 5.3263959884643555 \n",
      "Epoch 204: >>>>> loss: 5.309413909912109 \n",
      "Epoch 205: >>>>> loss: 5.379946231842041 \n",
      "Epoch 206: >>>>> loss: 5.302030086517334 \n",
      "Epoch 207: >>>>> loss: 5.317744731903076 \n",
      "Epoch 208: >>>>> loss: 5.540444374084473 \n",
      "Epoch 209: >>>>> loss: 5.302358627319336 \n",
      "Epoch 210: >>>>> loss: 5.38538122177124 \n",
      "Epoch 211: >>>>> loss: 5.415989875793457 \n",
      "Epoch 212: >>>>> loss: 5.389938831329346 \n",
      "Epoch 213: >>>>> loss: 5.369144916534424 \n",
      "Epoch 214: >>>>> loss: 5.40007209777832 \n",
      "Epoch 215: >>>>> loss: 5.3577117919921875 \n",
      "Epoch 216: >>>>> loss: 5.3353495597839355 \n",
      "Epoch 217: >>>>> loss: 5.280599117279053 \n",
      "Epoch 218: >>>>> loss: 5.322994709014893 \n",
      "Epoch 219: >>>>> loss: 5.307888031005859 \n",
      "Epoch 220: >>>>> loss: 5.273429870605469 \n",
      "Epoch 221: >>>>> loss: 5.523187160491943 \n",
      "Epoch 222: >>>>> loss: 5.316158294677734 \n",
      "Epoch 223: >>>>> loss: 5.30739688873291 \n",
      "Epoch 224: >>>>> loss: 5.478218078613281 \n",
      "Epoch 225: >>>>> loss: 5.374216556549072 \n",
      "Epoch 226: >>>>> loss: 5.316701889038086 \n",
      "Epoch 227: >>>>> loss: 5.304981231689453 \n",
      "Epoch 228: >>>>> loss: 5.301668643951416 \n",
      "Epoch 229: >>>>> loss: 5.379645347595215 \n",
      "Epoch 230: >>>>> loss: 5.37502908706665 \n",
      "Epoch 231: >>>>> loss: 5.247328758239746 \n",
      "Epoch 232: >>>>> loss: 5.28000020980835 \n",
      "Epoch 233: >>>>> loss: 5.397502422332764 \n",
      "Epoch 234: >>>>> loss: 5.30181360244751 \n",
      "Epoch 235: >>>>> loss: 5.351040840148926 \n",
      "Epoch 236: >>>>> loss: 5.317880630493164 \n",
      "Epoch 237: >>>>> loss: 5.322956085205078 \n",
      "Epoch 238: >>>>> loss: 5.331319808959961 \n",
      "Epoch 239: >>>>> loss: 5.356069087982178 \n",
      "Epoch 240: >>>>> loss: 5.417614459991455 \n",
      "Epoch 241: >>>>> loss: 5.460001468658447 \n",
      "Epoch 242: >>>>> loss: 5.525623321533203 \n",
      "Epoch 243: >>>>> loss: 5.261363983154297 \n",
      "Epoch 244: >>>>> loss: 5.332686424255371 \n",
      "Epoch 245: >>>>> loss: 5.333426475524902 \n",
      "Epoch 246: >>>>> loss: 5.456291675567627 \n",
      "Epoch 247: >>>>> loss: 5.335970401763916 \n",
      "Epoch 248: >>>>> loss: 5.456709861755371 \n",
      "Epoch 249: >>>>> loss: 5.377648830413818 \n",
      "Epoch 250: >>>>> loss: 5.388959884643555 \n",
      "Epoch 251: >>>>> loss: 5.408688545227051 \n",
      "Epoch 252: >>>>> loss: 5.2625274658203125 \n",
      "Epoch 253: >>>>> loss: 5.2493133544921875 \n",
      "Epoch 254: >>>>> loss: 5.244240760803223 \n",
      "Epoch 255: >>>>> loss: 5.422647476196289 \n",
      "Epoch 256: >>>>> loss: 5.340724945068359 \n",
      "Epoch 257: >>>>> loss: 5.328279495239258 \n",
      "Epoch 258: >>>>> loss: 5.192783355712891 \n",
      "Epoch 259: >>>>> loss: 5.3253302574157715 \n",
      "Epoch 260: >>>>> loss: 5.335891246795654 \n",
      "Epoch 261: >>>>> loss: 5.278528213500977 \n",
      "Epoch 262: >>>>> loss: 5.45313835144043 \n",
      "Epoch 263: >>>>> loss: 5.3010101318359375 \n",
      "Epoch 264: >>>>> loss: 5.38567590713501 \n",
      "Epoch 265: >>>>> loss: 5.353534698486328 \n",
      "Epoch 266: >>>>> loss: 5.3112640380859375 \n",
      "Epoch 267: >>>>> loss: 5.4048004150390625 \n",
      "Epoch 268: >>>>> loss: 5.3034796714782715 \n",
      "Epoch 269: >>>>> loss: 5.210728168487549 \n",
      "Epoch 270: >>>>> loss: 5.387749195098877 \n",
      "Epoch 271: >>>>> loss: 5.389432907104492 \n",
      "Epoch 272: >>>>> loss: 5.300186634063721 \n",
      "Epoch 273: >>>>> loss: 5.321993350982666 \n",
      "Epoch 274: >>>>> loss: 5.478887557983398 \n",
      "Epoch 275: >>>>> loss: 5.297743320465088 \n",
      "Epoch 276: >>>>> loss: 5.35661506652832 \n",
      "Epoch 277: >>>>> loss: 5.427038192749023 \n",
      "Epoch 278: >>>>> loss: 5.27342414855957 \n",
      "Epoch 279: >>>>> loss: 5.306515693664551 \n",
      "Epoch 280: >>>>> loss: 5.306368350982666 \n",
      "Epoch 281: >>>>> loss: 5.352190971374512 \n",
      "Epoch 282: >>>>> loss: 5.282721996307373 \n",
      "Epoch 283: >>>>> loss: 5.342728137969971 \n",
      "Epoch 284: >>>>> loss: 5.209558486938477 \n",
      "Epoch 285: >>>>> loss: 5.220401287078857 \n",
      "Epoch 286: >>>>> loss: 5.42106819152832 \n",
      "Epoch 287: >>>>> loss: 5.1943488121032715 \n",
      "Epoch 288: >>>>> loss: 5.325942039489746 \n",
      "Epoch 289: >>>>> loss: 5.2353410720825195 \n",
      "Epoch 290: >>>>> loss: 5.224403381347656 \n",
      "Epoch 291: >>>>> loss: 5.261226177215576 \n",
      "Epoch 292: >>>>> loss: 5.30769157409668 \n",
      "Epoch 293: >>>>> loss: 5.424317836761475 \n",
      "Epoch 294: >>>>> loss: 5.43418550491333 \n",
      "Epoch 295: >>>>> loss: 5.290602684020996 \n",
      "Epoch 296: >>>>> loss: 5.262045383453369 \n",
      "Epoch 297: >>>>> loss: 5.267845153808594 \n",
      "Epoch 298: >>>>> loss: 5.373356819152832 \n",
      "Epoch 299: >>>>> loss: 5.177335262298584 \n",
      "Epoch 300: >>>>> loss: 5.288317680358887 \n",
      "Epoch 301: >>>>> loss: 5.34812068939209 \n",
      "Epoch 302: >>>>> loss: 5.201122760772705 \n",
      "Epoch 303: >>>>> loss: 5.288670539855957 \n",
      "Epoch 304: >>>>> loss: 5.305776596069336 \n",
      "Epoch 305: >>>>> loss: 5.398435115814209 \n",
      "Epoch 306: >>>>> loss: 5.46513032913208 \n",
      "Epoch 307: >>>>> loss: 5.469550132751465 \n",
      "Epoch 308: >>>>> loss: 5.322686195373535 \n",
      "Epoch 309: >>>>> loss: 5.223207950592041 \n",
      "Epoch 310: >>>>> loss: 5.201072692871094 \n",
      "Epoch 311: >>>>> loss: 5.250332355499268 \n",
      "Epoch 312: >>>>> loss: 5.360912322998047 \n",
      "Epoch 313: >>>>> loss: 5.228113174438477 \n",
      "Epoch 314: >>>>> loss: 5.269375801086426 \n",
      "Epoch 315: >>>>> loss: 5.335694313049316 \n",
      "Epoch 316: >>>>> loss: 5.325864315032959 \n",
      "Epoch 317: >>>>> loss: 5.396401405334473 \n",
      "Epoch 318: >>>>> loss: 5.3828935623168945 \n",
      "Epoch 319: >>>>> loss: 5.3117780685424805 \n",
      "Epoch 320: >>>>> loss: 5.382936477661133 \n",
      "Epoch 321: >>>>> loss: 5.382139205932617 \n",
      "Epoch 322: >>>>> loss: 5.402709007263184 \n",
      "Epoch 323: >>>>> loss: 5.360274314880371 \n",
      "Epoch 324: >>>>> loss: 5.1827287673950195 \n",
      "Epoch 325: >>>>> loss: 5.27409553527832 \n",
      "Epoch 326: >>>>> loss: 5.345770835876465 \n",
      "Epoch 327: >>>>> loss: 5.283784866333008 \n",
      "Epoch 328: >>>>> loss: 5.441997528076172 \n",
      "Epoch 329: >>>>> loss: 5.2982892990112305 \n",
      "Epoch 330: >>>>> loss: 5.39454460144043 \n",
      "Epoch 331: >>>>> loss: 5.3849077224731445 \n",
      "Epoch 332: >>>>> loss: 5.282311916351318 \n",
      "Epoch 333: >>>>> loss: 5.327722549438477 \n",
      "Epoch 334: >>>>> loss: 5.242415428161621 \n",
      "Epoch 335: >>>>> loss: 5.164021015167236 \n",
      "Epoch 336: >>>>> loss: 5.30415153503418 \n",
      "Epoch 337: >>>>> loss: 5.17204475402832 \n",
      "Epoch 338: >>>>> loss: 5.19824743270874 \n",
      "Epoch 339: >>>>> loss: 5.346578121185303 \n",
      "Epoch 340: >>>>> loss: 5.24738883972168 \n",
      "Epoch 341: >>>>> loss: 5.401309967041016 \n",
      "Epoch 342: >>>>> loss: 5.3784284591674805 \n",
      "Epoch 343: >>>>> loss: 5.214016914367676 \n",
      "Epoch 344: >>>>> loss: 5.152059555053711 \n",
      "Epoch 345: >>>>> loss: 5.309399604797363 \n",
      "Epoch 346: >>>>> loss: 5.212297439575195 \n",
      "Epoch 347: >>>>> loss: 5.496969699859619 \n",
      "Epoch 348: >>>>> loss: 5.3175578117370605 \n",
      "Epoch 349: >>>>> loss: 5.365839004516602 \n",
      "Epoch 350: >>>>> loss: 5.269561767578125 \n",
      "Epoch 351: >>>>> loss: 5.166640281677246 \n",
      "Epoch 352: >>>>> loss: 5.262882709503174 \n",
      "Epoch 353: >>>>> loss: 5.292431831359863 \n",
      "Epoch 354: >>>>> loss: 5.439839839935303 \n",
      "Epoch 355: >>>>> loss: 5.249904632568359 \n",
      "Epoch 356: >>>>> loss: 5.336002349853516 \n",
      "Epoch 357: >>>>> loss: 5.235507965087891 \n",
      "Epoch 358: >>>>> loss: 5.258472442626953 \n",
      "Epoch 359: >>>>> loss: 5.16172456741333 \n",
      "Epoch 360: >>>>> loss: 5.162015914916992 \n",
      "Epoch 361: >>>>> loss: 5.154791831970215 \n",
      "Epoch 362: >>>>> loss: 5.313690662384033 \n",
      "Epoch 363: >>>>> loss: 5.3419671058654785 \n",
      "Epoch 364: >>>>> loss: 5.28675651550293 \n",
      "Epoch 365: >>>>> loss: 5.419455528259277 \n",
      "Epoch 366: >>>>> loss: 5.271859169006348 \n",
      "Epoch 367: >>>>> loss: 5.344605445861816 \n",
      "Epoch 368: >>>>> loss: 5.274445533752441 \n",
      "Epoch 369: >>>>> loss: 5.382620334625244 \n",
      "Epoch 370: >>>>> loss: 5.221802711486816 \n",
      "Epoch 371: >>>>> loss: 5.275618553161621 \n",
      "Epoch 372: >>>>> loss: 5.4082746505737305 \n",
      "Epoch 373: >>>>> loss: 5.148099899291992 \n",
      "Epoch 374: >>>>> loss: 5.387424468994141 \n",
      "Epoch 375: >>>>> loss: 5.293212413787842 \n",
      "Epoch 376: >>>>> loss: 5.168846607208252 \n",
      "Epoch 377: >>>>> loss: 5.125445365905762 \n",
      "Epoch 378: >>>>> loss: 5.255928993225098 \n",
      "Epoch 379: >>>>> loss: 5.185028076171875 \n",
      "Epoch 380: >>>>> loss: 5.351734638214111 \n",
      "Epoch 381: >>>>> loss: 5.311619758605957 \n",
      "Epoch 382: >>>>> loss: 5.157362461090088 \n",
      "Epoch 383: >>>>> loss: 5.312432289123535 \n",
      "Epoch 384: >>>>> loss: 5.4048871994018555 \n",
      "Epoch 385: >>>>> loss: 5.24211311340332 \n",
      "Epoch 386: >>>>> loss: 5.240382671356201 \n",
      "Epoch 387: >>>>> loss: 5.353519439697266 \n",
      "Epoch 388: >>>>> loss: 5.322700023651123 \n",
      "Epoch 389: >>>>> loss: 5.324179649353027 \n",
      "Epoch 390: >>>>> loss: 5.133244514465332 \n",
      "Epoch 391: >>>>> loss: 5.417233467102051 \n",
      "Epoch 392: >>>>> loss: 5.244149208068848 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 393: >>>>> loss: 5.375254154205322 \n",
      "Epoch 394: >>>>> loss: 5.28487491607666 \n",
      "Epoch 395: >>>>> loss: 5.433322906494141 \n",
      "Epoch 396: >>>>> loss: 5.257129192352295 \n",
      "Epoch 397: >>>>> loss: 5.119325637817383 \n",
      "Epoch 398: >>>>> loss: 5.324708938598633 \n",
      "Epoch 399: >>>>> loss: 5.350872039794922 \n",
      "Epoch 400: >>>>> loss: 5.150448799133301 \n",
      "Epoch 401: >>>>> loss: 5.226493835449219 \n",
      "Epoch 402: >>>>> loss: 5.262145042419434 \n",
      "Epoch 403: >>>>> loss: 5.314591884613037 \n",
      "Epoch 404: >>>>> loss: 5.325690269470215 \n",
      "Epoch 405: >>>>> loss: 5.153660774230957 \n",
      "Epoch 406: >>>>> loss: 5.123804092407227 \n",
      "Epoch 407: >>>>> loss: 5.399303436279297 \n",
      "Epoch 408: >>>>> loss: 5.118254661560059 \n",
      "Epoch 409: >>>>> loss: 5.20853328704834 \n",
      "Epoch 410: >>>>> loss: 5.354971885681152 \n",
      "Epoch 411: >>>>> loss: 5.318849563598633 \n",
      "Epoch 412: >>>>> loss: 5.17145299911499 \n",
      "Epoch 413: >>>>> loss: 5.284162521362305 \n",
      "Epoch 414: >>>>> loss: 5.380411624908447 \n",
      "Epoch 415: >>>>> loss: 5.462703704833984 \n",
      "Epoch 416: >>>>> loss: 5.126431465148926 \n",
      "Epoch 417: >>>>> loss: 5.089278221130371 \n",
      "Epoch 418: >>>>> loss: 5.330816745758057 \n",
      "Epoch 419: >>>>> loss: 5.437046051025391 \n",
      "Epoch 420: >>>>> loss: 5.092885494232178 \n",
      "Epoch 421: >>>>> loss: 5.130681991577148 \n",
      "Epoch 422: >>>>> loss: 5.245985984802246 \n",
      "Epoch 423: >>>>> loss: 5.296109199523926 \n",
      "Epoch 424: >>>>> loss: 5.401235580444336 \n",
      "Epoch 425: >>>>> loss: 5.263257026672363 \n",
      "Epoch 426: >>>>> loss: 5.128757476806641 \n",
      "Epoch 427: >>>>> loss: 5.265594005584717 \n",
      "Epoch 428: >>>>> loss: 5.069619178771973 \n",
      "Epoch 429: >>>>> loss: 5.269775390625 \n",
      "Epoch 430: >>>>> loss: 5.212403297424316 \n",
      "Epoch 431: >>>>> loss: 5.116191864013672 \n",
      "Epoch 432: >>>>> loss: 5.359030723571777 \n",
      "Epoch 433: >>>>> loss: 5.376911163330078 \n",
      "Epoch 434: >>>>> loss: 5.329807758331299 \n",
      "Epoch 435: >>>>> loss: 5.329686164855957 \n",
      "Epoch 436: >>>>> loss: 5.405241966247559 \n",
      "Epoch 437: >>>>> loss: 5.167466640472412 \n",
      "Epoch 438: >>>>> loss: 5.300922393798828 \n",
      "Epoch 439: >>>>> loss: 5.303289413452148 \n",
      "Epoch 440: >>>>> loss: 5.3595685958862305 \n",
      "Epoch 441: >>>>> loss: 5.220956802368164 \n",
      "Epoch 442: >>>>> loss: 5.379058361053467 \n",
      "Epoch 443: >>>>> loss: 5.122806549072266 \n",
      "Epoch 444: >>>>> loss: 5.2899627685546875 \n",
      "Epoch 445: >>>>> loss: 5.225156784057617 \n",
      "Epoch 446: >>>>> loss: 5.260004043579102 \n",
      "Epoch 447: >>>>> loss: 5.324767112731934 \n",
      "Epoch 448: >>>>> loss: 5.3042449951171875 \n",
      "Epoch 449: >>>>> loss: 5.263547897338867 \n",
      "Epoch 450: >>>>> loss: 5.088199615478516 \n",
      "Epoch 451: >>>>> loss: 5.159851551055908 \n",
      "Epoch 452: >>>>> loss: 5.067445755004883 \n",
      "Epoch 453: >>>>> loss: 5.328011512756348 \n",
      "Epoch 454: >>>>> loss: 5.391849994659424 \n",
      "Epoch 455: >>>>> loss: 5.339685916900635 \n",
      "Epoch 456: >>>>> loss: 5.260204792022705 \n",
      "Epoch 457: >>>>> loss: 5.1448235511779785 \n",
      "Epoch 458: >>>>> loss: 5.345623970031738 \n",
      "Epoch 459: >>>>> loss: 5.176474571228027 \n",
      "Epoch 460: >>>>> loss: 5.2320098876953125 \n",
      "Epoch 461: >>>>> loss: 5.258484363555908 \n",
      "Epoch 462: >>>>> loss: 5.243337631225586 \n",
      "Epoch 463: >>>>> loss: 5.227924346923828 \n",
      "Epoch 464: >>>>> loss: 5.355869293212891 \n",
      "Epoch 465: >>>>> loss: 5.137507438659668 \n",
      "Epoch 466: >>>>> loss: 5.089493751525879 \n",
      "Epoch 467: >>>>> loss: 5.282493591308594 \n",
      "Epoch 468: >>>>> loss: 5.163789749145508 \n",
      "Epoch 469: >>>>> loss: 5.324480056762695 \n",
      "Epoch 470: >>>>> loss: 5.211177349090576 \n",
      "Epoch 471: >>>>> loss: 5.122814178466797 \n",
      "Epoch 472: >>>>> loss: 5.086686611175537 \n",
      "Epoch 473: >>>>> loss: 5.324438095092773 \n",
      "Epoch 474: >>>>> loss: 5.231592178344727 \n",
      "Epoch 475: >>>>> loss: 5.235753059387207 \n",
      "Epoch 476: >>>>> loss: 5.119772911071777 \n",
      "Epoch 477: >>>>> loss: 5.285299301147461 \n",
      "Epoch 478: >>>>> loss: 5.288354873657227 \n",
      "Epoch 479: >>>>> loss: 5.391994476318359 \n",
      "Epoch 480: >>>>> loss: 5.090746879577637 \n",
      "Epoch 481: >>>>> loss: 5.111443042755127 \n",
      "Epoch 482: >>>>> loss: 5.086294651031494 \n",
      "Epoch 483: >>>>> loss: 5.383229732513428 \n",
      "Epoch 484: >>>>> loss: 5.329672336578369 \n",
      "Epoch 485: >>>>> loss: 5.24393892288208 \n",
      "Epoch 486: >>>>> loss: 5.295078277587891 \n",
      "Epoch 487: >>>>> loss: 5.365976333618164 \n",
      "Epoch 488: >>>>> loss: 5.146535873413086 \n",
      "Epoch 489: >>>>> loss: 5.282398700714111 \n",
      "Epoch 490: >>>>> loss: 5.215519905090332 \n",
      "Epoch 491: >>>>> loss: 5.242635726928711 \n",
      "Epoch 492: >>>>> loss: 5.191767692565918 \n",
      "Epoch 493: >>>>> loss: 5.119231224060059 \n",
      "Epoch 494: >>>>> loss: 5.3048810958862305 \n",
      "Epoch 495: >>>>> loss: 5.366188049316406 \n",
      "Epoch 496: >>>>> loss: 5.421483039855957 \n",
      "Epoch 497: >>>>> loss: 5.147263050079346 \n",
      "Epoch 498: >>>>> loss: 5.327655792236328 \n",
      "Epoch 499: >>>>> loss: 5.425464630126953 \n",
      "Epoch 500: >>>>> loss: 5.054295063018799 \n"
     ]
    }
   ],
   "source": [
    "def pretrain(model,train_loader,data_aug,optimizer,criterion,end,start=1,parallel=True):\n",
    "\n",
    "    # Check device setting\n",
    "    if parallel == True:\n",
    "        print('GPU')\n",
    "        model = model.to(device)\n",
    "        data_aug = data_aug.to(device)\n",
    "\n",
    "    else:\n",
    "        print('CPU')\n",
    "        model = model.cpu()\n",
    "\n",
    "    print('Start Training')\n",
    "    record = {'train':[],'validation':[]}\n",
    "    i = start\n",
    "\n",
    "    #Loop\n",
    "    while i <= end:\n",
    "        print(f\"Epoch {i}: \", end='')\n",
    "        for b, (X, y) in enumerate(train_loader):\n",
    "            \n",
    "            if parallel == True:\n",
    "                X = X.to(device)\n",
    "                y = y.to(device)\n",
    "                \n",
    "            print(f\">\", end='')\n",
    "            optimizer.zero_grad()\n",
    "            batch_size = X.shape[0]\n",
    "            # we concatenate the two verison of X at dim0\n",
    "            X = torch.cat(data_aug(X), dim=0)\n",
    "            logits = model(X)\n",
    "            # we split logit with batch size so we get back the two encoded  data\n",
    "            logits = torch.split(logits, [batch_size, batch_size], dim=0)\n",
    "            # We concatenate the two version at dim1 \n",
    "            logits = torch.cat((logits[0].unsqueeze(1),logits[1].unsqueeze(1)),dim=1)\n",
    "            loss = criterion(logits,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # One epoch completed\n",
    "        l = loss.tolist()\n",
    "        record['train'].append(l)\n",
    "        print(f' loss: {l} ')\n",
    "        i += 1\n",
    "        del X,y,logits\n",
    "\n",
    "    model = model.cpu()\n",
    "    return model,record\n",
    "\n",
    "pretrain_model, record = pretrain(pretrain_model,train_loader,data_aug,optimizer,criterion,pre_train_epochs,start=1,parallel=parallel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record logging and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import make_directory\n",
    "\n",
    "def record_log(mode,epochs,record,cmtx=None,cls=None):\n",
    "    if mode == 'pretrain':\n",
    "        path = make_directory(exp_name+'_pretrain',epoch=epochs,filepath=PATH+'/record/')\n",
    "        pd.DataFrame(record['train'],columns=['train_loss']).to_csv(path+'_loss.csv')\n",
    "    elif mode == 'finetuning':\n",
    "        path = make_directory(exp_name+'_finetuning',epoch=epochs,filepath=PATH+'/record/')\n",
    "        pd.DataFrame(record['train'],columns=['train_loss']).to_csv(path+'_loss.csv')\n",
    "        pd.DataFrame(record['validation'],columns=['validation_accuracy']).to_csv(path+'_accuracy.csv')\n",
    "        cls.to_csv(path+'_report.csv')\n",
    "        cmtx.to_csv(path+'_cmtx.csv')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_log('pretrain',pre_train_epochs,record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import save_checkpoint\n",
    "\n",
    "def save(mode,model,optimizer,epochs):\n",
    "    if mode == 'pretrain':\n",
    "        path = make_directory(exp_name+'_pretrain',epoch=epochs,filepath=PATH+'/models/saved_models/')\n",
    "        save_checkpoint(model, optimizer, epochs, path)\n",
    "    elif mode == 'finetuning':\n",
    "        path = make_directory(exp_name+'_finetuning',epoch=epochs,filepath=PATH+'/models/saved_models/')\n",
    "        save_checkpoint(model, optimizer, epochs, path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save('pretrain',pretrain_model,optimizer,pre_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del criterion, optimizer, record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finetuning, we only take the encoder from the pretrain model, freeze the encoder and build a classifier (512>64>7) upon it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ED_module, Classifier\n",
    "\n",
    "def freeze_network(model):\n",
    "    for _, p in model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def create_finetune_model(enc=None):\n",
    "    # External libraries required\n",
    "    if enc == None:\n",
    "        enc = Encoder([64,128,512])\n",
    "    else:\n",
    "        enc = freeze_network(enc)\n",
    "    clf = Classifier(512,64,7)\n",
    "    model = ED_module(encoder=enc,decoder=clf)\n",
    "    return model\n",
    "\n",
    "\n",
    "# finetune_model = create_finetune_model(None)\n",
    "finetune_model = create_finetune_model(pretrain_model.encoder)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = create_optimizer('finetuning',finetune_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [128, 64, 31, 31]           1,664\n",
      "       BatchNorm2d-2          [128, 64, 31, 31]             128\n",
      "              ReLU-3          [128, 64, 31, 31]               0\n",
      "         MaxPool2d-4          [128, 64, 15, 15]               0\n",
      "            Conv2d-5           [128, 128, 7, 7]          73,856\n",
      "       BatchNorm2d-6           [128, 128, 7, 7]             256\n",
      "              ReLU-7           [128, 128, 7, 7]               0\n",
      "            Lambda-8           [128, 128, 7, 7]               0\n",
      "            Conv2d-9           [128, 512, 3, 3]         262,656\n",
      "           Lambda-10           [128, 512, 3, 3]               0\n",
      "             Tanh-11           [128, 512, 3, 3]               0\n",
      "        AvgPool2d-12           [128, 512, 1, 1]               0\n",
      "          Encoder-13                 [128, 512]               0\n",
      "           Linear-14                  [128, 64]          32,832\n",
      "           Linear-15                   [128, 7]             455\n",
      "       Classifier-16                   [128, 7]               0\n",
      "================================================================\n",
      "Total params: 371,847\n",
      "Trainable params: 371,847\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 2.06\n",
      "Forward/backward pass size (MB): 233.33\n",
      "Params size (MB): 1.42\n",
      "Estimated Total Size (MB): 236.81\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(finetune_model,(1,*enlarge_size),batch_size=bsz,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def short_evaluation(model,data_aug,test_loader,parallel=True):\n",
    "    # copy the model to cpu\n",
    "    if parallel == True:\n",
    "        model = model.cpu()\n",
    "        data_aug = data_aug.cpu()\n",
    "    with torch.no_grad():\n",
    "        for X_test, y_test in test_loader:\n",
    "            X_test,_ = data_aug(X_test)\n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val, 1)[1]\n",
    "            acc = accuracy_score(y_test.view(-1), predicted.view(-1))\n",
    "    # send model back to gpu\n",
    "    if parallel == True:\n",
    "        model = model.cuda()\n",
    "    return acc\n",
    "\n",
    "\n",
    "def finetuning(model, data_aug, train_loader, criterion, optimizer, end, start = 1, test_loader = None, parallel = None, **kwargs):\n",
    "\n",
    "    # Check device setting\n",
    "    if parallel == True:\n",
    "        print('GPU')\n",
    "        model = model.to(device)\n",
    "        data_aug = data_aug.to(device)\n",
    "    else:\n",
    "        print('CPU')\n",
    "\n",
    "    print('Start Training')\n",
    "    record = {'train':[],'validation':[]}\n",
    "    i = start\n",
    "    #Loop\n",
    "    while i <= end:\n",
    "        print(f\"Epoch {i}: \", end='')\n",
    "        for b, (X_train, y_train) in enumerate(train_loader):\n",
    "\n",
    "            if parallel == True:\n",
    "                X_train = X_train.cuda() #.to(device)\n",
    "\n",
    "            X_train,_ = data_aug(X_train)\n",
    "\n",
    "            print(f\">\", end='')\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_train)\n",
    "\n",
    "            if parallel == True:\n",
    "                X_train = X_train.cpu()\n",
    "                del X_train\n",
    "                y_train = y_train.cuda()\n",
    "\n",
    "            loss   = criterion(y_pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if parallel == True:\n",
    "                y_pred = y_pred.cpu()\n",
    "                y_train = y_train.cpu()\n",
    "                del y_pred,y_train\n",
    "\n",
    "        # One epoch completed\n",
    "        loss = loss.tolist()\n",
    "        record['train'].append(loss)\n",
    "        print(f' loss: {loss} ',end='')\n",
    "        if (test_loader != None) and i%10 ==0 :\n",
    "            acc = short_evaluation(model,data_aug,test_loader,parallel)\n",
    "            record['validation'].append(acc)\n",
    "            print(f' accuracy: {acc}')\n",
    "        else:\n",
    "            print('')\n",
    "        i += 1\n",
    "\n",
    "    model = model.cpu()\n",
    "    return model, record\n",
    "\n",
    "finetune_model, record = finetuning(finetune_model , data_aug, train_loader, criterion, optimizer, fine_tune_epochs, 1, test_loader, parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import evaluation\n",
    "\n",
    "cmtx,cls = evaluation(finetune_model,test_loader,label_encoder=lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_log('finetuning',fine_tune_epochs,record,cmtx,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('finetuning',finetune_model,optimizer,fine_tune_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "Start Training\n",
      "Epoch 1: >>>>> loss: 1.1421394348144531 \n",
      "Epoch 2: >>>>> loss: 1.09224271774292 \n",
      "Epoch 3: >>>>> loss: 1.2326371669769287 \n",
      "Epoch 4: >>>>> loss: 1.1167579889297485 \n",
      "Epoch 5: >>>>> loss: 1.3625119924545288 \n",
      "Epoch 6: >>>>> loss: 1.1908559799194336 \n",
      "Epoch 7: >>>>> loss: 1.076444387435913 \n",
      "Epoch 8: >>>>> loss: 1.328795075416565 \n",
      "Epoch 9: >>>>> loss: 1.1221295595169067 \n",
      "Epoch 10: >>>>> loss: 1.0692933797836304  accuracy: 0.531578947368421\n",
      "Epoch 11: >>>>> loss: 1.3332382440567017 \n",
      "Epoch 12: >>>>> loss: 1.4822489023208618 \n",
      "Epoch 13: >>>>> loss: 1.3546075820922852 \n",
      "Epoch 14: >>>>> loss: 1.2067177295684814 \n",
      "Epoch 15: >>>>> loss: 1.124380111694336 \n",
      "Epoch 16: >>>>> loss: 0.9782644510269165 \n",
      "Epoch 17: >>>>> loss: 1.0654231309890747 \n",
      "Epoch 18: >>>>> loss: 1.2197911739349365 \n",
      "Epoch 19: >>>>> loss: 1.1384150981903076 \n",
      "Epoch 20: >>>>> loss: 1.2830332517623901  accuracy: 0.48947368421052634\n",
      "Epoch 21: >>>>> loss: 1.2567675113677979 \n",
      "Epoch 22: >>>>> loss: 1.2567341327667236 \n",
      "Epoch 23: >>>>> loss: 0.9989224076271057 \n",
      "Epoch 24: >>>>> loss: 1.2526873350143433 \n",
      "Epoch 25: >>>>> loss: 1.3125991821289062 \n",
      "Epoch 26: >>>>> loss: 1.2423787117004395 \n",
      "Epoch 27: >>>>> loss: 1.1890305280685425 \n",
      "Epoch 28: >>>>> loss: 1.295385718345642 \n",
      "Epoch 29: >>>>> loss: 0.9375067949295044 \n",
      "Epoch 30: >>>>> loss: 1.2672326564788818  accuracy: 0.5894736842105263\n",
      "Epoch 31: >>>>> loss: 1.0484014749526978 \n",
      "Epoch 32: >>>>> loss: 1.303504228591919 \n",
      "Epoch 33: >>>>> loss: 1.0216244459152222 \n",
      "Epoch 34: >>>>> loss: 1.009853720664978 \n",
      "Epoch 35: >>>>> loss: 1.2141685485839844 \n",
      "Epoch 36: >>>>> loss: 0.9603686928749084 \n",
      "Epoch 37: >>>>> loss: 1.2203834056854248 \n",
      "Epoch 38: >>>>> loss: 1.2015302181243896 \n",
      "Epoch 39: >>>>> loss: 0.9321526885032654 \n",
      "Epoch 40: >>>>> loss: 0.9699621796607971  accuracy: 0.6\n",
      "Epoch 41: >>>>> loss: 1.2811079025268555 \n",
      "Epoch 42: >>>>> loss: 1.270830750465393 \n",
      "Epoch 43: >>>>> loss: 1.3051284551620483 \n",
      "Epoch 44: >>>>> loss: 1.15464186668396 \n",
      "Epoch 45: >>>>> loss: 0.9435791969299316 \n",
      "Epoch 46: >>>>> loss: 1.4324842691421509 \n",
      "Epoch 47: >>>>> loss: 1.250821828842163 \n",
      "Epoch 48: >>>>> loss: 1.318145751953125 \n",
      "Epoch 49: >>>>> loss: 1.2744135856628418 \n",
      "Epoch 50: >>>>> loss: 1.0375386476516724  accuracy: 0.5421052631578948\n",
      "Epoch 51: >>>>> loss: 1.24205482006073 \n",
      "Epoch 52: >>>>> loss: 1.1684587001800537 \n",
      "Epoch 53: >>>>> loss: 0.8813132643699646 \n",
      "Epoch 54: >>>>> loss: 1.320703387260437 \n",
      "Epoch 55: >>>>> loss: 0.8157377243041992 \n",
      "Epoch 56: >>>>> loss: 0.8193318843841553 \n",
      "Epoch 57: >>>>> loss: 0.7017003297805786 \n",
      "Epoch 58: >>>>> loss: 1.2310590744018555 \n",
      "Epoch 59: >>>>> loss: 0.7822209000587463 \n",
      "Epoch 60: >>>>> loss: 0.7840062379837036  accuracy: 0.6736842105263158\n",
      "Epoch 61: >>>>> loss: 0.8925004005432129 \n",
      "Epoch 62: >>>>> loss: 0.7651580572128296 \n",
      "Epoch 63: >>>>> loss: 1.1250489950180054 \n",
      "Epoch 64: >>>>> loss: 0.7542058825492859 \n",
      "Epoch 65: >>>>> loss: 0.7253336906433105 \n",
      "Epoch 66: >>>>> loss: 0.748173177242279 \n",
      "Epoch 67: >>>>> loss: 1.253381371498108 \n",
      "Epoch 68: >>>>> loss: 1.1300456523895264 \n",
      "Epoch 69: >>>>> loss: 0.7240027189254761 \n",
      "Epoch 70: >>>>> loss: 0.8040570020675659  accuracy: 0.5578947368421052\n",
      "Epoch 71: >>>>> loss: 1.1332099437713623 \n",
      "Epoch 72: >>>>> loss: 0.7873795628547668 \n",
      "Epoch 73: >>>>> loss: 1.2128980159759521 \n",
      "Epoch 74: >>>>> loss: 0.7117512226104736 \n",
      "Epoch 75: >>>>> loss: 0.710214376449585 \n",
      "Epoch 76: >>>>> loss: 0.7747032046318054 \n",
      "Epoch 77: >>>>> loss: 0.6635488867759705 \n",
      "Epoch 78: >>>>> loss: 1.3928862810134888 \n",
      "Epoch 79: >>>>> loss: 0.7331699132919312 \n",
      "Epoch 80: >>>>> loss: 0.761921226978302  accuracy: 0.6684210526315789\n",
      "Epoch 81: >>>>> loss: 1.3131041526794434 \n",
      "Epoch 82: >>>>> loss: 1.2424544095993042 \n",
      "Epoch 83: >>>>> loss: 0.6691220998764038 \n",
      "Epoch 84: >>>>> loss: 0.7801586985588074 \n",
      "Epoch 85: >>>>> loss: 1.1394712924957275 \n",
      "Epoch 86: >>>>> loss: 0.6975105404853821 \n",
      "Epoch 87: >>>>> loss: 0.6481437087059021 \n",
      "Epoch 88: >>>>> loss: 0.598570704460144 \n",
      "Epoch 89: >>>>> loss: 1.2748981714248657 \n",
      "Epoch 90: >>>>> loss: 1.1985423564910889  accuracy: 0.6631578947368421\n",
      "Epoch 91: >>>>> loss: 1.2932103872299194 \n",
      "Epoch 92: >>>>> loss: 1.2138080596923828 \n",
      "Epoch 93: >>>>> loss: 1.4090758562088013 \n",
      "Epoch 94: >>>>> loss: 1.2903152704238892 \n",
      "Epoch 95: >>>>> loss: 0.626186728477478 \n",
      "Epoch 96: >>>>> loss: 0.7225406765937805 \n",
      "Epoch 97: >>>>> loss: 1.1891311407089233 \n",
      "Epoch 98: >>>>> loss: 1.1337021589279175 \n",
      "Epoch 99: >>>>> loss: 1.3431452512741089 \n",
      "Epoch 100: >>>>> loss: 0.6749253273010254  accuracy: 0.47368421052631576\n",
      "Epoch 101: >>>>> loss: 0.6389788389205933 \n",
      "Epoch 102: >>>>> loss: 0.6290541291236877 \n",
      "Epoch 103: >>>>> loss: 1.3535499572753906 \n",
      "Epoch 104: >>>>> loss: 1.2453460693359375 \n",
      "Epoch 105: >>>>> loss: 1.078289270401001 \n",
      "Epoch 106: >>>>> loss: 1.1773269176483154 \n",
      "Epoch 107: >>>>> loss: 1.245724081993103 \n",
      "Epoch 108: >>>>> loss: 0.5445972084999084 \n",
      "Epoch 109: >>>>> loss: 1.319637417793274 \n",
      "Epoch 110: >>>>> loss: 0.5734077095985413  accuracy: 0.6684210526315789\n",
      "Epoch 111: >>>>> loss: 0.6037281155586243 \n",
      "Epoch 112: >>>>> loss: 0.5897066593170166 \n",
      "Epoch 113: >>>>> loss: 1.3511908054351807 \n",
      "Epoch 114: >>>>> loss: 1.458014726638794 \n",
      "Epoch 115: >>>>> loss: 0.5592190623283386 \n",
      "Epoch 116: >>>>> loss: 1.1499296426773071 \n",
      "Epoch 117: >>>>> loss: 0.513670027256012 \n",
      "Epoch 118: >>>>> loss: 0.5837152004241943 \n",
      "Epoch 119: >>>>> loss: 1.1253976821899414 \n",
      "Epoch 120: >>>>> loss: 1.2897950410842896  accuracy: 0.7263157894736842\n",
      "Epoch 121: >>>>> loss: 1.2435497045516968 \n",
      "Epoch 122: >>>>> loss: 1.493915319442749 \n",
      "Epoch 123: >>>>> loss: 1.2751845121383667 \n",
      "Epoch 124: >>>>> loss: 1.351598858833313 \n",
      "Epoch 125: >>>>> loss: 1.178510308265686 \n",
      "Epoch 126: >>>>> loss: 0.4283560812473297 \n",
      "Epoch 127: >>>>> loss: 1.1540745496749878 \n",
      "Epoch 128: >>>>> loss: 1.2207382917404175 \n",
      "Epoch 129: >>>>> loss: 0.4466739892959595 \n",
      "Epoch 130: >>>>> loss: 0.4245680272579193  accuracy: 0.5105263157894737\n",
      "Epoch 131: >>>>> loss: 0.4709489941596985 \n",
      "Epoch 132: >>>>> loss: 1.3720072507858276 \n",
      "Epoch 133: >>>>> loss: 1.1983968019485474 \n",
      "Epoch 134: >>>>> loss: 1.329187273979187 \n",
      "Epoch 135: >>>>> loss: 1.2599090337753296 \n",
      "Epoch 136: >>>>> loss: 0.5058011412620544 \n",
      "Epoch 137: >>>>> loss: 1.045355200767517 \n",
      "Epoch 138: >>>>> loss: 1.1995538473129272 \n",
      "Epoch 139: >>>>> loss: 0.48217248916625977 \n",
      "Epoch 140: >>>>> loss: 0.4424990117549896  accuracy: 0.6842105263157895\n",
      "Epoch 141: >>>>> loss: 0.4965626895427704 \n",
      "Epoch 142: >>>>> loss: 1.278051733970642 \n",
      "Epoch 143: >>>>> loss: 0.46334779262542725 \n",
      "Epoch 144: >>>>> loss: 0.4500885009765625 \n",
      "Epoch 145: >>>>> loss: 1.1888093948364258 \n",
      "Epoch 146: >>>>> loss: 1.3601435422897339 \n",
      "Epoch 147: >>>>> loss: 1.289244532585144 \n",
      "Epoch 148: >>>>> loss: 1.2163463830947876 \n",
      "Epoch 149: >>>>> loss: 0.569238007068634 \n",
      "Epoch 150: >>>>> loss: 1.1675571203231812  accuracy: 0.7\n",
      "Epoch 151: >>>>> loss: 0.4062732458114624 \n",
      "Epoch 152: >>>>> loss: 0.39691948890686035 \n",
      "Epoch 153: >>>>> loss: 0.9940975904464722 \n",
      "Epoch 154: >>>>> loss: 0.33793094754219055 \n",
      "Epoch 155: >>>>> loss: 0.46928298473358154 \n",
      "Epoch 156: >>>>> loss: 0.3272573947906494 \n",
      "Epoch 157: >>>>> loss: 1.2478963136672974 \n",
      "Epoch 158: >>>>> loss: 1.1567175388336182 \n",
      "Epoch 159: >>>>> loss: 0.43268656730651855 \n",
      "Epoch 160: >>>>> loss: 1.3547483682632446  accuracy: 0.6684210526315789\n",
      "Epoch 161: >>>>> loss: 1.4343217611312866 \n",
      "Epoch 162: >>>>> loss: 0.3891433775424957 \n",
      "Epoch 163: >>>>> loss: 0.4298941195011139 \n",
      "Epoch 164: >>>>> loss: 1.3918397426605225 \n",
      "Epoch 165: >>>>> loss: 1.2975592613220215 \n",
      "Epoch 166: >>>>> loss: 1.5141706466674805 \n",
      "Epoch 167: >>>>> loss: 0.43045932054519653 \n",
      "Epoch 168: >>>>> loss: 0.4123994708061218 \n",
      "Epoch 169: >>>>> loss: 1.1848976612091064 \n",
      "Epoch 170: >>>>> loss: 0.7871755361557007  accuracy: 0.7105263157894737\n",
      "Epoch 171: >>>>> loss: 1.555629014968872 \n",
      "Epoch 172: >>>>> loss: 0.4415143132209778 \n",
      "Epoch 173: >>>>> loss: 1.296947717666626 \n",
      "Epoch 174: >>>>> loss: 0.4465019702911377 \n",
      "Epoch 175: >>>>> loss: 0.4319474697113037 \n",
      "Epoch 176: >>>>> loss: 0.379856675863266 \n",
      "Epoch 177: >>>>> loss: 1.1270484924316406 \n",
      "Epoch 178: >>>>> loss: 1.2086280584335327 \n",
      "Epoch 179: >>>>> loss: 1.3656315803527832 \n",
      "Epoch 180: >>>>> loss: 0.35266733169555664  accuracy: 0.6473684210526316\n",
      "Epoch 181: >>>>> loss: 0.358425110578537 \n",
      "Epoch 182: >>>>> loss: 0.3927213251590729 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183: >>>>> loss: 1.2118840217590332 \n",
      "Epoch 184: >>>>> loss: 1.3489304780960083 \n",
      "Epoch 185: >>>>> loss: 1.257980465888977 \n",
      "Epoch 186: >>>>> loss: 0.28101927042007446 \n",
      "Epoch 187: >>>>> loss: 0.3816763758659363 \n",
      "Epoch 188: >>>>> loss: 0.3407120108604431 \n",
      "Epoch 189: >>>>> loss: 1.3119910955429077 \n",
      "Epoch 190: >>>>> loss: 1.078587293624878  accuracy: 0.7368421052631579\n",
      "Epoch 191: >>>>> loss: 0.3806036412715912 \n",
      "Epoch 192: >>>>> loss: 1.2969286441802979 \n",
      "Epoch 193: >>>>> loss: 1.1460391283035278 \n",
      "Epoch 194: >>>>> loss: 1.5167796611785889 \n",
      "Epoch 195: >>>>> loss: 1.240458607673645 \n",
      "Epoch 196: >>>>> loss: 1.3349157571792603 \n",
      "Epoch 197: >>>>> loss: 0.4032554030418396 \n",
      "Epoch 198: >>>>> loss: 1.1011329889297485 \n",
      "Epoch 199: >>>>> loss: 0.3816370666027069 \n",
      "Epoch 200: >>>>> loss: 0.320182204246521  accuracy: 0.6842105263157895\n",
      "Epoch 201: >>>>> loss: 0.3059037923812866 \n",
      "Epoch 202: >>>>> loss: 0.3218662440776825 \n",
      "Epoch 203: >>>>> loss: 0.28663450479507446 \n",
      "Epoch 204: >>>>> loss: 0.26106780767440796 \n",
      "Epoch 205: >>>>> loss: 1.346477746963501 \n",
      "Epoch 206: >>>>> loss: 1.2821602821350098 \n",
      "Epoch 207: >>>>> loss: 1.2965075969696045 \n",
      "Epoch 208: >>>>> loss: 0.28930649161338806 \n",
      "Epoch 209: >>>>> loss: 0.31028836965560913 \n",
      "Epoch 210: >>>>> loss: 0.31729355454444885  accuracy: 0.6842105263157895\n",
      "Epoch 211: >>>>> loss: 0.39089375734329224 \n",
      "Epoch 212: >>>>> loss: 0.37666836380958557 \n",
      "Epoch 213: >>>>> loss: 1.3137660026550293 \n",
      "Epoch 214: >>>>> loss: 0.2756001651287079 \n",
      "Epoch 215: >>>>> loss: 0.27734455466270447 \n",
      "Epoch 216: >>>>> loss: 1.2307260036468506 \n",
      "Epoch 217: >>>>> loss: 0.2935834527015686 \n",
      "Epoch 218: >>>>> loss: 1.2022888660430908 \n",
      "Epoch 219: >>>>> loss: 0.24970082938671112 \n",
      "Epoch 220: >>>>> loss: 1.2585777044296265  accuracy: 0.6736842105263158\n",
      "Epoch 221: >>>>> loss: 0.30398082733154297 \n",
      "Epoch 222: >>>>> loss: 0.2915153503417969 \n",
      "Epoch 223: >>>>> loss: 1.2769709825515747 \n",
      "Epoch 224: >>>>> loss: 1.150611162185669 \n",
      "Epoch 225: >>>>> loss: 0.2872147262096405 \n",
      "Epoch 226: >>>>> loss: 0.308954119682312 \n",
      "Epoch 227: >>>>> loss: 0.26453346014022827 \n",
      "Epoch 228: >>>>> loss: 0.26503226161003113 \n",
      "Epoch 229: >>>>> loss: 1.2678618431091309 \n",
      "Epoch 230: >>>>> loss: 1.2105965614318848  accuracy: 0.7105263157894737\n",
      "Epoch 231: >>>>> loss: 0.19796860218048096 \n",
      "Epoch 232: >>>>> loss: 0.21481846272945404 \n",
      "Epoch 233: >>>>> loss: 1.1594505310058594 \n",
      "Epoch 234: >>>>> loss: 0.233272522687912 \n",
      "Epoch 235: >>>>> loss: 0.20692436397075653 \n",
      "Epoch 236: >>>>> loss: 1.2701548337936401 \n",
      "Epoch 237: >>>>> loss: 0.2482876032590866 \n",
      "Epoch 238: >>>>> loss: 0.2536407709121704 \n",
      "Epoch 239: >>>>> loss: 0.22375716269016266 \n",
      "Epoch 240: >>>>> loss: 0.23419000208377838  accuracy: 0.4842105263157895\n",
      "Epoch 241: >>>>> loss: 0.2697126269340515 \n",
      "Epoch 242: >>>>> loss: 1.1577285528182983 \n",
      "Epoch 243: >>>>> loss: 1.3878589868545532 \n",
      "Epoch 244: >>>>> loss: 1.2803674936294556 \n",
      "Epoch 245: >>>>> loss: 1.280146837234497 \n",
      "Epoch 246: >>>>> loss: 0.26832425594329834 \n",
      "Epoch 247: >>>>> loss: 1.3380320072174072 \n",
      "Epoch 248: >>>>> loss: 1.3727717399597168 \n",
      "Epoch 249: >>>>> loss: 0.23366917669773102 \n",
      "Epoch 250: >>>>> loss: 1.2906320095062256  accuracy: 0.5105263157894737\n",
      "Epoch 251: >>>>> loss: 0.22067172825336456 \n",
      "Epoch 252: >>>>> loss: 1.2623566389083862 \n",
      "Epoch 253: >>>>> loss: 0.2545907199382782 \n",
      "Epoch 254: >>>>> loss: 0.1931343823671341 \n",
      "Epoch 255: >>>>> loss: 0.2420554906129837 \n",
      "Epoch 256: >>>>> loss: 1.2682232856750488 \n",
      "Epoch 257: >>>>> loss: 0.1888294667005539 \n",
      "Epoch 258: >>>>> loss: 0.2138952761888504 \n",
      "Epoch 259: >>>>> loss: 1.2583813667297363 \n",
      "Epoch 260: >>>>> loss: 1.3307037353515625  accuracy: 0.5052631578947369\n",
      "Epoch 261: >>>>> loss: 0.22953878343105316 \n",
      "Epoch 262: >>>>> loss: 1.2808016538619995 \n",
      "Epoch 263: >>>>> loss: 0.259567528963089 \n",
      "Epoch 264: >>>>> loss: 0.19406163692474365 \n",
      "Epoch 265: >>>>> loss: 0.1939091980457306 \n",
      "Epoch 266: >>>>> loss: 1.2151050567626953 \n",
      "Epoch 267: >>>>> loss: 0.2240871638059616 \n",
      "Epoch 268: >>>>> loss: 0.1942959725856781 \n",
      "Epoch 269: >>>>> loss: 0.204278364777565 \n",
      "Epoch 270: >>>>> loss: 0.20454919338226318  accuracy: 0.7157894736842105\n",
      "Epoch 271: >>>>> loss: 1.133164882659912 \n",
      "Epoch 272: >>>>> loss: 0.21564266085624695 \n",
      "Epoch 273: >>>>> loss: 0.25894680619239807 \n",
      "Epoch 274: >>>>> loss: 1.268917441368103 \n",
      "Epoch 275: >>>>> loss: 0.22241587936878204 \n",
      "Epoch 276: >>>>> loss: 0.21765848994255066 \n",
      "Epoch 277: >>>>> loss: 0.17643558979034424 \n",
      "Epoch 278: >>>>> loss: 0.24461568892002106 \n",
      "Epoch 279: >>>>> loss: 0.2534486651420593 \n",
      "Epoch 280: >>>>> loss: 0.1919468641281128  accuracy: 0.5421052631578948\n",
      "Epoch 281: >>>>> loss: 1.2083722352981567 \n",
      "Epoch 282: >>>>> loss: 0.16964441537857056 \n",
      "Epoch 283: >>>>> loss: 1.3471404314041138 \n",
      "Epoch 284: >>>>> loss: 1.1554778814315796 \n",
      "Epoch 285: >>>>> loss: 1.3304495811462402 \n",
      "Epoch 286: >>>>> loss: 1.2445462942123413 \n",
      "Epoch 287: >>>>> loss: 1.1053600311279297 \n",
      "Epoch 288: >>>>> loss: 0.2117176353931427 \n",
      "Epoch 289: >>>>> loss: 1.1105824708938599 \n",
      "Epoch 290: >>>>> loss: 0.22855840623378754  accuracy: 0.6421052631578947\n",
      "Epoch 291: >>>>> loss: 1.2300388813018799 \n",
      "Epoch 292: >>>>> loss: 1.2635823488235474 \n",
      "Epoch 293: >>>>> loss: 0.20503437519073486 \n",
      "Epoch 294: >>>>> loss: 1.3359184265136719 \n",
      "Epoch 295: >>>>> loss: 0.19027604162693024 \n",
      "Epoch 296: >>>>> loss: 0.1912926286458969 \n",
      "Epoch 297: >>>>> loss: 0.205674409866333 \n",
      "Epoch 298: >>>>> loss: 0.14678512513637543 \n",
      "Epoch 299: >>>>> loss: 0.15657444298267365 \n",
      "Epoch 300: >>>>> loss: 0.17102374136447906  accuracy: 0.4789473684210526\n"
     ]
    }
   ],
   "source": [
    "# Normal\n",
    "finetune_model, record = finetuning(finetune_model , data_aug, train_loader, criterion, optimizer, fine_tune_epochs, 1, test_loader, parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n",
      "Start Training\n",
      "Epoch 1: >>>>> loss: 1.8875242471694946 \n",
      "Epoch 2: >>>>> loss: 1.855168342590332 \n",
      "Epoch 3: >>>>> loss: 1.8436195850372314 \n",
      "Epoch 4: >>>>> loss: 1.826266884803772 \n",
      "Epoch 5: >>>>> loss: 1.8050285577774048 \n",
      "Epoch 6: >>>>> loss: 1.7451403141021729 \n",
      "Epoch 7: >>>>> loss: 1.6992255449295044 \n",
      "Epoch 8: >>>>> loss: 1.72196364402771 \n",
      "Epoch 9: >>>>> loss: 1.6971665620803833 \n",
      "Epoch 10: >>>>> loss: 1.7223783731460571  accuracy: 0.43157894736842106\n",
      "Epoch 11: >>>>> loss: 1.6832969188690186 \n",
      "Epoch 12: >>>>> loss: 1.6404898166656494 \n",
      "Epoch 13: >>>>> loss: 1.6534546613693237 \n",
      "Epoch 14: >>>>> loss: 1.6362974643707275 \n",
      "Epoch 15: >>>>> loss: 1.5436838865280151 \n",
      "Epoch 16: >>>>> loss: 1.4615058898925781 \n",
      "Epoch 17: >>>>> loss: 1.6109257936477661 \n",
      "Epoch 18: >>>>> loss: 1.5991570949554443 \n",
      "Epoch 19: >>>>> loss: 1.5764433145523071 \n",
      "Epoch 20: >>>>> loss: 1.562514305114746  accuracy: 0.45789473684210524\n",
      "Epoch 21: >>>>> loss: 1.4584593772888184 \n",
      "Epoch 22: >>>>> loss: 1.3952381610870361 \n",
      "Epoch 23: >>>>> loss: 1.4916958808898926 \n",
      "Epoch 24: >>>>> loss: 1.448191523551941 \n",
      "Epoch 25: >>>>> loss: 1.4238568544387817 \n",
      "Epoch 26: >>>>> loss: 1.5915985107421875 \n",
      "Epoch 27: >>>>> loss: 1.4247934818267822 \n",
      "Epoch 28: >>>>> loss: 1.5332229137420654 \n",
      "Epoch 29: >>>>> loss: 1.4730716943740845 \n",
      "Epoch 30: >>>>> loss: 1.525625228881836  accuracy: 0.46842105263157896\n",
      "Epoch 31: >>>>> loss: 1.4009813070297241 \n",
      "Epoch 32: >>>>> loss: 1.3716657161712646 \n",
      "Epoch 33: >>>>> loss: 1.479973554611206 \n",
      "Epoch 34: >>>>> loss: 1.4032684564590454 \n",
      "Epoch 35: >>>>> loss: 1.3380123376846313 \n",
      "Epoch 36: >>>>> loss: 1.2806906700134277 \n",
      "Epoch 37: >>>>> loss: 1.351826786994934 \n",
      "Epoch 38: >>>>> loss: 1.3067902326583862 \n",
      "Epoch 39: >>>>> loss: 1.5360687971115112 \n",
      "Epoch 40: >>>>> loss: 1.3800010681152344  accuracy: 0.5052631578947369\n",
      "Epoch 41: >>>>> loss: 1.45432710647583 \n",
      "Epoch 42: >>>>> loss: 1.4982142448425293 \n",
      "Epoch 43: >>>>> loss: 1.424576759338379 \n",
      "Epoch 44: >>>>> loss: 1.5328311920166016 \n",
      "Epoch 45: >>>>> loss: 1.5075786113739014 \n",
      "Epoch 46: >>>>> loss: 1.4180721044540405 \n",
      "Epoch 47: >>>>> loss: 1.3211193084716797 \n",
      "Epoch 48: >>>>> loss: 1.4256465435028076 \n",
      "Epoch 49: >>>>> loss: 1.4501019716262817 \n",
      "Epoch 50: >>>>> loss: 1.4417601823806763  accuracy: 0.4842105263157895\n",
      "Epoch 51: >>>>> loss: 1.4596174955368042 \n",
      "Epoch 52: >>>>> loss: 1.3379899263381958 \n",
      "Epoch 53: >>>>> loss: 1.3356943130493164 \n",
      "Epoch 54: >>>>> loss: 1.3946336507797241 \n",
      "Epoch 55: >>>>> loss: 1.3417028188705444 \n",
      "Epoch 56: >>>>> loss: 1.3767075538635254 \n",
      "Epoch 57: >>>>> loss: 1.3349403142929077 \n",
      "Epoch 58: >>>>> loss: 1.3687069416046143 \n",
      "Epoch 59: >>>>> loss: 1.374206304550171 \n",
      "Epoch 60: >>>>> loss: 1.5253691673278809  accuracy: 0.47368421052631576\n",
      "Epoch 61: >>>>> loss: 1.3426144123077393 \n",
      "Epoch 62: >>>>> loss: 1.3385878801345825 \n",
      "Epoch 63: >>>>> loss: 1.336349606513977 \n",
      "Epoch 64: >>>>> loss: 1.3145005702972412 \n",
      "Epoch 65: >>>>> loss: 1.3526395559310913 \n",
      "Epoch 66: >>>>> loss: 1.4273236989974976 \n",
      "Epoch 67: >>>>> loss: 1.3783471584320068 \n",
      "Epoch 68: >>>>> loss: 1.3590441942214966 \n",
      "Epoch 69: >>>>> loss: 1.3334417343139648 \n",
      "Epoch 70: >>>>> loss: 1.3629472255706787  accuracy: 0.5105263157894737\n",
      "Epoch 71: >>>>> loss: 1.2674158811569214 \n",
      "Epoch 72: >>>>> loss: 1.4678351879119873 \n",
      "Epoch 73: >>>>> loss: 1.456337571144104 \n",
      "Epoch 74: >>>>> loss: 1.297714114189148 \n",
      "Epoch 75: >>>>> loss: 1.2830801010131836 \n",
      "Epoch 76: >>>>> loss: 1.3656851053237915 \n",
      "Epoch 77: >>>>> loss: 1.3588545322418213 \n",
      "Epoch 78: >>>>> loss: 1.4371874332427979 \n",
      "Epoch 79: >>>>> loss: 1.4501991271972656 \n",
      "Epoch 80: >>>>> loss: 1.262776255607605  accuracy: 0.5052631578947369\n",
      "Epoch 81: >>>>> loss: 1.3592759370803833 \n",
      "Epoch 82: >>>>> loss: 1.1364076137542725 \n",
      "Epoch 83: >>>>> loss: 1.435347080230713 \n",
      "Epoch 84: >>>>> loss: 1.245306134223938 \n",
      "Epoch 85: >>>>> loss: 1.2015949487686157 \n",
      "Epoch 86: >>>>> loss: 1.4439369440078735 \n",
      "Epoch 87: >>>>> loss: 1.3610107898712158 \n",
      "Epoch 88: >>>>> loss: 1.3049362897872925 \n",
      "Epoch 89: >>>>> loss: 1.3065494298934937 \n",
      "Epoch 90: >>>>> loss: 1.161842703819275  accuracy: 0.47368421052631576\n",
      "Epoch 91: >>>>> loss: 1.3020939826965332 \n",
      "Epoch 92: >>>>> loss: 1.3243739604949951 \n",
      "Epoch 93: >>>>> loss: 1.4944437742233276 \n",
      "Epoch 94: >>>>> loss: 1.3807456493377686 \n",
      "Epoch 95: >>>>> loss: 1.198503851890564 \n",
      "Epoch 96: >>>>> loss: 1.2592936754226685 \n",
      "Epoch 97: >>>>> loss: 1.227674961090088 \n",
      "Epoch 98: >>>>> loss: 1.3308061361312866 \n",
      "Epoch 99: >>>>> loss: 1.2404776811599731 \n",
      "Epoch 100: >>>>> loss: 1.278255581855774  accuracy: 0.5210526315789473\n",
      "Epoch 101: >>>>> loss: 1.2130428552627563 \n",
      "Epoch 102: >>>>> loss: 1.2705997228622437 \n",
      "Epoch 103: >>>>> loss: 1.216610312461853 \n",
      "Epoch 104: >>>>> loss: 1.361368179321289 \n",
      "Epoch 105: >>>>> loss: 1.1815026998519897 \n",
      "Epoch 106: >>>>> loss: 1.341393232345581 \n",
      "Epoch 107: >>>>> loss: 1.0597528219223022 \n",
      "Epoch 108: >>>>> loss: 1.2666501998901367 \n",
      "Epoch 109: >>>>> loss: 1.378548502922058 \n",
      "Epoch 110: >>>>> loss: 1.0875612497329712  accuracy: 0.5631578947368421\n",
      "Epoch 111: >>>>> loss: 1.2507160902023315 \n",
      "Epoch 112: >>>>> loss: 1.2516924142837524 \n",
      "Epoch 113: >>>>> loss: 1.2197182178497314 \n",
      "Epoch 114: >>>>> loss: 1.2090727090835571 \n",
      "Epoch 115: >>>>> loss: 1.2226048707962036 \n",
      "Epoch 116: >>>>> loss: 1.225367784500122 \n",
      "Epoch 117: >>>>> loss: 1.3101109266281128 \n",
      "Epoch 118: >>>>> loss: 1.1909290552139282 \n",
      "Epoch 119: >>>>> loss: 1.210446834564209 \n",
      "Epoch 120: >>>>> loss: 1.164908528327942  accuracy: 0.49473684210526314\n",
      "Epoch 121: >>>>> loss: 1.313244104385376 \n",
      "Epoch 122: >>>>> loss: 1.3639936447143555 \n",
      "Epoch 123: >>>>> loss: 1.2015174627304077 \n",
      "Epoch 124: >>>>> loss: 1.109593152999878 \n",
      "Epoch 125: >>>>> loss: 1.2327203750610352 \n",
      "Epoch 126: >>>>> loss: 1.1930484771728516 \n",
      "Epoch 127: >>>>> loss: 1.15514075756073 \n",
      "Epoch 128: >>>>> loss: 1.200945258140564 \n",
      "Epoch 129: >>>>> loss: 1.1649941205978394 \n",
      "Epoch 130: >>>>> loss: 1.393231987953186  accuracy: 0.5421052631578948\n",
      "Epoch 131: >>>>> loss: 1.3209099769592285 \n",
      "Epoch 132: >>>>> loss: 1.184564232826233 \n",
      "Epoch 133: >>>>> loss: 1.237023949623108 \n",
      "Epoch 134: >>>>> loss: 1.1051303148269653 \n",
      "Epoch 135: >>>>> loss: 1.0334839820861816 \n",
      "Epoch 136: >>>>> loss: 1.3202730417251587 \n",
      "Epoch 137: >>>>> loss: 1.4163885116577148 \n",
      "Epoch 138: >>>>> loss: 1.177093505859375 \n",
      "Epoch 139: >>>>> loss: 1.1376060247421265 \n",
      "Epoch 140: >>>>> loss: 1.153253436088562  accuracy: 0.5526315789473685\n",
      "Epoch 141: >>>>> loss: 1.2117191553115845 \n",
      "Epoch 142: >>>>> loss: 1.1925727128982544 \n",
      "Epoch 143: >>>>> loss: 1.374686598777771 \n",
      "Epoch 144: >>>>> loss: 1.383646011352539 \n",
      "Epoch 145: >>>>> loss: 1.1051528453826904 \n",
      "Epoch 146: >>>>> loss: 1.1684156656265259 \n",
      "Epoch 147: >>>>> loss: 1.0998636484146118 \n",
      "Epoch 148: >>>>> loss: 1.4210025072097778 \n",
      "Epoch 149: >>>>> loss: 1.087467908859253 \n",
      "Epoch 150: >>>>> loss: 1.4172236919403076  accuracy: 0.531578947368421\n",
      "Epoch 151: >>>>> loss: 1.355872631072998 \n",
      "Epoch 152: >>>>> loss: 1.2325351238250732 \n",
      "Epoch 153: >>>>> loss: 1.337677240371704 \n",
      "Epoch 154: >>>>> loss: 1.2517131567001343 \n",
      "Epoch 155: >>>>> loss: 1.2611442804336548 \n",
      "Epoch 156: >>>>> loss: 1.0405151844024658 \n",
      "Epoch 157: >>>>> loss: 1.3328304290771484 \n",
      "Epoch 158: >>>>> loss: 1.0797911882400513 \n",
      "Epoch 159: >>>>> loss: 1.3771837949752808 \n",
      "Epoch 160: >>>>> loss: 1.0919718742370605  accuracy: 0.5421052631578948\n",
      "Epoch 161: >>>>> loss: 1.1537803411483765 \n",
      "Epoch 162: >>>>> loss: 1.063237190246582 \n",
      "Epoch 163: >>>>> loss: 1.2393267154693604 \n",
      "Epoch 164: >>>>> loss: 0.9838662147521973 \n",
      "Epoch 165: >>>>> loss: 1.1697498559951782 \n",
      "Epoch 166: >>>>> loss: 1.3111919164657593 \n",
      "Epoch 167: >>>>> loss: 1.36867356300354 \n",
      "Epoch 168: >>>>> loss: 1.1608844995498657 \n",
      "Epoch 169: >>>>> loss: 1.1576796770095825 \n",
      "Epoch 170: >>>>> loss: 1.271857738494873  accuracy: 0.5894736842105263\n",
      "Epoch 171: >>>>> loss: 1.062150239944458 \n",
      "Epoch 172: >>>>> loss: 1.1105667352676392 \n",
      "Epoch 173: >>>>> loss: 1.1858975887298584 \n",
      "Epoch 174: >>>>> loss: 1.2298681735992432 \n",
      "Epoch 175: >>>>> loss: 1.3164515495300293 \n",
      "Epoch 176: >>>>> loss: 1.3495041131973267 \n",
      "Epoch 177: >>>>> loss: 1.1456077098846436 \n",
      "Epoch 178: >>>>> loss: 1.1488511562347412 \n",
      "Epoch 179: >>>>> loss: 1.0389341115951538 \n",
      "Epoch 180: >>>>> loss: 1.3395134210586548  accuracy: 0.5421052631578948\n",
      "Epoch 181: >>>>> loss: 1.0198991298675537 \n",
      "Epoch 182: >>>>> loss: 1.1113858222961426 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 183: >>>>> loss: 1.0556892156600952 \n",
      "Epoch 184: >>>>> loss: 1.167283296585083 \n",
      "Epoch 185: >>>>> loss: 1.344879150390625 \n",
      "Epoch 186: >>>>> loss: 1.128980040550232 \n",
      "Epoch 187: >>>>> loss: 1.4071509838104248 \n",
      "Epoch 188: >>>>> loss: 1.3462942838668823 \n",
      "Epoch 189: >>>>> loss: 1.051360845565796 \n",
      "Epoch 190: >>>>> loss: 1.259808897972107  accuracy: 0.5421052631578948\n",
      "Epoch 191: >>>>> loss: 1.0031287670135498 \n",
      "Epoch 192: >>>>> loss: 1.2465404272079468 \n",
      "Epoch 193: >>>>> loss: 1.0576963424682617 \n",
      "Epoch 194: >>>>> loss: 1.3436250686645508 \n",
      "Epoch 195: >>>>> loss: 1.2193162441253662 \n",
      "Epoch 196: >>>>> loss: 1.1716508865356445 \n",
      "Epoch 197: >>>>> loss: 1.0499998331069946 \n",
      "Epoch 198: >>>>> loss: 1.1551618576049805 \n",
      "Epoch 199: >>>>> loss: 0.9820274114608765 \n",
      "Epoch 200: >>>>> loss: 1.4822421073913574  accuracy: 0.5105263157894737\n",
      "Epoch 201: >>>>> loss: 1.2301011085510254 \n",
      "Epoch 202: >>>>> loss: 1.2018553018569946 \n",
      "Epoch 203: >>>>> loss: 1.0777796506881714 \n",
      "Epoch 204: >>>>> loss: 1.051023006439209 \n",
      "Epoch 205: >>>>> loss: 1.0660548210144043 \n",
      "Epoch 206: >>>>> loss: 1.0023949146270752 \n",
      "Epoch 207: >>>>> loss: 0.985319972038269 \n",
      "Epoch 208: >>>>> loss: 1.088271141052246 \n",
      "Epoch 209: >>>>> loss: 1.2650185823440552 \n",
      "Epoch 210: >>>>> loss: 1.0344597101211548  accuracy: 0.5842105263157895\n",
      "Epoch 211: >>>>> loss: 1.328615427017212 \n",
      "Epoch 212: >>>>> loss: 1.3175945281982422 \n",
      "Epoch 213: >>>>> loss: 1.022729516029358 \n",
      "Epoch 214: >>>>> loss: 1.20170259475708 \n",
      "Epoch 215: >>>>> loss: 1.2548552751541138 \n",
      "Epoch 216: >>>>> loss: 1.0184224843978882 \n",
      "Epoch 217: >>>>> loss: 1.054568886756897 \n",
      "Epoch 218: >>>>> loss: 1.1053930521011353 \n",
      "Epoch 219: >>>>> loss: 1.2645477056503296 \n",
      "Epoch 220: >>>>> loss: 1.2173702716827393  accuracy: 0.5578947368421052\n",
      "Epoch 221: >>>>> loss: 1.4890152215957642 \n",
      "Epoch 222: >>>>> loss: 1.359203815460205 \n",
      "Epoch 223: >>>>> loss: 1.109660267829895 \n",
      "Epoch 224: >>>>> loss: 1.0438429117202759 \n",
      "Epoch 225: >>>>> loss: 0.9908579587936401 \n",
      "Epoch 226: >>>>> loss: 1.24429452419281 \n",
      "Epoch 227: >>>>> loss: 1.0755332708358765 \n",
      "Epoch 228: >>>>> loss: 1.3821040391921997 \n",
      "Epoch 229: >>>>> loss: 1.0011574029922485 \n",
      "Epoch 230: >>>>> loss: 1.2703909873962402  accuracy: 0.5578947368421052\n",
      "Epoch 231: >>>>> loss: 1.081817388534546 \n",
      "Epoch 232: >>>>> loss: 1.0896998643875122 \n",
      "Epoch 233: >>>>> loss: 1.2895548343658447 \n",
      "Epoch 234: >>>>> loss: 1.0995080471038818 \n",
      "Epoch 235: >>>>> loss: 1.0675880908966064 \n",
      "Epoch 236: >>>>> loss: 1.2308083772659302 \n",
      "Epoch 237: >>>>> loss: 1.1782745122909546 \n",
      "Epoch 238: >>>>> loss: 1.227565884590149 \n",
      "Epoch 239: >>>>> loss: 1.033849835395813 \n",
      "Epoch 240: >>>>> loss: 1.1750421524047852  accuracy: 0.5736842105263158\n",
      "Epoch 241: >>>>> loss: 1.0334181785583496 \n",
      "Epoch 242: >>>>> loss: 1.279114007949829 \n",
      "Epoch 243: >>>>> loss: 1.1258641481399536 \n",
      "Epoch 244: >>>>> loss: 1.153432846069336 \n",
      "Epoch 245: >>>>> loss: 1.2536686658859253 \n",
      "Epoch 246: >>>>> loss: 1.027866244316101 \n",
      "Epoch 247: >>>>> loss: 0.8867518305778503 \n",
      "Epoch 248: >>>>> loss: 1.3640449047088623 \n",
      "Epoch 249: >>>>> loss: 0.923108696937561 \n",
      "Epoch 250: >>>>> loss: 0.9808631539344788  accuracy: 0.47368421052631576\n",
      "Epoch 251: >>>>> loss: 1.0394554138183594 \n",
      "Epoch 252: >>>>> loss: 0.9284805059432983 \n",
      "Epoch 253: >>>>> loss: 1.2054020166397095 \n",
      "Epoch 254: >>>>> loss: 1.4006316661834717 \n",
      "Epoch 255: >>>>> loss: 1.2914122343063354 \n",
      "Epoch 256: >>>>> loss: 1.387056827545166 \n",
      "Epoch 257: >>>>> loss: 1.2597227096557617 \n",
      "Epoch 258: >>>>> loss: 1.44341242313385 \n",
      "Epoch 259: >>>>> loss: 1.239508032798767 \n",
      "Epoch 260: >>>>> loss: 0.9229456186294556  accuracy: 0.5894736842105263\n",
      "Epoch 261: >>>>> loss: 0.9313910007476807 \n",
      "Epoch 262: >>>>> loss: 1.3583567142486572 \n",
      "Epoch 263: >>>>> loss: 1.205956220626831 \n",
      "Epoch 264: >>>>> loss: 1.0365321636199951 \n",
      "Epoch 265: >>>>> loss: 1.0076892375946045 \n",
      "Epoch 266: >>>>> loss: 1.1367064714431763 \n",
      "Epoch 267: >>>>> loss: 1.2303879261016846 \n",
      "Epoch 268: >>>>> loss: 1.3148733377456665 \n",
      "Epoch 269: >>>>> loss: 0.9409441947937012 \n",
      "Epoch 270: >>>>> loss: 1.0027421712875366  accuracy: 0.5105263157894737\n",
      "Epoch 271: >>>>> loss: 0.9357597231864929 \n",
      "Epoch 272: >>>>> loss: 1.2330946922302246 \n",
      "Epoch 273: >>>>> loss: 1.3768914937973022 \n",
      "Epoch 274: >>>>> loss: 1.017329216003418 \n",
      "Epoch 275: >>>>> loss: 0.9227669835090637 \n",
      "Epoch 276: >>>>> loss: 1.4599241018295288 \n",
      "Epoch 277: >>>>> loss: 1.2715739011764526 \n",
      "Epoch 278: >>>>> loss: 1.275689721107483 \n",
      "Epoch 279: >>>>> loss: 1.229327917098999 \n",
      "Epoch 280: >>>>> loss: 1.3250757455825806  accuracy: 0.5157894736842106\n",
      "Epoch 281: >>>>> loss: 1.264703631401062 \n",
      "Epoch 282: >>>>> loss: 1.028610110282898 \n",
      "Epoch 283: >>>>> loss: 0.9007497429847717 \n",
      "Epoch 284: >>>>> loss: 0.9484716653823853 \n",
      "Epoch 285: >>>>> loss: 1.1866613626480103 \n",
      "Epoch 286: >>>>> loss: 1.1840945482254028 \n",
      "Epoch 287: >>>>> loss: 1.1303430795669556 \n",
      "Epoch 288: >>>>> loss: 0.9813252091407776 \n",
      "Epoch 289: >>>>> loss: 1.415846586227417 \n",
      "Epoch 290: >>>>> loss: 1.323842167854309  accuracy: 0.5894736842105263\n",
      "Epoch 291: >>>>> loss: 1.0199275016784668 \n",
      "Epoch 292: >>>>> loss: 1.1236380338668823 \n",
      "Epoch 293: >>>>> loss: 0.9231566786766052 \n",
      "Epoch 294: >>>>> loss: 1.2302634716033936 \n",
      "Epoch 295: >>>>> loss: 1.222773790359497 \n",
      "Epoch 296: >>>>> loss: 1.1845829486846924 \n",
      "Epoch 297: >>>>> loss: 1.1234188079833984 \n",
      "Epoch 298: >>>>> loss: 1.182856559753418 \n",
      "Epoch 299: >>>>> loss: 1.092941403388977 \n",
      "Epoch 300: >>>>> loss: 1.4276491403579712  accuracy: 0.5526315789473685\n"
     ]
    }
   ],
   "source": [
    "# Pretrained\n",
    "finetune_model, record = finetuning(finetune_model , data_aug, train_loader, criterion, optimizer, fine_tune_epochs, 1, test_loader, parallel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
