{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn  # for heatmaps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import loadmat\n",
    "\n",
    "# link = r\"D:\\external_data\\Experiment4\"\n",
    "# filename = r\"\\Dataset_PWR_WiFi.mat\"\n",
    "# directory = link + filename\n",
    "\n",
    "# mat = loadmat(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import experimental dataset 2\n",
    "# folderpath2 = \"D:/external_data/Experiment3/csv_files/exp_2\"  # CHANGE THIS IF THE PATH CHANGED\n",
    "# df_exp2 = import_clean_data('exp2',folderpath2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise spectrogram\n",
    "img = img.transpose()\n",
    "img = img[:,::10]\n",
    "plt.figure(figsize = (15,50))\n",
    "plt.imshow(img,cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from data.process_data import stacking\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def visual_spectrogram(img,title='spectrogram'):\n",
    "    \n",
    "    if len(img.shape) == 3:\n",
    "        \n",
    "        img.squeeze()\n",
    "    \n",
    "    img = MinMaxScaler(feature_range=(0,1)).fit_transform(img)\n",
    "    \n",
    "#     img = img.reshape(*img.shape[:-1],1)\n",
    "    \n",
    "#     img = np.concatenate((img,img,img),axis=2)\n",
    "    \n",
    "    plt.figure(figsize = (15,50))\n",
    "    plt.imshow(img,cmap='jet')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "        \n",
    "#visualize weights for alexnet - first conv layer\n",
    "alexnet = models.alexnet(pretrained=False)\n",
    "\n",
    "# Store all conv layer in alexnet\n",
    "k = []\n",
    "for m in alexnet.modules():\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        print(m.weight.data.shape)\n",
    "        k.append(m)\n",
    "    else:  \n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_single_filter(weight):\n",
    "    \n",
    "    img = MinMaxScaler(feature_range=(0,1)).fit_transform(weight)\n",
    "    \n",
    "#     img = img.reshape(*img.shape[:-1],1)\n",
    "    \n",
    "#     img = np.concatenate((img,img,img),axis=2)\n",
    "    \n",
    "#     plt.figure(figsize = (15,10))\n",
    "    plt.imshow(weight,cmap='jet')\n",
    "#     plt.show()\n",
    "    \n",
    "    return \n",
    "\n",
    "def visualise_filters(layer,figsize=(20,15)):\n",
    "    \n",
    "    weights = layer.weight.data\n",
    "\n",
    "    if weights.shape[0] > weights.shape[1]:\n",
    "        \n",
    "        weights = weights.transpose(1,0)\n",
    "        \n",
    "    rows = weights.shape[0]\n",
    "    \n",
    "    cols = weights.shape[1]\n",
    "    \n",
    "    axes = []\n",
    "    \n",
    "    fig= plt.figure()\n",
    "\n",
    "    for n in range(rows):\n",
    "\n",
    "        for c in range(cols):\n",
    "            \n",
    "            axes.append(fig.add_subplot(rows, cols, n*cols+c+1))\n",
    "\n",
    "            single_filter = weights[n,c]\n",
    "        \n",
    "            visual_single_filter(single_filter)\n",
    "            \n",
    "#     fig.tight_layout()    \n",
    "    \n",
    "    plt.show()     \n",
    "    \n",
    "    return \n",
    "\n",
    "visualise_filters(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import gmtime, strftime\n",
    "# a = time.time()\n",
    "# model  = train(model, train_loader, criterion, optimizer, 48)\n",
    "# print(time.time() - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1024)\n",
    "torch.manual_seed(1024)\n",
    "torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "class model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(model, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTraining(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PreTraining, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "from data import process_data\n",
    "from data.import_data import import_experimental_data\n",
    "# from data.process_data import DatasetObject\n",
    "from models.train import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 files.\n",
      "input_user10_bendfwd.csv annotation_user10_bendfwd.csv user10\n",
      "input_user10_kneel.csv annotation_user10_kneel.csv user10\n",
      "input_user10_lie.csv annotation_user10_lie.csv user10\n",
      "input_user10_sit.csv annotation_user10_sit.csv user10\n",
      "input_user10_sitrotate.csv annotation_user10_sitrotate.csv user10\n",
      "input_user10_stand.csv annotation_user10_stand.csv user10\n",
      "input_user10_standrotate.csv annotation_user10_standrotate.csv user10\n",
      "input_user10_walking.csv annotation_user10_walking.csv user10\n",
      "input_user1_bendfwd.csv annotation_user1_bendfwd.csv user1\n",
      "input_user1_kneel.csv annotation_user1_kneel.csv user1\n",
      "input_user1_lie.csv annotation_user1_lie.csv user1\n",
      "input_user1_sit.csv annotation_user1_sit.csv user1\n",
      "input_user1_sitrotate.csv annotation_user1_sitrotate.csv user1\n",
      "input_user1_stand.csv annotation_user1_stand.csv user1\n",
      "input_user1_standrotate.csv annotation_user1_standrotate.csv user1\n",
      "input_user1_walking.csv annotation_user1_walking.csv user1\n",
      "input_user2_bendfwd.csv annotation_user2_bendfwd.csv user2\n",
      "input_user2_kneel.csv annotation_user2_kneel.csv user2\n",
      "input_user2_lie.csv annotation_user2_lie.csv user2\n",
      "input_user2_sit.csv annotation_user2_sit.csv user2\n",
      "input_user2_sitrotate.csv annotation_user2_sitrotate.csv user2\n",
      "input_user2_stand.csv annotation_user2_stand.csv user2\n",
      "input_user2_standrotate.csv annotation_user2_standrotate.csv user2\n",
      "input_user2_walking.csv annotation_user2_walking.csv user2\n",
      "input_user3_bendfwd.csv annotation_user3_bendfwd.csv user3\n",
      "input_user3_kneel.csv annotation_user3_kneel.csv user3\n",
      "input_user3_lie.csv annotation_user3_lie.csv user3\n",
      "input_user3_sit.csv annotation_user3_sit.csv user3\n",
      "input_user3_sitrotate.csv annotation_user3_sitrotate.csv user3\n",
      "input_user3_stand.csv annotation_user3_stand.csv user3\n",
      "input_user3_standrotate.csv annotation_user3_standrotate.csv user3\n",
      "input_user3_walking.csv annotation_user3_walking.csv user3\n",
      "input_user4_bendfwd.csv annotation_user4_bendfwd.csv user4\n",
      "input_user4_kneel.csv annotation_user4_kneel.csv user4\n",
      "input_user4_lie.csv annotation_user4_lie.csv user4\n",
      "input_user4_sit.csv annotation_user4_sit.csv user4\n",
      "input_user4_sitrotate.csv annotation_user4_sitrotate.csv user4\n",
      "input_user4_stand.csv annotation_user4_stand.csv user4\n",
      "input_user4_standrotate.csv annotation_user4_standrotate.csv user4\n",
      "input_user4_walking.csv annotation_user4_walking.csv user4\n",
      "input_user5_bendfwd.csv annotation_user5_bendfwd.csv user5\n",
      "input_user5_kneel.csv annotation_user5_kneel.csv user5\n",
      "input_user5_lie.csv annotation_user5_lie.csv user5\n",
      "input_user5_sit.csv annotation_user5_sit.csv user5\n",
      "input_user5_sitrotate.csv annotation_user5_sitrotate.csv user5\n",
      "input_user5_stand.csv annotation_user5_stand.csv user5\n",
      "input_user5_standrotate.csv annotation_user5_standrotate.csv user5\n",
      "input_user5_walking.csv annotation_user5_walking.csv user5\n",
      "input_user6_bendfwd.csv annotation_user6_bendfwd.csv user6\n",
      "input_user6_kneel.csv annotation_user6_kneel.csv user6\n",
      "input_user6_lie.csv annotation_user6_lie.csv user6\n",
      "input_user6_sit.csv annotation_user6_sit.csv user6\n",
      "input_user6_sitrotate.csv annotation_user6_sitrotate.csv user6\n",
      "input_user6_stand.csv annotation_user6_stand.csv user6\n",
      "input_user6_standrotate.csv annotation_user6_standrotate.csv user6\n",
      "input_user6_walking.csv annotation_user6_walking.csv user6\n",
      "input_user7_bendfwd.csv annotation_user7_bendfwd.csv user7\n",
      "input_user7_kneel.csv annotation_user7_kneel.csv user7\n",
      "input_user7_lie.csv annotation_user7_lie.csv user7\n",
      "input_user7_sit.csv annotation_user7_sit.csv user7\n",
      "input_user7_sitrotate.csv annotation_user7_sitrotate.csv user7\n",
      "input_user7_stand.csv annotation_user7_stand.csv user7\n",
      "input_user7_standrotate.csv annotation_user7_standrotate.csv user7\n",
      "input_user7_walking.csv annotation_user7_walking.csv user7\n",
      "input_user8_bendfwd.csv annotation_user8_bendfwd.csv user8\n",
      "input_user8_kneel.csv annotation_user8_kneel.csv user8\n",
      "input_user8_lie.csv annotation_user8_lie.csv user8\n",
      "input_user8_sit.csv annotation_user8_sit.csv user8\n",
      "input_user8_sitrotate.csv annotation_user8_sitrotate.csv user8\n",
      "input_user8_stand.csv annotation_user8_stand.csv user8\n",
      "input_user8_standrotate.csv annotation_user8_standrotate.csv user8\n",
      "input_user8_walking.csv annotation_user8_walking.csv user8\n",
      "input_user9_bendfwd.csv annotation_user9_bendfwd.csv user9\n",
      "input_user9_kneel.csv annotation_user9_kneel.csv user9\n",
      "input_user9_lie.csv annotation_user9_lie.csv user9\n",
      "input_user9_sit.csv annotation_user9_sit.csv user9\n",
      "input_user9_sitrotate.csv annotation_user9_sitrotate.csv user9\n",
      "input_user9_stand.csv annotation_user9_stand.csv user9\n",
      "input_user9_standrotate.csv annotation_user9_standrotate.csv user9\n",
      "input_user9_walking.csv annotation_user9_walking.csv user9\n",
      "index 0 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 1 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 2 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 3 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 4 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 5 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 6 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 7 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 8 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "index 9 arrays sizes ------ X:  (55992, 90)  Y:  (55992, 1)  Z:  (55992, 1)\n",
      "size of DatasetObject ------ :  (10, 3)\n",
      "train set: [1, 2, 3, 4, 5, 6, 7, 8, 9] \ttest set: [0]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import data\n",
    "folderpath1 = \"E:/external_data/Experiment3/csv_files/exp_1\"\n",
    "df_exp1 = import_experimental_data(folderpath1)\n",
    "\n",
    "# 2. Process data\n",
    "X_ls, y_ls = seperate_dataframes(df_exp1)\n",
    "del df_exp1\n",
    "\n",
    "# 3. DatasetObject\n",
    "exp_1 = create_datasetobject(X_ls, y_ls)\n",
    "del X_ls,y_ls\n",
    "\n",
    "# 4. Load data into dataloaders\n",
    "idxs = [0]\n",
    "(X_train, y_train,_),(X_test, y_test,_) = exp_1(idxs,return_train_sets=True)\n",
    "train_loader, test_loader = create_dataloaders(X_train, y_train, X_test, y_test)\n",
    "del X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch 105: >>>>>>>>>>>>> loss: 404.9288024902344\n",
      "Epoch 106: >>>>>>>>>>>>> loss: 380.29608154296875\n",
      "Epoch 107: >>>>>>>>>>>>> loss: 346.3295593261719\n",
      "Epoch 108: >>>>>>>>>>>>> loss: 404.4480895996094\n",
      "Epoch 109: >>>>>>>>>>>>> loss: 374.84027099609375\n",
      "Epoch 110: >>>>>>>>>>>>> loss: 394.3640441894531\n",
      "Epoch 111: >>>>>>>>>>>>> loss: 395.0081787109375\n",
      "Epoch 112: >>>>>>>>>>>>> loss: 384.3662109375\n",
      "Epoch 113: >>>>>>>>>>>>> loss: 419.6768798828125\n",
      "Epoch 114: >>>>>>>>>>>>> loss: 382.2357482910156\n",
      "Epoch 115: >>>>>>>>>>>>> loss: 400.4671630859375\n",
      "Epoch 116: >>>>>>>>>>>>> loss: 419.6222229003906\n",
      "Epoch 117: >>>>>>>>>>>>> loss: 572.9689331054688\n",
      "Epoch 118: >>>>>>>>>>>>> loss: 551.9775390625\n",
      "Epoch 119: >>>>>>>>>>>>> loss: 472.08685302734375\n",
      "Epoch 120: >>>>>>>>>>>>> loss: 459.9753723144531\n",
      "Epoch 121: >>>>>>>>>>>>> loss: 428.83074951171875\n",
      "Epoch 122: >>>>>>>>>>>>> loss: 387.3523864746094\n",
      "Epoch 123: >>>>>>>>>>>>> loss: 404.9145812988281\n",
      "Epoch 124: >>>>>>>>>>>>> loss: 368.30902099609375\n",
      "Epoch 125: >>>>>>>>>>>>> loss: 376.7373352050781\n",
      "Epoch 126: >>>>>>>>>>>>> loss: 400.10467529296875\n",
      "Epoch 127: >>>>>>>>>>>>> loss: 409.1720886230469\n",
      "Epoch 128: >>>>>>>>>>>>> loss: 414.8338623046875\n",
      "Epoch 129: >>>>>>>>>>>>> loss: 384.8233642578125\n",
      "Epoch 130: >>>>>>>>>>>>> loss: 398.0115661621094\n",
      "Epoch 131: >>>>>>>>>>>>> loss: 399.992919921875\n",
      "Epoch 132: >>>>>>>>>>>>> loss: 393.47564697265625\n",
      "Epoch 133: >>>>>>>>>>>>> loss: 377.6020812988281\n",
      "Epoch 134: >>>>>>>>>>>>> loss: 415.6776123046875\n",
      "Epoch 135: >>>>>>>>>>>>> loss: 401.56878662109375\n",
      "Epoch 136: >>>>>>>>>>>>> loss: 378.3880920410156\n",
      "Epoch 137: >>>>>>>>>>>>> loss: 378.6496887207031\n",
      "Epoch 138: >>>>>>>>>>>>> loss: 376.92529296875\n",
      "Epoch 139: >>>>>>>>>>>>> loss: 373.2276306152344\n",
      "Epoch 140: >>>>>>>>>>>>> loss: 395.6400451660156\n",
      "Epoch 141: >>>>>>>>>>>>> loss: 379.1392822265625\n",
      "Epoch 142: >>>>>>>>>>>>> loss: 391.481689453125\n",
      "Epoch 143: >>>>>>>>>>>>> loss: 383.5667419433594\n",
      "Epoch 144: >>>>>>>>>>>>> loss: 372.9677734375\n",
      "Epoch 145: >>>>>>>>>>>>> loss: 399.0340881347656\n",
      "Epoch 146: >>>>>>>>>>>>> loss: 391.8900451660156\n",
      "Epoch 147: >>>>>>>>>>>>> loss: 375.3111877441406\n",
      "Epoch 148: >>>>>>>>>>>>> loss: 365.67138671875\n",
      "Epoch 149: >>>>>>>>>>>>> loss: 402.26666259765625\n",
      "Epoch 150: >>>>>>>>>>>>> loss: 391.12457275390625\n",
      "Epoch 151: >>>>>>>>>>>>> loss: 394.0849914550781\n",
      "Epoch 152: >>>>>>>>>>>>> loss: 385.0057678222656\n",
      "Epoch 153: >>>>>>>>>>>>> loss: 381.6777648925781\n",
      "Epoch 154: >>>>>>>>>>>>> loss: 393.6404113769531\n",
      "Epoch 155: >>>>>>>>>>>>> loss: 404.4588928222656\n",
      "Epoch 156: >>>>>>>>>>>>> loss: 399.2554626464844\n",
      "Epoch 157: >>>>>>>>>>>>> loss: 379.0878601074219\n",
      "Epoch 158: >>>>>>>>>>>>> loss: 392.7464904785156\n",
      "Epoch 159: >>>>>>>>>>>>> loss: 370.7097473144531\n",
      "Epoch 160: >>>>>>>>>>>>> loss: 382.55035400390625\n",
      "Epoch 161: >>>>>>>>>>>>> loss: 369.01947021484375\n",
      "Epoch 162: >>>>>>>>>>>>> loss: 389.0409240722656\n",
      "Epoch 163: >>>>>>>>>>>>> loss: 346.1990966796875\n",
      "Epoch 164: >>>>>>>>>>>>> loss: 370.9296569824219\n",
      "Epoch 165: >>>>>>>>>>>>> loss: 387.40863037109375\n",
      "Epoch 166: >>>>>>>>>>>>> loss: 362.9643859863281\n",
      "Epoch 167: >>>>>>>>>>>>> loss: 375.2839660644531\n",
      "Epoch 168: >>>>>>>>>>>>> loss: 369.0340881347656\n",
      "Epoch 169: >>>>>>>>>>>>> loss: 381.12347412109375\n",
      "Epoch 170: >>>>>>>>>>>>> loss: 382.5167541503906\n",
      "Epoch 171: >>>>>>>>>>>>> loss: 374.16534423828125\n",
      "Epoch 172: >>>>>>>>>>>>> loss: 378.7361755371094\n",
      "Epoch 173: >>>>>>>>>>>>> loss: 417.71795654296875\n",
      "Epoch 174: >>>>>>>>>>>>> loss: 387.3095703125\n",
      "Epoch 175: >>>>>>>>>>>>> loss: 398.7179260253906\n",
      "Epoch 176: >>>>>>>>>>>>> loss: 357.707763671875\n",
      "Epoch 177: >>>>>>>>>>>>> loss: 403.22906494140625\n",
      "Epoch 178: >>>>>>>>>>>>> loss: 371.31085205078125\n",
      "Epoch 179: >>>>>>>>>>>>> loss: 379.51116943359375\n",
      "Epoch 180: >>>>>>>>>>>>> loss: 371.9106750488281\n",
      "Epoch 181: >>>>>>>>>>>>> loss: 381.03875732421875\n",
      "Epoch 182: >>>>>>>>>>>>> loss: 390.6269226074219\n",
      "Epoch 183: >>>>>>>>>>>>> loss: 365.50445556640625\n",
      "Epoch 184: >>>>>>>>>>>>> loss: 371.6724853515625\n",
      "Epoch 185: >>>>>>>>>>>>> loss: 376.8695068359375\n",
      "Epoch 186: >>>>>>>>>>>>> loss: 373.1820068359375\n",
      "Epoch 187: >>>>>>>>>>>>> loss: 360.1983642578125\n",
      "Epoch 188: >>>>>>>>>>>>> loss: 366.7449645996094\n",
      "Epoch 189: >>>>>>>>>>>>> loss: 346.4981689453125\n",
      "Epoch 190: >>>>>>>>>>>>> loss: 380.39520263671875\n",
      "Epoch 191: >>>>>>>>>>>>> loss: 371.724365234375\n",
      "Epoch 192: >>>>>>>>>>>>> loss: 381.8864440917969\n",
      "Epoch 193: >>>>>>>>>>>>> loss: 390.16748046875\n",
      "Epoch 194: >>>>>>>>>>>>> loss: 420.2523193359375\n",
      "Epoch 195: >>>>>>>>>>>>> loss: 380.2215881347656\n",
      "Epoch 196: >>>>>>>>>>>>> loss: 367.7498474121094\n",
      "Epoch 197: >>>>>>>>>>>>> loss: 339.1604919433594\n",
      "Epoch 198: >>>>>>>>>>>>> loss: 379.7235412597656\n",
      "Epoch 199: >>>>>>>>>>>>> loss: 404.984619140625\n",
      "Epoch 200: >>>>>>>>>>>>> loss: 364.9639892578125\n",
      "save checkpoint in : ./models/saved_models/AEPretrain_module_checkpoint_200__2021_01_13_12_13\n",
      "Epoch 201: >>>>>>>>>>>>> loss: 366.9754333496094\n",
      "Epoch 202: >>>>>>>>>>>>> loss: 362.83477783203125\n",
      "Epoch 203: >>>>>>>>>>>>> loss: 407.345703125\n",
      "Epoch 204: >>>>>>>>>>>>> loss: 350.8680725097656\n",
      "Epoch 205: >>>>>>>>>>>>> loss: 389.771240234375\n",
      "Epoch 206: >>>>>>>>>>>>> loss: 367.55474853515625\n",
      "Epoch 207: >>>>>>>>>>>>> loss: 381.7515869140625\n",
      "Epoch 208: >>>>>>>>>>>>> loss: 392.7127990722656\n",
      "Epoch 209: >>>>>>>>>>>>> loss: 355.1074523925781\n",
      "Epoch 210: >>>>>>>>>>>>> loss: 366.5833435058594\n",
      "Epoch 211: >>>>>>>>>>>>> loss: 372.9560852050781\n",
      "Epoch 212: >>>>>>>>>>>>> loss: 378.3883056640625\n",
      "Epoch 213: >>>>>>>>>>>>> loss: 408.9779968261719\n",
      "Epoch 214: >>>>>>>>>>>>> loss: 412.36444091796875\n",
      "Epoch 215: >>>>>>>>>>>>> loss: 366.20538330078125\n",
      "Epoch 216: >>>>>>>>>>>>> loss: 383.0339660644531\n",
      "Epoch 217: >>>>>>>>>>>>> loss: 393.9681701660156\n",
      "Epoch 218: >>>>>>>>>>>>> loss: 373.42315673828125\n",
      "Epoch 219: >>>>>>>>>>>>> loss: 369.27276611328125\n",
      "Epoch 220: >>>>>>>>>>>>> loss: 400.00079345703125\n",
      "Epoch 221: >>>>>>>>>>>>> loss: 355.52587890625\n",
      "Epoch 222: >>>>>>>>>>>>> loss: 371.9017028808594\n",
      "Epoch 223: >>>>>>>>>>>>> loss: 367.23040771484375\n",
      "Epoch 224: >>>>>>>>>>>>> loss: 368.3473205566406\n",
      "Epoch 225: >>>>>>>>>>>>> loss: 374.5209655761719\n",
      "Epoch 226: >>>>>>>>>>>>> loss: 389.1584777832031\n",
      "Epoch 227: >>>>>>>>>>>>> loss: 377.9040832519531\n",
      "Epoch 228: >>>>>>>>>>>>> loss: 403.2281799316406\n",
      "Epoch 229: >>>>>>>>>>>>> loss: 362.5959167480469\n",
      "Epoch 230: >>>>>>>>>>>>> loss: 371.7010498046875\n",
      "Epoch 231: >>>>>>>>>>>>> loss: 361.61285400390625\n",
      "Epoch 232: >>>>>>>>>>>>> loss: 378.5235900878906\n",
      "Epoch 233: >>>>>>>>>>>>> loss: 380.6766357421875\n",
      "Epoch 234: >>>>>>>>>>>>> loss: 380.0660095214844\n",
      "Epoch 235: >>>>>>>>>>>>> loss: 362.57623291015625\n",
      "Epoch 236: >>>>>>>>>>>>> loss: 362.6252136230469\n",
      "Epoch 237: >>>>>>>>>>>>> loss: 369.74407958984375\n",
      "Epoch 238: >>>>>>>>>>>>> loss: 381.3927917480469\n",
      "Epoch 239: >>>>>>>>>>>>> loss: 377.61529541015625\n",
      "Epoch 240: >>>>>>>>>>>>> loss: 367.31866455078125\n",
      "Epoch 241: >>>>>>>>>>>>> loss: 398.5310363769531\n",
      "Epoch 242: >>>>>>>>>>>>> loss: 528.5384521484375\n",
      "Epoch 243: >>>>>>>>>>>>> loss: 413.13421630859375\n",
      "Epoch 244: >>>>>>>>>>>>> loss: 372.98638916015625\n",
      "Epoch 245: >>>>>>>>>>>>> loss: 380.2268371582031\n",
      "Epoch 246: >>>>>>>>>>>>> loss: 386.84698486328125\n",
      "Epoch 247: >>>>>>>>>>>>> loss: 370.4731140136719\n",
      "Epoch 248: >>>>>>>>>>>>> loss: 389.78924560546875\n",
      "Epoch 249: >>>>>>>>>>>>> loss: 361.07049560546875\n",
      "Epoch 250: >>>>>>>>>>>>> loss: 340.7955627441406\n",
      "Epoch 251: >>>>>>>>>>>>> loss: 372.10284423828125\n",
      "Epoch 252: >>>>>>>>>>>>> loss: 361.170166015625\n",
      "Epoch 253: >>>>>>>>>>>>> loss: 367.5504455566406\n",
      "Epoch 254: >>>>>>>>>>>>> loss: 363.13433837890625\n",
      "Epoch 255: >>>>>>>>>>>>> loss: 352.2354736328125\n",
      "Epoch 256: >>>>>>>>>>>>> loss: 369.7864685058594\n",
      "Epoch 257: >>>>>>>>>>>>> loss: 374.93865966796875\n",
      "Epoch 258: >>>>>>>>>>>>> loss: 391.5775146484375\n",
      "Epoch 259: >>>>>>>>>>>>> loss: 387.20794677734375\n",
      "Epoch 260: >>>>>>>>>>>>> loss: 386.07470703125\n",
      "Epoch 261: >>>>>>>>>>>>> loss: 370.3722229003906\n",
      "Epoch 262: >>>>>>>>>>>>> loss: 384.4020080566406\n",
      "Epoch 263: >>>>>>>>>>>>> loss: 374.8177185058594\n",
      "Epoch 264: >>>>>>>>>>>>> loss: 357.65545654296875\n",
      "Epoch 265: >>>>>>>>>>>>> loss: 386.2044372558594\n",
      "Epoch 266: >>>>>>>>>>>>> loss: 362.2113037109375\n",
      "Epoch 267: >>>>>>>>>>>>> loss: 387.4832763671875\n",
      "Epoch 268: >>>>>>>>>>>>> loss: 367.43194580078125\n",
      "Epoch 269: >>>>>>>>>>>>> loss: 359.9817199707031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 270: >>>>>>>>>>>>> loss: 397.2841796875\n",
      "Epoch 271: >>>>>>>>>>>>> loss: 364.1202392578125\n",
      "Epoch 272: >>>>>>>>>>>>> loss: 353.5174560546875\n",
      "Epoch 273: >>>>>>>>>>>>> loss: 372.90618896484375\n",
      "Epoch 274: >>>>>>>>>>>>> loss: 366.4631652832031\n",
      "Epoch 275: >>>>>>>>>>>>> loss: 385.9159851074219\n",
      "Epoch 276: >>>>>>>>>>>>> loss: 389.7668762207031\n",
      "Epoch 277: >>>>>>>>>>>>> loss: 363.4786071777344\n",
      "Epoch 278: >>>>>>>>>>>>> loss: 399.7900390625\n",
      "Epoch 279: >>>>>>>>>>>>> loss: 356.1817626953125\n",
      "Epoch 280: >>>>>>>>>>>>> loss: 380.22418212890625\n",
      "Epoch 281: >>>>>>>>>>>>> loss: 359.0148010253906\n",
      "Epoch 282: >>>>>>>>>>>>> loss: 334.2544250488281\n",
      "Epoch 283: >>>>>>>>>>>>> loss: 343.4923095703125\n",
      "Epoch 284: >>>>>>>>>>>>> loss: 350.2142639160156\n",
      "Epoch 285: >>>>>>>>>>>>> loss: 374.9696960449219\n",
      "Epoch 286: >>>>>>>>>>>>> loss: 377.9765319824219\n",
      "Epoch 287: >>>>>>>>>>>>> loss: 355.0858154296875\n",
      "Epoch 288: >>>>>>>>>>>>> loss: 368.6839904785156\n",
      "Epoch 289: >>>>>>>>>>>>> loss: 359.736328125\n",
      "Epoch 290: >>>>>>>>>>>>> loss: 381.9800109863281\n",
      "Epoch 291: >>>>>>>>>>>>> loss: 374.1278991699219\n",
      "Epoch 292: >>>>>>>>>>>>> loss: 330.01416015625\n",
      "Epoch 293: >>>>>>>>>>>>> loss: 329.0580749511719\n",
      "Epoch 294: >>>>>>>>>>>>> loss: 369.3190612792969\n",
      "Epoch 295: >>>>>>>>>>>>> loss: 364.85711669921875\n",
      "Epoch 296: >>>>>>>>>>>>> loss: 416.686767578125\n",
      "Epoch 297: >>>>>>>>>>>>> loss: 365.9120178222656\n",
      "Epoch 298: >>>>>>>>>>>>> loss: 380.00067138671875\n",
      "Epoch 299: >>>>>>>>>>>>> loss: 356.10198974609375\n",
      "Epoch 300: >>>>>>>>>>>>> loss: 375.2721862792969\n",
      "save checkpoint in : ./models/saved_models/AEPretrain_module_checkpoint_300__2021_01_13_13_27\n",
      "Epoch 301: >>>>>>>>>>>>> loss: 372.7392578125\n",
      "Epoch 302: >>>>>>>>>>>>> loss: 343.7509460449219\n",
      "Epoch 303: >>>>>>>>>>>>> loss: 385.2886962890625\n",
      "Epoch 304: >>>>>>>>>>>>> loss: 346.60821533203125\n",
      "Epoch 305: >>>>>>>>>>>>> loss: 369.5537414550781\n",
      "Epoch 306: >>>>>>>>>>>>> loss: 365.4984436035156\n",
      "Epoch 307: >>>>>>>>>>>>> loss: 351.2285461425781\n",
      "Epoch 308: >>>>>>>>>>>>> loss: 332.6311340332031\n",
      "Epoch 309: >>>>>>>>>>>>> loss: 346.4692077636719\n",
      "Epoch 310: >>>>>>>>>>>>> loss: 394.6999206542969\n",
      "Epoch 311: >>>>>>>>>>>>> loss: 362.0399475097656\n",
      "Epoch 312: >>>>>>>>>>>>> loss: 362.1613464355469\n",
      "Epoch 313: >>>>>>>>>>>>> loss: 348.20294189453125\n",
      "Epoch 314: >>>>>>>>>>>>> loss: 340.98291015625\n",
      "Epoch 315: >>>>>>>>>>>>> loss: 355.7679748535156\n",
      "Epoch 316: >>>>>>>>>>>>> loss: 357.14892578125\n",
      "Epoch 317: >>>>>>>>>>>>> loss: 378.91534423828125\n",
      "Epoch 318: >>>>>>>>>>>>> loss: 371.40472412109375\n",
      "Epoch 319: >>>>>>>>>>>>> loss: 345.4648132324219\n",
      "Epoch 320: >>>>>>>>>>>>> loss: 352.2268981933594\n",
      "Epoch 321: >>>>>>>>>>>>> loss: 338.08660888671875\n",
      "Epoch 322: >>>>>>>>>>>>> loss: 353.1050109863281\n",
      "Epoch 323: >>>>>>>>>>>>> loss: 366.3208312988281\n",
      "Epoch 324: >>>>>>>>>>>>> loss: 367.0311279296875\n",
      "Epoch 325: >>>>>>>>>>>>> loss: 356.6055603027344\n",
      "Epoch 326: >>>>>>>>>>>>> loss: 360.2143249511719\n",
      "Epoch 327: >>>>>>>>>>>>> loss: 349.2088928222656\n",
      "Epoch 328: >>>>>>>>>>>>> loss: 366.83502197265625\n",
      "Epoch 329: >>>>>>>>>>>>> loss: 361.47967529296875\n",
      "Epoch 330: >>>>>>>>>>>>> loss: 341.3940124511719\n",
      "Epoch 331: >>>>>>>>>>>>> loss: 377.18572998046875\n",
      "Epoch 332: >>>>>>>>>>>>> loss: 349.26495361328125\n",
      "Epoch 333: >>>>>>>>>>>>> loss: 383.2687683105469\n",
      "Epoch 334: >>>>>>>>>>>>> loss: 342.4791259765625\n",
      "Epoch 335: >>>>>>>>>>>>> loss: 361.35723876953125\n",
      "Epoch 336: >>>>>>>>>>>>> loss: 338.9581604003906\n",
      "Epoch 337: >>>>>>>>>>>>> loss: 350.9101867675781\n",
      "Epoch 338: >>>>>>>>>>>>> loss: 355.3584899902344\n",
      "Epoch 339: >>>>>>>>>>>>> loss: 348.08123779296875\n",
      "Epoch 340: >>>>>>>>>>>>> loss: 344.08123779296875\n",
      "Epoch 341: >>>>>>>>>>>>> loss: 375.2967529296875\n",
      "Epoch 342: >>>>>>>>>>>>> loss: 351.1624450683594\n",
      "Epoch 343: >>>>>>>>>>>>> loss: 343.6591491699219\n",
      "Epoch 344: >>>>>>>>>>>>> loss: 375.3709411621094\n",
      "Epoch 345: >>>>>>>>>>>>> loss: 361.6717834472656\n",
      "Epoch 346: >>>>>>>>>>>>> loss: 351.0969543457031\n",
      "Epoch 347: >>>>>>>>>>>>> loss: 361.9808349609375\n",
      "Epoch 348: >>>>>>>>>>>>> loss: 358.2420349121094\n",
      "Epoch 349: >>>>>>>>>>>>> loss: 354.5975341796875\n",
      "Epoch 350: >>>>>>>>>>>>> loss: 345.94049072265625\n",
      "Epoch 351: >>>>>>>>>>>>> loss: 360.4749450683594\n",
      "Epoch 352: >>>>>>>>>>>>> loss: 365.46173095703125\n",
      "Epoch 353: >>>>>>>>>>>>> loss: 364.5799560546875\n",
      "Epoch 354: >>>>>>>>>>>>> loss: 394.2265319824219\n",
      "Epoch 355: >>>>>>>>>>>>> loss: 344.4340515136719\n",
      "Epoch 356: >>>>>>>>>>>>> loss: 366.85589599609375\n",
      "Epoch 357: >>>>>>>>>>>>> loss: 375.26934814453125\n",
      "Epoch 358: >>>>>>>>>>>>> loss: 352.5536804199219\n",
      "Epoch 359: >>>>>>>>>>>>> loss: 365.3118591308594\n",
      "Epoch 360: >>>>>>>>>>>>> loss: 377.56048583984375\n",
      "Epoch 361: >>>>>>>>>>>>> loss: 390.21368408203125\n",
      "Epoch 362: >>>>>>>>>>>>> loss: 369.322265625\n",
      "Epoch 363: >>>>>>>>>>>>> loss: 370.39434814453125\n",
      "Epoch 364: >>>>>>>>>>>>> loss: 357.9026184082031\n",
      "Epoch 365: >>>>>>>>>>>>> loss: 351.9729919433594\n",
      "Epoch 366: >>>>>>>>>>>>> loss: 353.6394348144531\n",
      "Epoch 367: >>>>>>>>>>>>> loss: 345.157958984375\n",
      "Epoch 368: >>>>>>>>>>>>> loss: 339.82464599609375\n",
      "Epoch 369: >>>>>>>>>>>>> loss: 370.2328796386719\n",
      "Epoch 370: >>>>>>>>>>>>> loss: 368.0623779296875\n",
      "Epoch 371: >>>>>>>>>>>>> loss: 378.11907958984375\n",
      "Epoch 372: >>>>>>>>>>>>> loss: 395.14959716796875\n",
      "Epoch 373: >>>>>>>>>>>>> loss: 373.8992004394531\n",
      "Epoch 374: >>>>>>>>>>>>> loss: 386.6300354003906\n",
      "Epoch 375: >>>>>>>>>>>>> loss: 335.48956298828125\n",
      "Epoch 376: >>>>>>>>>>>>> loss: 346.9210510253906\n",
      "Epoch 377: >>>>>>>>>>>>> loss: 324.3981628417969\n",
      "Epoch 378: >>>>>>>>>>>>> loss: 370.0219421386719\n",
      "Epoch 379: >>>>>>>>>>>>> loss: 399.88836669921875\n",
      "Epoch 380: >>>>>>>>>>>>> loss: 336.36602783203125\n",
      "Epoch 381: >>>>>>>>>>>>> loss: 346.72235107421875\n",
      "Epoch 382: >>>>>>>>>>>>> loss: 372.00341796875\n",
      "Epoch 383: >>>>>>>>>>>>> loss: 357.0324401855469\n",
      "Epoch 384: >>>>>>>>>>>>> loss: 348.4622802734375\n",
      "Epoch 385: >>>>>>>>>>>>> loss: 356.0016784667969\n",
      "Epoch 386: >>>>>>>>>>>>> loss: 368.3161315917969\n",
      "Epoch 387: >>>>>>>>>>>>> loss: 351.34619140625\n",
      "Epoch 388: >>>>>>>>>>>>> loss: 366.0240173339844\n",
      "Epoch 389: >>>>>>>>>>>>> loss: 371.5695495605469\n",
      "Epoch 390: >>>>>>>>>>>>> loss: 378.0728759765625\n",
      "Epoch 391: >>>>>>>>>>>>> loss: 355.34600830078125\n",
      "Epoch 392: >>>>>>>>>>>>> loss: 358.3124084472656\n",
      "Epoch 393: >>>>>>>>>>>>> loss: 340.6894226074219\n",
      "Epoch 394: >>>>>>>>>>>>> loss: 338.94036865234375\n",
      "Epoch 395: >>>>>>>>>>>>> loss: 336.5049743652344\n",
      "Epoch 396: >>>>>>>>>>>>> loss: 350.3034362792969\n",
      "Epoch 397: >>>>>>>>>>>>> loss: 351.940185546875\n",
      "Epoch 398: >>>>>>>>>>>>> loss: 365.7965393066406\n",
      "Epoch 399: >>>>>>>>>>>>> loss: 355.5451354980469\n",
      "Epoch 400: >>>>>>>>>>>>> loss: 341.970947265625\n",
      "save checkpoint in : ./models/saved_models/AEPretrain_module_checkpoint_400__2021_01_13_14_38\n",
      "Epoch 401: >>>>>>>>>>>>> loss: 367.24554443359375\n",
      "Epoch 402: >>>>>>>>>>>>> loss: 382.025634765625\n",
      "Epoch 403: >>>>>>>>>>>>> loss: 332.98724365234375\n",
      "Epoch 404: >>>>>>>>>>>>> loss: 373.22772216796875\n",
      "Epoch 405: >>>>>>>>>>>>> loss: 356.53631591796875\n",
      "Epoch 406: >>>>>>>>>>>>> loss: 364.8653869628906\n",
      "Epoch 407: >>>>>>>>>>>>> loss: 313.86590576171875\n",
      "Epoch 408: >>>>>>>>>>>>> loss: 381.8017883300781\n",
      "Epoch 409: >>>>>>>>>>>>> loss: 350.7975158691406\n",
      "Epoch 410: >>>>>>>>>>>>> loss: 341.6373291015625\n",
      "Epoch 411: >>>>>>>>>>>>> loss: 380.80035400390625\n",
      "Epoch 412: >>>>>>>>>>>>> loss: 390.8195495605469\n",
      "Epoch 413: >>>>>>>>>>>>> loss: 364.292236328125\n",
      "Epoch 414: >>>>>>>>>>>>> loss: 356.760986328125\n",
      "Epoch 415: >>>>>>>>>>>>> loss: 328.61328125\n",
      "Epoch 416: >>>>>>>>>>>>> loss: 364.561767578125\n",
      "Epoch 417: >>>>>>>>>>>>> loss: 361.2196350097656\n",
      "Epoch 418: >>>>>>>>>>>>> loss: 367.6268005371094\n",
      "Epoch 419: >>>>>>>>>>>>> loss: 366.9775390625\n",
      "Epoch 420: >>>>>>>>>>>>> loss: 356.7273254394531\n",
      "Epoch 421: >>>>>>>>>>>>> loss: 342.3283386230469\n",
      "Epoch 422: >>>>>>>>>>>>> loss: 362.38671875\n",
      "Epoch 423: >>>>>>>>>>>>> loss: 342.4593200683594\n",
      "Epoch 424: >>>>>>>>>>>>> loss: 354.912353515625\n",
      "Epoch 425: >>>>>>>>>>>>> loss: 356.5728454589844\n",
      "Epoch 426: >>>>>>>>>>>>> loss: 351.66217041015625\n",
      "Epoch 427: >>>>>>>>>>>>> loss: 335.6192626953125\n",
      "Epoch 428: >>>>>>>>>>>>> loss: 360.9325256347656\n",
      "Epoch 429: >>>>>>>>>>>>> loss: 370.6105651855469\n",
      "Epoch 430: >>>>>>>>>>>>> loss: 393.00567626953125\n",
      "Epoch 431: >>>>>>>>>>>>> loss: 375.0879211425781\n",
      "Epoch 432: >>>>>>>>>>>>> loss: 368.5274963378906\n",
      "Epoch 433: >>>>>>>>>>>>> loss: 355.15643310546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 434: >>>>>>>>>>>>> loss: 340.6492614746094\n",
      "Epoch 435: >>>>>>>>>>>>> loss: 357.8119201660156\n",
      "Epoch 436: >>>>>>>>>>>>> loss: 341.4598083496094\n",
      "Epoch 437: >>>>>>>>>>>>> loss: 351.6108093261719\n",
      "Epoch 438: >>>>>>>>>>>>> loss: 394.90484619140625\n",
      "Epoch 439: >>>>>>>>>>>>> loss: 375.1571044921875\n",
      "Epoch 440: >>>>>>>>>>>>> loss: 364.2898864746094\n",
      "Epoch 441: >>>>>>>>>>>>> loss: 337.50067138671875\n",
      "Epoch 442: >>>>>>>>>>>>> loss: 380.54168701171875\n",
      "Epoch 443: >>>>>>>>>>>>> loss: 365.7223205566406\n",
      "Epoch 444: >>>>>>>>>>>>> loss: 347.37860107421875\n",
      "Epoch 445: >>>>>>>>>>>>> loss: 336.8121032714844\n",
      "Epoch 446: >>>>>>>>>>>>> loss: 338.69183349609375\n",
      "Epoch 447: >>>>>>>>>>>>> loss: 348.83441162109375\n",
      "Epoch 448: >>>>>>>>>>>>> loss: 367.85302734375\n",
      "Epoch 449: >>>>>>>>>>>>> loss: 393.9852294921875\n",
      "Epoch 450: >>>>>>>>>>>>> loss: 362.5007019042969\n",
      "Epoch 451: >>>>>>>>>>>>> loss: 356.92596435546875\n",
      "Epoch 452: >>>>>>>>>>>>> loss: 364.88897705078125\n",
      "Epoch 453: >>>>>>>>>>>>> loss: 355.4370422363281\n",
      "Epoch 454: >>>>>>>>>>>>> loss: 354.80621337890625\n",
      "Epoch 455: >>>>>>>>>>>>> loss: 361.9556884765625\n",
      "Epoch 456: >>>>>>>>>>>>> loss: 350.5718688964844\n",
      "Epoch 457: >>>>>>>>>>>>> loss: 370.0607604980469\n",
      "Epoch 458: >>>>>>>>>>>>> loss: 372.52154541015625\n",
      "Epoch 459: >>>>>>>>>>>>> loss: 347.5550537109375\n",
      "Epoch 460: >>>>>>>>>>>>> loss: 348.0916442871094\n",
      "Epoch 461: >>>>>>>>>>>>> loss: 341.5867004394531\n",
      "Epoch 462: >>>>>>>>>>>>> loss: 350.3385314941406\n",
      "Epoch 463: >>>>>>>>>>>>> loss: 345.7333984375\n",
      "Epoch 464: >>>>>>>>>>>>> loss: 342.3935241699219\n",
      "Epoch 465: >>>>>>>>>>>>> loss: 334.7998962402344\n",
      "Epoch 466: >>>>>>>>>>>>> loss: 343.7641906738281\n",
      "Epoch 467: >>>>>>>>>>>>> loss: 376.3411560058594\n",
      "Epoch 468: >>>>>>>>>>>>> loss: 342.7993469238281\n",
      "Epoch 469: >>>>>>>>>>>>> loss: 354.443603515625\n",
      "Epoch 470: >>>>>>>>>>>>> loss: 371.9222412109375\n",
      "Epoch 471: >>>>>>>>>>>>> loss: 351.4447021484375\n",
      "Epoch 472: >>>>>>>>>>>>> loss: 360.6323547363281\n",
      "Epoch 473: >>>>>>>>>>>>> loss: 344.35107421875\n",
      "Epoch 474: >>>>>>>>>>>>> loss: 353.7673034667969\n",
      "Epoch 475: >>>>>>>>>>>>> loss: 331.8370361328125\n",
      "Epoch 476: >>>>>>>>>>>>> loss: 331.8475036621094\n",
      "Epoch 477: >>>>>>>>>>>>> loss: 352.97552490234375\n",
      "Epoch 478: >>>>>>>>>>>>> loss: 328.07281494140625\n",
      "Epoch 479: >>>>>>>>>>>>> loss: 343.0177307128906\n",
      "Epoch 480: >>>>>>>>>>>>> loss: 341.5240173339844\n",
      "Epoch 481: >>>>>>>>>>>>> loss: 338.5186767578125\n",
      "Epoch 482: >>>>>>>>>>>>> loss: 338.9825134277344\n",
      "Epoch 483: >>>>>>>>>>>>> loss: 382.6437683105469\n",
      "Epoch 484: >>>>>>>>>>>>> loss: 366.5208740234375\n",
      "Epoch 485: >>>>>>>>>>>>> loss: 348.95062255859375\n",
      "Epoch 486: >>>>>>>>>>>>> loss: 370.2817687988281\n",
      "Epoch 487: >>>>>>>>>>>>> loss: 351.2138671875\n",
      "Epoch 488: >>>>>>>>>>>>> loss: 382.2080993652344\n",
      "Epoch 489: >>>>>>>>>>>>> loss: 345.2935485839844\n",
      "Epoch 490: >>>>>>>>>>>>> loss: 334.7454528808594\n",
      "Epoch 491: >>>>>>>>>>>>> loss: 353.9278564453125\n",
      "Epoch 492: >>>>>>>>>>>>> loss: 353.6506652832031\n",
      "Epoch 493: >>>>>>>>>>>>> loss: 393.1413269042969\n",
      "Epoch 494: >>>>>>>>>>>>> loss: 342.70892333984375\n",
      "Epoch 495: >>>>>>>>>>>>> loss: 360.1168518066406\n",
      "Epoch 496: >>>>>>>>>>>>> loss: 349.1155090332031\n",
      "Epoch 497: >>>>>>>>>>>>> loss: 351.3597106933594\n",
      "Epoch 498: >>>>>>>>>>>>> loss: 378.2656555175781\n",
      "Epoch 499: >>>>>>>>>>>>> loss: 355.53253173828125\n",
      "Epoch 500: >>>>>>>>>>>>> loss: 347.1815490722656\n",
      "save checkpoint in : ./models/saved_models/AEPretrain_module_checkpoint_500__2021_01_13_15_56\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 500\n",
    "model  = train_unsupervised(model=model, train_loader=train_loader, criterion=criterion, \n",
    "                 optimizer=optimizer, end=epochs, start=105, evaluation=None, auto_save=True,\n",
    "                 name='AEPretrain_module',test_loader=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = Autoencoder(Encoder(),Decoder())\n",
    "criterion, optimizer = setting(mdl)\n",
    "mdl,_ = load_checkpoint(mdl,optimizer,'./models/saved_models/AEPretrain_module_checkpoint_500__2021_01_13_15_56')\n",
    "encoder = mdl.encoder\n",
    "cnn = CNN_module(encoder,Classifier(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion, optimizer = setting(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch 100: >>>>>>>>>>>>> loss: 0.01584584452211857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.36      0.50        25\n",
      "           1       1.00      0.92      0.96        25\n",
      "           2       1.00      0.36      0.53        25\n",
      "           3       0.38      0.48      0.42        25\n",
      "           4       0.72      0.52      0.60        25\n",
      "           5       0.50      0.60      0.55        25\n",
      "           6       0.56      1.00      0.71        25\n",
      "           7       0.12      0.16      0.14        25\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.64      0.55      0.55       200\n",
      "weighted avg       0.64      0.55      0.55       200\n",
      "\n",
      "Evaluation:\n",
      "[[ 9  0  0  0  0  6  0 10]\n",
      " [ 0 23  0  2  0  0  0  0]\n",
      " [ 2  0  9 11  0  0  0  3]\n",
      " [ 0  0  0 12  0  7  0  6]\n",
      " [ 0  0  0  2 13  2  0  8]\n",
      " [ 0  0  0  4  5 15  0  1]\n",
      " [ 0  0  0  0  0  0 25  0]\n",
      " [ 0  0  0  1  0  0 20  4]]\n",
      "save checkpoint in : ./models/saved_models/AEPretrain_baseline_checkpoint_100__2021_01_13_17_37\n",
      "Epoch 101: >>>>>>>>>>>>> loss: 0.0005594043177552521\n",
      "Epoch 102: >>>>>>>>>>>>> loss: 0.0013828568626195192\n",
      "Epoch 103: >>>>>>>>>>>>> loss: 0.015202856622636318\n",
      "Epoch 104: >>>>>>>>>>>>> loss: 0.006512579042464495\n",
      "Epoch 105: >>>>>>>>>>>>> loss: 0.007095505483448505\n",
      "Epoch 106: >>>>>>>>>>>>> loss: 0.0010189084568992257\n",
      "Epoch 107: >>>>>>>>>>>>> loss: 0.002954617841169238\n",
      "Epoch 108: >>>>>>>>>>>>> loss: 0.0033832381013780832\n",
      "Epoch 109: >>>>>>>>>>>>> loss: 0.018439553678035736\n",
      "Epoch 110: >>>>>>>>>>>>> loss: 0.023985767737030983\n",
      "Epoch 111: >>>>>>>>>>>>> loss: 0.03576795011758804\n",
      "Epoch 112: >>>>>>>>>>>>> loss: 0.010738818906247616\n",
      "Epoch 113: >>>>>>>>>>>>> loss: 0.0024544713087379932\n",
      "Epoch 114: >>>>>>>>>>>>> loss: 0.0008008223958313465\n",
      "Epoch 115: >>>>>>>>>>>>> loss: 0.008152400143444538\n",
      "Epoch 116: >>>>>>>>>>>>> loss: 0.027317794039845467\n",
      "Epoch 117: >>>>>>>>>>>>> loss: 0.05962072312831879\n",
      "Epoch 118: >>>>>>>>>>>>> loss: 0.007010314147919416\n",
      "Epoch 119: >>>>>>>>>>>>> loss: 0.013958410359919071\n",
      "Epoch 120: >>>>>>>>>>>>> loss: 0.02104630134999752\n",
      "Epoch 121: >>>>>>>>>>>>> loss: 0.019534748047590256\n",
      "Epoch 122: >>>>>>>>>>>>> loss: 0.015177100896835327\n",
      "Epoch 123: >>>>>>>>>>>>> loss: 0.0003807660541497171\n",
      "Epoch 124: >>>>>>>>>>>>> loss: 0.0026763726491481066\n",
      "Epoch 125: >>>>>>>>>>>>> loss: 0.0005986391333863139\n",
      "Epoch 126: >>>>>>>>>>>>> loss: 0.018121981993317604\n",
      "Epoch 127: >>>>>>>>>>>>> loss: 0.004789680242538452\n",
      "Epoch 128: >>>>>>>>>>>>> loss: 0.012359105981886387\n",
      "Epoch 129: >>>>>>>>>>>>> loss: 0.0006167106912471354\n",
      "Epoch 130: >>>>>>>>>>>>> loss: 0.003517306875437498\n",
      "Epoch 131: >>>>>>>>>>>>> loss: 0.008857729844748974\n",
      "Epoch 132: >>>>>>>>>>>>> loss: 0.006172418128699064\n",
      "Epoch 133: >>>>>>>>>>>>> loss: 0.001483433647081256\n",
      "Epoch 134: >>>>>>>>>>>>> loss: 0.003902415744960308\n",
      "Epoch 135: >>>>>>>>>>>>> loss: 0.002681829733774066\n",
      "Epoch 136: >>>>>>>>>>>>> loss: 0.01615714281797409\n",
      "Epoch 137: >>>>>>>>>>>>> loss: 0.001273241825401783\n",
      "Epoch 138: >>>>>>>>>>>>> loss: 0.009294714778661728\n",
      "Epoch 139: >>>>>>>>>>>>> loss: 0.018970537930727005\n",
      "Epoch 140: >>>>>>>>>>>>> loss: 0.0020274831913411617\n",
      "Epoch 141: >>>>>>>>>>>>> loss: 0.016109352931380272\n",
      "Epoch 142: >>>>>>>>>>>>> loss: 0.0023693779949098825\n",
      "Epoch 143: >>>>>>>>>>>>> loss: 0.0015446379547938704\n",
      "Epoch 144: >>>>>>>>>>>>> loss: 0.0001697828556643799\n",
      "Epoch 145: >>>>>>>>>>>>> loss: 0.00909963808953762\n",
      "Epoch 146: >>>>>>>>>>>>> loss: 0.005585454870015383\n",
      "Epoch 147: >>>>>>>>>>>>> loss: 0.014758443459868431\n",
      "Epoch 148: >>>>>>>>>>>>> loss: 0.023693861439824104\n",
      "Epoch 149: >>>>>>>>>>>>> loss: 0.014399726875126362\n",
      "Epoch 150: >>>>>>>>>>>>> loss: 0.0012062742607668042\n",
      "Epoch 151: >>>>>>>>>>>>> loss: 0.009033813141286373\n",
      "Epoch 152: >>>>>>>>>>>>> loss: 0.008652307093143463\n",
      "Epoch 153: >>>>>>>>>>>>> loss: 0.015770502388477325\n",
      "Epoch 154: >>>>>>>>>>>>> loss: 0.023250091820955276\n",
      "Epoch 155: >>>>>>>>>>>>> loss: 0.009885945357382298\n",
      "Epoch 156: >>>>>>>>>>>>> loss: 0.0044543067924678326\n",
      "Epoch 157: >>>>>>>>>>>>> loss: 0.00473535992205143\n",
      "Epoch 158: >>>>>>>>>>>>> loss: 0.0035643826704472303\n",
      "Epoch 159: >>>>>>>>>>>>> loss: 0.00184678565710783\n",
      "Epoch 160: >>>>>>>>>>>>> loss: 0.004970531444996595\n",
      "Epoch 161: >>>>>>>>>>>>> loss: 0.0010172417387366295\n",
      "Epoch 162: >>>>>>>>>>>>> loss: 0.004937758669257164\n",
      "Epoch 163: >>>>>>>>>>>>> loss: 6.83581383782439e-05\n",
      "Epoch 164: >>>>>>>>>>>>> loss: 0.00574555853381753\n",
      "Epoch 165: >>>>>>>>>>>>> loss: 0.00022514499141834676\n",
      "Epoch 166: >>>>>>>>>>>>> loss: 0.012189027853310108\n",
      "Epoch 167: >>>>>>>>>>>>> loss: 0.0019798693247139454\n",
      "Epoch 168: >>>>>>>>>>>>> loss: 0.008058864623308182\n",
      "Epoch 169: >>>>>>>>>>>>> loss: 0.0018803635612130165\n",
      "Epoch 170: >>>>>>>>>>>>> loss: 0.11047733575105667\n",
      "Epoch 171: >>>>>>>>>>>>> loss: 0.004194308537989855\n",
      "Epoch 172: >>>>>>>>>>>>> loss: 0.014245234429836273\n",
      "Epoch 173: >>>>>>>>>>>>> loss: 0.08258146047592163\n",
      "Epoch 174: >>>>>>>>>>>>> loss: 0.07236132770776749\n",
      "Epoch 175: >>>>>>>>>>>>> loss: 0.024148259311914444\n",
      "Epoch 176: >>>>>>>>>>>>> loss: 0.037032097578048706\n",
      "Epoch 177: >>>>>>>>>>>>> loss: 0.003629540791735053\n",
      "Epoch 178: >>>>>>>>>>>>> loss: 0.0029041962698101997\n",
      "Epoch 179: >>>>>>>>>>>>> loss: 0.020295418798923492\n",
      "Epoch 180: >>>>>>>>>>>>> loss: 0.04595238342881203\n",
      "Epoch 181: >>>>>>>>>>>>> loss: 0.006088875234127045\n",
      "Epoch 182: >>>>>>>>>>>>> loss: 0.004540665075182915\n",
      "Epoch 183: >>>>>>>>>>>>> loss: 0.0020957596134394407\n",
      "Epoch 184: >>>>>>>>>>>>> loss: 0.0014093463541939855\n",
      "Epoch 185: >>>>>>>>>>>>> loss: 0.0007920327479951084\n",
      "Epoch 186: >>>>>>>>>>>>> loss: 0.0010792503599077463\n",
      "Epoch 187: >>>>>>>>>>>>> loss: 0.001308600651100278\n",
      "Epoch 188: >>>>>>>>>>>>> loss: 0.007590952329337597\n",
      "Epoch 189: >>>>>>>>>>>>> loss: 0.0006988987443037331\n",
      "Epoch 190: >>>>>>>>>>>>> loss: 0.0013257345417514443\n",
      "Epoch 191: >>>>>>>>>>>>> loss: 0.0008144252933561802\n",
      "Epoch 192: >>>>>>>>>>>>> loss: 0.0011015229392796755\n",
      "Epoch 193: >>>>>>>>>>>>> loss: 0.001960637280717492\n",
      "Epoch 194: >>>>>>>>>>>>> loss: 0.012268614955246449\n",
      "Epoch 195: >>>>>>>>>>>>> loss: 0.0042864796705543995\n",
      "Epoch 196: >>>>>>>>>>>>> loss: 0.000244725844822824\n",
      "Epoch 197: >>>>>>>>>>>>> loss: 0.015214793384075165\n",
      "Epoch 198: >>>>>>>>>>>>> loss: 0.00852859765291214\n",
      "Epoch 199: >>>>>>>>>>>>> loss: 0.00207703048363328\n",
      "Epoch 200: >>>>>>>>>>>>> loss: 0.016332266852259636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.32      0.41        25\n",
      "           1       1.00      0.72      0.84        25\n",
      "           2       0.56      0.40      0.47        25\n",
      "           3       0.39      0.44      0.42        25\n",
      "           4       0.57      0.32      0.41        25\n",
      "           5       0.47      0.56      0.51        25\n",
      "           6       0.45      1.00      0.62        25\n",
      "           7       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.47       200\n",
      "   macro avg       0.50      0.47      0.46       200\n",
      "weighted avg       0.50      0.47      0.46       200\n",
      "\n",
      "Evaluation:\n",
      "[[ 8  0  6  0  0  6  0  5]\n",
      " [ 1 18  1  1  0  0  0  4]\n",
      " [ 4  0 10 11  0  0  0  0]\n",
      " [ 1  0  0 11  0  6  0  7]\n",
      " [ 0  0  1  2  8  2  7  5]\n",
      " [ 0  0  0  3  6 14  0  2]\n",
      " [ 0  0  0  0  0  0 25  0]\n",
      " [ 0  0  0  0  0  2 23  0]]\n",
      "save checkpoint in : ./models/saved_models/AEPretrain_baseline_checkpoint_200__2021_01_13_18_12\n",
      "Epoch 201: >>>>>>>>>>>>> loss: 0.012021161615848541\n",
      "Epoch 202: >>>>>>>>>>>>> loss: 0.0015146125806495547\n",
      "Epoch 203: >>>>>>>>>>>>> loss: 0.03457453101873398\n",
      "Epoch 204: >>>>>>>>>>>>> loss: 0.00041181271080859005\n",
      "Epoch 205: >>>>>>>>>>>>> loss: 0.02640848234295845\n",
      "Epoch 206: >>>>>>>>>>>>> loss: 0.00873392540961504\n",
      "Epoch 207: >>>>>>>>>>>>> loss: 0.007639418821781874\n",
      "Epoch 208: >>>>>>>>>>>>> loss: 0.0005860201199539006\n",
      "Epoch 209: >>>>>>>>>>>>> loss: 0.0004501065122894943\n",
      "Epoch 210: >>>>>>>>>>>>> loss: 0.045663680881261826\n",
      "Epoch 211: >>>>>>>>>>>>> loss: 0.005604095291346312\n",
      "Epoch 212: >>>>>>>>>>>>> loss: 0.0016512245638296008\n",
      "Epoch 213: >>>>>>>>>>>>> loss: 0.0013906704261898994\n",
      "Epoch 214: >>>>>>>>>>>>> loss: 0.03694172576069832\n",
      "Epoch 215: >>>>>>>>>>>>> loss: 0.0033948556520044804\n",
      "Epoch 216: >>>>>>>>>>>>> loss: 0.019935287535190582\n",
      "Epoch 217: >>>>>>>>>>>>> loss: 0.004854673985391855\n",
      "Epoch 218: >>>>>>>>>>>>> loss: 0.0010995701886713505\n",
      "Epoch 219: >>>>>>>>>>>>> loss: 0.02673487924039364\n",
      "Epoch 220: >>>>>>>>>>>>> loss: 0.0001576649519847706\n",
      "Epoch 221: >>>>>>>>>>>>> loss: 0.004603056237101555\n",
      "Epoch 222: >>>>>>>>>>>>> loss: 0.0010046456009149551\n",
      "Epoch 223: >>>>>>>>>>>>> loss: 0.0004287018673494458\n",
      "Epoch 224: >>>>>>>>>>>>> loss: 0.0036132007371634245\n",
      "Epoch 225: >>>>>>>>>>>>> loss: 0.027645299211144447\n",
      "Epoch 226: >>>>>>>>>>>>> loss: 0.0006488817743957043\n",
      "Epoch 227: >>>>>>>>>>>>> loss: 0.013123304583132267\n",
      "Epoch 228: >>>>>>>>>>>>> loss: 0.06055844575166702\n",
      "Epoch 229: >>>>>>>>>>>>> loss: 0.018714215606451035\n",
      "Epoch 230: >>>>>>>>>>>>> loss: 0.09002973139286041\n",
      "Epoch 231: >>>>>>>>>>>>> loss: 0.0040540313348174095\n",
      "Epoch 232: >>>>>>>>>>>>> loss: 0.008848587051033974\n",
      "Epoch 233: >>>>>>>>>>>>> loss: 0.0185514148324728\n",
      "Epoch 234: >>>>>>>>>>>>> loss: 0.0057649146765470505\n",
      "Epoch 235: >>>>>>>>>>>>> loss: 0.017709314823150635\n",
      "Epoch 236: >>>>>>>>>>>>> loss: 0.014730464667081833\n",
      "Epoch 237: >>>>>>>>>>>>> loss: 0.004624661989510059\n",
      "Epoch 238: >>>>>>>>>>>>> loss: 0.03527500852942467\n",
      "Epoch 239: >>>>>>>>>>>>> loss: 0.02818325161933899\n",
      "Epoch 240: >>>>>>>>>>>>> loss: 0.0008719564066268504\n",
      "Epoch 241: >>>>>>>>>>>>> loss: 0.0016461082268506289\n",
      "Epoch 242: >>>>>>>>>>>>> loss: 0.00966900959610939\n",
      "Epoch 243: >>>>>>>>>>>>> loss: 0.0009204773814417422\n",
      "Epoch 244: >>>>>>>>>>>>> loss: 4.717694901046343e-05\n",
      "Epoch 245: >>>>>>>>>>>>> loss: 0.04066173732280731\n",
      "Epoch 246: >>>>>>>>>>>>> loss: 0.00040483870543539524\n",
      "Epoch 247: >>>>>>>>>>>>> loss: 0.001705266535282135\n",
      "Epoch 248: >>>>>>>>>>>>> loss: 0.02682626247406006\n",
      "Epoch 249: >>>>>>>>>>>>> loss: 0.0020532971248030663\n",
      "Epoch 250: >>>>>>>>>>>>> loss: 0.007120953407138586\n",
      "Epoch 251: >>>>>>>>>>>>> loss: 0.06287630647420883\n",
      "Epoch 252: >>>>>>>>>>>>> loss: 0.04519885405898094\n",
      "Epoch 253: >>>>>>>>>>>>> loss: 0.07522797584533691\n",
      "Epoch 254: >>>>>>>>>>>>> loss: 0.037170831114053726\n",
      "Epoch 255: >>>>>>>>>>>>> loss: 0.014304189942777157\n",
      "Epoch 256: >>>>>>>>>>>>> loss: 0.00038712448440492153\n",
      "Epoch 257: >>>>>>>>>>>>> loss: 0.0047411625273525715\n",
      "Epoch 258: >>>>>>>>>>>>> loss: 0.02132963202893734\n",
      "Epoch 259: >>>>>>>>>>>>> loss: 0.002032109536230564\n",
      "Epoch 260: >>>>>>>>>>>>> loss: 0.017184261232614517\n",
      "Epoch 261: >>>>>>>>>>>>> loss: 0.03284443914890289\n",
      "Epoch 262: >>>>>>>>>>>>> loss: 0.18323686718940735\n",
      "Epoch 263: >>>>>>>>>>>>> loss: 0.02821074239909649\n",
      "Epoch 264: >>>>>>>>>>>>> loss: 0.016346482560038567\n",
      "Epoch 265: >>>>>>>>>>>>> loss: 0.00035307317739352584\n",
      "Epoch 266: >>>>>>>>>>>>> loss: 0.003397376975044608\n",
      "Epoch 267: >>>>>>>>>>>>> loss: 0.00026343154604546726\n",
      "Epoch 268: >>>>>>>>>>>>> loss: 0.010591116733849049\n",
      "Epoch 269: >>>>>>>>>>>>> loss: 0.00776835810393095\n",
      "Epoch 270: >>>>>>>>>>>>> loss: 0.0016754306852817535\n",
      "Epoch 271: >>>>>>>>>>>>> loss: 0.012883514165878296\n",
      "Epoch 272: >>>>>>>>>>>>> loss: 0.01571222022175789\n",
      "Epoch 273: >>>>>>>>>>>>> loss: 0.0004695447569247335\n",
      "Epoch 274: >>>>>>>>>>>>> loss: 0.007734048645943403\n",
      "Epoch 275: >>>>>>>>>>>>> loss: 0.007333657238632441\n",
      "Epoch 276: >>>>>>>>>>>>> loss: 0.002825390547513962\n",
      "Epoch 277: >>>>>>>>>>>>> loss: 0.0008057838422246277\n",
      "Epoch 278: >>>>>>>>>>>>> loss: 0.0009253611206077039\n",
      "Epoch 279: >>>>>>>>>>>>> loss: 0.007954179309308529\n",
      "Epoch 280: >>>>>>>>>>>>> loss: 0.00018302560783922672\n",
      "Epoch 281: >>>>>>>>>>>>> loss: 0.00011447371798567474\n",
      "Epoch 282: >>>>>>>>>>>>> loss: 0.0012776486109942198\n",
      "Epoch 283: >>>>>>>>>>>>> loss: 0.004763753153383732\n",
      "Epoch 284: >>>>>>>>>>>>> loss: 0.006033983081579208\n",
      "Epoch 285: >>>>>>>>>>>>> loss: 0.001315362169407308\n",
      "Epoch 286: >>>>>>>>>>>>> loss: 0.000142211327329278\n",
      "Epoch 287: >>>>>>>>>>>>> loss: 0.0012600950431078672\n",
      "Epoch 288: >>>>>>>>>>>>> loss: 0.0024903719313442707\n",
      "Epoch 289: >>>>>>>>>>>>> loss: 0.0002655952994246036\n",
      "Epoch 290: >>>>>>>>>>>>> loss: 0.05277986451983452\n",
      "Epoch 291: >>>>>>>>>>>>> loss: 0.07183856517076492\n",
      "Epoch 292: >>>>>>>>>>>>> loss: 0.00012115522258682176\n",
      "Epoch 293: >>>>>>>>>>>>> loss: 0.0016864771023392677\n",
      "Epoch 294: >>>>>>>>>>>>> loss: 0.000501401664223522\n",
      "Epoch 295: >>>>>>>>>>>>> loss: 0.00056183070410043\n",
      "Epoch 296: >>>>>>>>>>>>> loss: 0.02312421426177025\n",
      "Epoch 297: >>>>>>>>>>>>> loss: 0.0001223296858370304\n",
      "Epoch 298: >>>>>>>>>>>>> loss: 0.0027489953208714724\n",
      "Epoch 299: >>>>>>>>>>>>> loss: 0.004909556359052658\n",
      "Epoch 300: >>>>>>>>>>>>> loss: 1.98977595573524e-05\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.48      0.59        25\n",
      "           1       1.00      0.72      0.84        25\n",
      "           2       0.53      0.40      0.45        25\n",
      "           3       0.39      0.48      0.43        25\n",
      "           4       0.82      0.56      0.67        25\n",
      "           5       0.52      0.64      0.57        25\n",
      "           6       0.52      0.96      0.68        25\n",
      "           7       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.53       200\n",
      "   macro avg       0.57      0.53      0.53       200\n",
      "weighted avg       0.57      0.53      0.53       200\n",
      "\n",
      "Evaluation:\n",
      "[[12  0  5  0  0  7  0  1]\n",
      " [ 0 18  0  2  0  4  0  1]\n",
      " [ 2  0 10 12  0  0  0  1]\n",
      " [ 1  0  0 12  0  3  0  9]\n",
      " [ 0  0  3  1 14  0  0  7]\n",
      " [ 0  0  0  4  2 16  0  3]\n",
      " [ 1  0  0  0  0  0 24  0]\n",
      " [ 0  0  1  0  1  1 22  0]]\n",
      "save checkpoint in : ./models/saved_models/AEPretrain_baseline_checkpoint_300__2021_01_13_18_46\n",
      "Epoch 301: >>>>>>>>>>>>> loss: 0.007019884418696165\n",
      "Epoch 302: >>>>>>>>>>>>> loss: 0.002243560738861561\n",
      "Epoch 303: >>>>>>>>>>>>> loss: 0.00026967376470565796\n",
      "Epoch 304: >>>>>>>>>>>>> loss: 0.010900142602622509\n",
      "Epoch 305: >>>>>>>>>>>>> loss: 0.010985960252583027\n",
      "Epoch 306: >>>>>>>>>>>>> loss: 3.917865342373261e-06\n",
      "Epoch 307: >>>>>>>>>>>>> loss: 2.2732976503903046e-05\n",
      "Epoch 308: >>>>>>>>>>>>> loss: 0.0004867599345743656\n",
      "Epoch 309: >>>>>>>>>>>>> loss: 1.7719739844324067e-05\n",
      "Epoch 310: >>>>>>>>>>>>> loss: 0.03232194483280182\n",
      "Epoch 311: >>>>>>>>>>>>> loss: 0.011235603131353855\n",
      "Epoch 312: >>>>>>>>>>>>> loss: 0.02159738913178444\n",
      "Epoch 313: >>>>>>>>>>>>> loss: 0.0002711876586545259\n",
      "Epoch 314: >>>>>>>>>>>>> loss: 0.14737196266651154\n",
      "Epoch 315: >>>>>>>>>>>>> loss: 0.04292670264840126\n",
      "Epoch 316: >>>>>>>>>>>>> loss: 0.03134914115071297\n",
      "Epoch 317: >>>>>>>>>>>>> loss: 0.015805277973413467\n",
      "Epoch 318: >>>>>>>>>>>>> loss: 0.01766374707221985\n",
      "Epoch 319: >>>>>>>>>>>>> loss: 0.00015580873878207058\n",
      "Epoch 320: >>>>>>>>>>>>> loss: 0.009686902165412903\n",
      "Epoch 321: >>>>>>>>>>>>> loss: 0.036550942808389664\n",
      "Epoch 322: >>>>>>>>>>>>> loss: 0.002606372581794858\n",
      "Epoch 323: >>>>>>>>>>>>> loss: 0.003552016569301486\n",
      "Epoch 324: >>>>>>>>>>>>> loss: 9.935606067301705e-05\n",
      "Epoch 325: >>>>>>>>>>>>> loss: 0.03595931828022003\n",
      "Epoch 326: >>>>>>>>>>>>> loss: 0.060638248920440674\n",
      "Epoch 327: >>>>>>>>>>>>> loss: 0.0018033167580142617\n",
      "Epoch 328: >>>>>>>>>>>>> loss: 0.013282605446875095\n",
      "Epoch 329: >>>>>>>>>>>>> loss: 0.04082098975777626\n",
      "Epoch 330: >>>>>>>>>>>>> loss: 0.04489700496196747\n",
      "Epoch 331: >>>>>>>>>>>>> loss: 0.054108839482069016\n",
      "Epoch 332: >>>>>>>>>>>>> loss: 0.01198439858853817\n",
      "Epoch 333: >>>>>>>>>>>>> loss: 0.02245037630200386\n",
      "Epoch 334: >>>>>>>>>>>>> loss: 0.0014343223301693797\n",
      "Epoch 335: >>>>>>>>>>>>> loss: 0.01761796325445175\n",
      "Epoch 336: >>>>>>>>>>>>> loss: 0.00011627723142737523\n",
      "Epoch 337: >>>>>>>>>>>>> loss: 6.159977056086063e-05\n",
      "Epoch 338: >>>>>>>>>>>>> loss: 0.0015291456365957856\n",
      "Epoch 339: >>>>>>>>>>>>> loss: 0.003020624164491892\n",
      "Epoch 340: >>>>>>>>>>>>> loss: 0.000283245142782107\n",
      "Epoch 341: >>>>>>>>>>>>> loss: 0.000763231422752142\n",
      "Epoch 342: >>>>>>>>>>>>> loss: 0.0010558405192568898\n",
      "Epoch 343: >>>>>>>>>>>>> loss: 0.00011226847709622234\n",
      "Epoch 344: >>>>>>>>>>>>> loss: 0.0020096341613680124\n",
      "Epoch 345: >>>>>>>>>>>>> loss: 3.9520065911347046e-05\n",
      "Epoch 346: >>>>>>>>>>>>> loss: 4.118422657484189e-05\n",
      "Epoch 347: >>>>>>>>>>>>> loss: 0.005348125472664833\n",
      "Epoch 348: >>>>>>>>>>>>> loss: 0.02069704234600067\n",
      "Epoch 349: >>>>>>>>>>>>> loss: 0.000857895240187645\n",
      "Epoch 350: >>>>>>>>>>>>> loss: 3.7888781662331894e-05\n",
      "Epoch 351: >>>>>>>>>>>>> loss: 0.0070448508486151695\n",
      "Epoch 352: >>>>>>>>>>>>> loss: 4.3097908928757533e-05\n",
      "Epoch 353: >>>>>>>>>>>>> loss: 4.3003852624678984e-05\n",
      "Epoch 354: >>>>>>>>>>>>> loss: 0.00034962885547429323\n",
      "Epoch 355: >>>>>>>>>>>>> loss: 0.00022548731067217886\n",
      "Epoch 356: >>>>>>>>>>>>> loss: 1.4593741070711985e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 357: >>>>>>>>>>>>> loss: 0.00026589646586216986\n",
      "Epoch 358: >>>>>>>>>>>>> loss: 8.294134204334114e-06\n",
      "Epoch 359: >>>>>>>>>>>>> loss: 1.5823177818674594e-05\n",
      "Epoch 360: >>>>>>>>>>>>> loss: 0.0011745686642825603\n",
      "Epoch 361: >>>>>>>>>>>>> loss: 0.011049394495785236\n",
      "Epoch 362: >>>>>>>>>>>>> loss: 4.748464561998844e-05\n",
      "Epoch 363: >>>>>>>>>>>>> loss: 0.0007144450792111456\n",
      "Epoch 364: >>>>>>>>>>>>> loss: 0.000516003929078579\n",
      "Epoch 365: >>>>>>>>>>>>> loss: 0.0056405263021588326\n",
      "Epoch 366: >>>>>>>>>>>>> loss: 3.605617530411109e-05\n",
      "Epoch 367: >>>>>>>>>>>>> loss: 0.00028921058401465416\n",
      "Epoch 368: >>>>>>>>>>>>> loss: 0.00016224849969148636\n",
      "Epoch 369: >>>>>>>>>>>>> loss: 0.0004348265356384218\n",
      "Epoch 370: >>>>>>>>>>>>> loss: 0.0026760075706988573\n",
      "Epoch 371: >>>>>>>>>>>>> loss: 0.00015237713523674756\n",
      "Epoch 372: >>>>>>>>>>>>> loss: 0.00034413617686368525\n",
      "Epoch 373: >>>>>>>>>>>>> loss: 3.2275223929900676e-05\n",
      "Epoch 374: >>>>>>>>>>>>> loss: 7.840039324946702e-05\n",
      "Epoch 375: >>>>>>>>>>>>> loss: 0.00011984648153884336\n",
      "Epoch 376: >>>>>>>>>>>>> loss: 0.0005971660721115768\n",
      "Epoch 377: >>>>>>>>>>>>> loss: 0.00010193787602474913\n",
      "Epoch 378: >>>>>>>>>>>>> loss: 8.061741391429678e-05\n",
      "Epoch 379: >>>>>>>>>>>>> loss: 3.4158811104134656e-06\n",
      "Epoch 380: >>>>>>>>>>>>> loss: 0.0003630349528975785\n",
      "Epoch 381: >>>>>>>>>>>>> loss: 6.350955663947389e-05\n",
      "Epoch 382: >>>>>>>>>>>>> loss: 0.002638392150402069\n",
      "Epoch 383: >>>>>>>>>>>>> loss: 0.0005931761115789413\n",
      "Epoch 384: >>>>>>>>>>>>> loss: 0.0008946715970523655\n",
      "Epoch 385: >>>>>>>>>>>>> loss: 0.021248050034046173\n",
      "Epoch 386: >>>>>>>>>>>>> loss: 6.0126181779196486e-05\n",
      "Epoch 387: >>>>>>>>>>>>> loss: 0.000999818672426045\n",
      "Epoch 388: >>>>>>>>>>>>> loss: 4.2321309592807665e-05\n",
      "Epoch 389: >>>>>>>>>>>>> loss: 0.0014667268842458725\n",
      "Epoch 390: >>>>>>>>>>>>> loss: 0.006447039544582367\n",
      "Epoch 391: >>>>>>>>>>>>> loss: 3.591403583413921e-05\n",
      "Epoch 392: >>>>>>>>>>>>> loss: 0.00010263318108627573\n",
      "Epoch 393: >>>>>>>>>>>>> loss: 0.0007090154686011374\n",
      "Epoch 394: >>>>>>>>>>>>> loss: 0.2566620409488678\n",
      "Epoch 395: >>>>>>>>>>>>> loss: 0.0010577894281595945\n",
      "Epoch 396: >>>>>>>>>>>>> loss: 0.00013425288489088416\n",
      "Epoch 397: >>>>>>>>>>>>> loss: 0.07487954199314117\n",
      "Epoch 398: >>>>>>>>>>>>> loss: 0.03412710875272751\n",
      "Epoch 399: >>>>>>>>>>>>> loss: 0.016760537400841713\n",
      "Epoch 400: >>>>>>>>>>>>> loss: 0.032110050320625305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.68      0.72        25\n",
      "           1       0.80      0.80      0.80        25\n",
      "           2       1.00      0.32      0.48        25\n",
      "           3       0.29      0.36      0.32        25\n",
      "           4       0.82      0.56      0.67        25\n",
      "           5       0.48      0.60      0.54        25\n",
      "           6       0.52      0.96      0.68        25\n",
      "           7       0.10      0.08      0.09        25\n",
      "\n",
      "    accuracy                           0.55       200\n",
      "   macro avg       0.60      0.55      0.54       200\n",
      "weighted avg       0.60      0.55      0.54       200\n",
      "\n",
      "Evaluation:\n",
      "[[17  2  0  0  0  6  0  0]\n",
      " [ 0 20  0  2  0  0  0  3]\n",
      " [ 1  2  8 13  0  0  0  1]\n",
      " [ 4  0  0  9  0  8  0  4]\n",
      " [ 0  0  0  3 14  2  0  6]\n",
      " [ 0  0  0  3  3 15  0  4]\n",
      " [ 0  1  0  0  0  0 24  0]\n",
      " [ 0  0  0  1  0  0 22  2]]\n",
      "save checkpoint in : ./models/saved_models/AEPretrain_baseline_checkpoint_400__2021_01_13_19_22\n",
      "Epoch 401: >>>>>>>>>>>>> loss: 0.001782644190825522\n",
      "Epoch 402: >>>>>>>>>>>>> loss: 3.630907303886488e-05\n",
      "Epoch 403: >>>>>>>>>>>>> loss: 0.02625795640051365\n",
      "Epoch 404: >>>>>>>>>>>>> loss: 0.0011101897107437253\n",
      "Epoch 405: >>>>>>>>>>>>> loss: 0.020945250988006592\n",
      "Epoch 406: >>>>>>>>>>>>> loss: 0.0007603009580634534\n",
      "Epoch 407: >>>>>>>>>>>>> loss: 0.008424072526395321\n",
      "Epoch 408: >>>>>>>>>>>>> loss: 0.001265501487068832\n",
      "Epoch 409: >>>>>>>>>>>>> loss: 0.0006860783905722201\n",
      "Epoch 410: >>>>>>>>>>>>> loss: 8.835829794406891e-05\n",
      "Epoch 411: >>>>>>>>>>>>> loss: 0.0005351805593818426\n",
      "Epoch 412: >>>>>>>>>>>>> loss: 0.0007889242842793465\n",
      "Epoch 413: >>>>>>>>>>>>> loss: 0.00016873845015652478\n",
      "Epoch 414: >>>>>>>>>>>>> loss: 0.0010934052988886833\n",
      "Epoch 415: >>>>>>>>>>>>> loss: 6.486145139206201e-06\n",
      "Epoch 416: >>>>>>>>>>>>> loss: 0.083841472864151\n",
      "Epoch 417: >>>>>>>>>>>>> loss: 0.0011303425999358296\n",
      "Epoch 418: >>>>>>>>>>>>> loss: 0.045435830950737\n",
      "Epoch 419: >>>>>>>>>>>>> loss: 0.0011097867973148823\n",
      "Epoch 420: >>>>>>>>>>>>> loss: 0.00048752533621154726\n",
      "Epoch 421: >>>>>>>>>>>>> loss: 0.017093295231461525\n",
      "Epoch 422: >>>>>>>>>>>>> loss: 0.00031643020338378847\n",
      "Epoch 423: >>>>>>>>>>>>> loss: 0.0003285481652710587\n",
      "Epoch 424: >>>>>>>>>>>>> loss: 0.018704460933804512\n",
      "Epoch 425: >>>>>>>>>>>>> loss: 0.002839301247149706\n",
      "Epoch 426: >>>>>>>>>>>>> loss: 0.00043445112532936037\n",
      "Epoch 427: >>>>>>>>>>>>> loss: 0.0123874731361866\n",
      "Epoch 428: >>>>>>>>>>>>> loss: 0.05208238214254379\n",
      "Epoch 429: >>>>>>>>>>>>> loss: 0.004841431975364685\n",
      "Epoch 430: >>>>>>>>>>>>> loss: 0.001945304567925632\n",
      "Epoch 431: >>>>>>>>>>>>> loss: 0.0036449399776756763\n",
      "Epoch 432: >>>>>>>>>>>>> loss: 0.023768311366438866\n",
      "Epoch 433: >>>>>>>>>>>>> loss: 0.012800476513803005\n",
      "Epoch 434: >>>>>>>>>>>>> loss: 0.0003060619637835771\n",
      "Epoch 435: >>>>>>>>>>>>> loss: 2.3583299480378628e-05\n",
      "Epoch 436: >>>>>>>>>>>>> loss: 3.5099460546916816e-06\n",
      "Epoch 437: >>>>>>>>>>>>> loss: 4.3326159357093275e-05\n",
      "Epoch 438: >>>>>>>>>>>>> loss: 1.7505755749880336e-05\n",
      "Epoch 439: >>>>>>>>>>>>> loss: 0.00013842298358213156\n",
      "Epoch 440: >>>>>>>>>>>>> loss: 1.4249199864480033e-07\n",
      "Epoch 441: >>>>>>>>>>>>> loss: 9.387384807268973e-07\n",
      "Epoch 442: >>>>>>>>>>>>> loss: 0.03108486533164978\n",
      "Epoch 443: >>>>>>>>>>>>> loss: 0.018317021429538727\n",
      "Epoch 444: >>>>>>>>>>>>> loss: 0.0007435218431055546\n",
      "Epoch 445: >>>>>>>>>>>>> loss: 0.019971435889601707\n",
      "Epoch 446: >>>>>>>>>>>>> loss: 8.556196007702965e-06\n",
      "Epoch 447: >>>>>>>>>>>>> loss: 0.0015375143848359585\n",
      "Epoch 448: >>>>>>>>>>>>> loss: 9.662559750722721e-05\n",
      "Epoch 449: >>>>>>>>>>>>> loss: 8.277445886051282e-06\n",
      "Epoch 450: >>>>>>>>>>>>> loss: 0.00022106464894022793\n",
      "Epoch 451: >>>>>>>>>>>>> loss: 1.0629476491885725e-05\n",
      "Epoch 452: >>>>>>>>>>>>> loss: 9.873538147076033e-06\n",
      "Epoch 453: >>>>>>>>>>>>> loss: 1.545908162370324e-05\n",
      "Epoch 454: >>>>>>>>>>>>> loss: 0.0007890013512223959\n",
      "Epoch 455: >>>>>>>>>>>>> loss: 5.027576207794482e-06\n",
      "Epoch 456: >>>>>>>>>>>>> loss: 3.291709435870871e-05\n",
      "Epoch 457: >>>>>>>>>>>>> loss: 4.872480531048495e-06\n",
      "Epoch 458: >>>>>>>>>>>>> loss: 0.0002977565454784781\n",
      "Epoch 459: >>>>>>>>>>>>> loss: 5.782099105999805e-05\n",
      "Epoch 460: >>>>>>>>>>>>> loss: 0.00024756716447882354\n",
      "Epoch 461: >>>>>>>>>>>>> loss: 0.0009422543807886541\n",
      "Epoch 462: >>>>>>>>>>>>> loss: 7.88943907537032e-06\n",
      "Epoch 463: >>>>>>>>>>>>> loss: 8.483022611471824e-06\n",
      "Epoch 464: >>>>>>>>>>>>> loss: 0.00012417940888553858\n",
      "Epoch 465: >>>>>>>>>>>>> loss: 0.00019080567290075123\n",
      "Epoch 466: >>>>>>>>>>>>> loss: 0.0005455297068692744\n",
      "Epoch 467: >>>>>>>>>>>>> loss: 0.00020745770598296076\n",
      "Epoch 468: >>>>>>>>>>>>> loss: 6.083758489694446e-05\n",
      "Epoch 469: >>>>>>>>>>>>> loss: 2.28890548896743e-05\n",
      "Epoch 470: >>>>>>>>>>>>> loss: 6.398039431587677e-07\n",
      "Epoch 471: >>>>>>>>>>>>> loss: 9.387521231474238e-07\n",
      "Epoch 472: >>>>>>>>>>>>> loss: 2.7348280582373263e-06\n",
      "Epoch 473: >>>>>>>>>>>>> loss: 1.2861321465607034e-06\n",
      "Epoch 474: >>>>>>>>>>>>> loss: 2.605212284834124e-05\n",
      "Epoch 475: >>>>>>>>>>>>> loss: 4.516880949267943e-07\n",
      "Epoch 476: >>>>>>>>>>>>> loss: 1.3401420346781379e-06\n",
      "Epoch 477: >>>>>>>>>>>>> loss: 2.4840215701260604e-05\n",
      "Epoch 478: >>>>>>>>>>>>> loss: 1.393112415826181e-05\n",
      "Epoch 479: >>>>>>>>>>>>> loss: 0.0003551235713530332\n",
      "Epoch 480: >>>>>>>>>>>>> loss: 0.0012352783232927322\n",
      "Epoch 481: >>>>>>>>>>>>> loss: 7.645586265425663e-06\n",
      "Epoch 482: >>>>>>>>>>>>> loss: 2.0768402464454994e-07\n",
      "Epoch 483: >>>>>>>>>>>>> loss: 0.00018887942133005708\n",
      "Epoch 484: >>>>>>>>>>>>> loss: 1.9761069779633544e-05\n",
      "Epoch 485: >>>>>>>>>>>>> loss: 7.684971205890179e-05\n",
      "Epoch 486: >>>>>>>>>>>>> loss: 9.471428938923054e-07\n",
      "Epoch 487: >>>>>>>>>>>>> loss: 3.0529394280165434e-05\n",
      "Epoch 488: >>>>>>>>>>>>> loss: 2.2767557311453857e-05\n",
      "Epoch 489: >>>>>>>>>>>>> loss: 4.2526906327111647e-05\n",
      "Epoch 490: >>>>>>>>>>>>> loss: 3.575752998585813e-05\n",
      "Epoch 491: >>>>>>>>>>>>> loss: 2.589211544545833e-05\n",
      "Epoch 492: >>>>>>>>>>>>> loss: 5.168780603526102e-07\n",
      "Epoch 493: >>>>>>>>>>>>> loss: 2.681617479538545e-05\n",
      "Epoch 494: >>>>>>>>>>>>> loss: 1.4788757880523917e-06\n",
      "Epoch 495: >>>>>>>>>>>>> loss: 0.0008865522686392069\n",
      "Epoch 496: >>>>>>>>>>>>> loss: 2.1799467049277155e-06\n",
      "Epoch 497: >>>>>>>>>>>>> loss: 8.754296914048609e-07\n",
      "Epoch 498: >>>>>>>>>>>>> loss: 0.00023893384786788374\n",
      "Epoch 499: >>>>>>>>>>>>> loss: 0.00018622091738507152\n",
      "Epoch 500: >>>>>>>>>>>>> loss: 9.293746188632213e-06\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.28      0.42        25\n",
      "           1       0.88      0.92      0.90        25\n",
      "           2       0.47      0.36      0.41        25\n",
      "           3       0.46      0.44      0.45        25\n",
      "           4       0.78      0.56      0.65        25\n",
      "           5       0.48      0.52      0.50        25\n",
      "           6       0.62      1.00      0.77        25\n",
      "           7       0.13      0.20      0.16        25\n",
      "\n",
      "    accuracy                           0.54       200\n",
      "   macro avg       0.59      0.54      0.53       200\n",
      "weighted avg       0.59      0.54      0.53       200\n",
      "\n",
      "Evaluation:\n",
      "[[ 7  0  6  0  0  6  0  6]\n",
      " [ 0 23  0  0  0  0  0  2]\n",
      " [ 1  2  9 12  0  0  0  1]\n",
      " [ 0  0  0 11  0  8  0  6]\n",
      " [ 0  0  0  1 14  0  0 10]\n",
      " [ 0  0  0  0  4 13  0  8]\n",
      " [ 0  0  0  0  0  0 25  0]\n",
      " [ 0  1  4  0  0  0 15  5]]\n",
      "save checkpoint in : ./models/saved_models/AEPretrain_baseline_checkpoint_500__2021_01_13_19_58\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "model  = train(model=cnn, train_loader=train_loader, criterion=criterion, \n",
    "                 optimizer=optimizer, end=epochs, start=100, evaluation=True, auto_save=True,\n",
    "                 name='AEPretrain_baseline',test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "  DataAugmentation-1  [[-1, 1, 1000, 90], [-1, 1, 1000, 90]]               0\n",
      "            Conv2d-2          [-1, 64, 200, 18]           1,664\n",
      "              ReLU-3          [-1, 64, 200, 18]               0\n",
      "            Lambda-4          [-1, 64, 200, 18]               0\n",
      "         MaxPool2d-5          [-1, 64, 100, 17]               0\n",
      "            Conv2d-6           [-1, 128, 33, 5]          73,856\n",
      "              ReLU-7           [-1, 128, 33, 5]               0\n",
      "            Lambda-8           [-1, 128, 33, 5]               0\n",
      "         MaxPool2d-9           [-1, 128, 16, 4]               0\n",
      "           Conv2d-10            [-1, 256, 8, 2]         131,328\n",
      "             ReLU-11            [-1, 256, 8, 2]               0\n",
      "           Lambda-12            [-1, 256, 8, 2]               0\n",
      "        MaxPool2d-13            [-1, 256, 4, 1]               0\n",
      "          Encoder-14                 [-1, 1024]               0\n",
      "           Conv2d-15          [-1, 64, 200, 18]           1,664\n",
      "             ReLU-16          [-1, 64, 200, 18]               0\n",
      "           Lambda-17          [-1, 64, 200, 18]               0\n",
      "        MaxPool2d-18          [-1, 64, 100, 17]               0\n",
      "           Conv2d-19           [-1, 128, 33, 5]          73,856\n",
      "             ReLU-20           [-1, 128, 33, 5]               0\n",
      "           Lambda-21           [-1, 128, 33, 5]               0\n",
      "        MaxPool2d-22           [-1, 128, 16, 4]               0\n",
      "           Conv2d-23            [-1, 256, 8, 2]         131,328\n",
      "             ReLU-24            [-1, 256, 8, 2]               0\n",
      "           Lambda-25            [-1, 256, 8, 2]               0\n",
      "        MaxPool2d-26            [-1, 256, 4, 1]               0\n",
      "          Encoder-27                 [-1, 1024]               0\n",
      "           Linear-28                  [-1, 128]         131,072\n",
      "             ReLU-29                  [-1, 128]               0\n",
      "           Linear-30                    [-1, 8]           1,024\n",
      "           Linear-31                  [-1, 128]         131,072\n",
      "             ReLU-32                  [-1, 128]               0\n",
      "           Linear-33                    [-1, 8]           1,024\n",
      "================================================================\n",
      "Total params: 677,888\n",
      "Trainable params: 677,888\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 3751.43\n",
      "Params size (MB): 2.59\n",
      "Estimated Total Size (MB): 3754.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from models.self_supervised import *\n",
    "from models.CNN import Encoder\n",
    "\n",
    "def build_simclr():\n",
    "    encoder = Encoder()\n",
    "    dataug = DataAugmentation(size=(1000,90))\n",
    "    n_features = [1024,128,8]\n",
    "    simclr = SimCLR(encoder=encoder, n_features=n_features, transformer=dataug)\n",
    "    return simclr,dataug\n",
    "\n",
    "def setting_(model):\n",
    "    criterion = NT_Xent(batch_size=128,temperature=0.1)\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=0.003)\n",
    "    return criterion, optimizer\n",
    "\n",
    "def train_unsupervised(model, train_loader, criterion, optimizer, epochs, auto_save = False):\n",
    "    print('Start Training')\n",
    "    for i in range(epochs):\n",
    "        print(f'epoch: {i+1},  batch:', end='')\n",
    "        for b, (X,_) in enumerate(train_loader):\n",
    "            print(\">\",end='')\n",
    "            optimizer.zero_grad()\n",
    "            z1,z2 = model(X)\n",
    "            loss = criterion(z1,z2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f' loss: {loss.tolist()}')\n",
    "        \n",
    "        if (i+1)%100 == 0 and auto_save == True:\n",
    "            \n",
    "            directory = make_directory('SimCLR_training', epoch=i+1, filepath='./models/saved_models/')\n",
    "            save_checkpoint(model,optimizer,i+1,directory)\n",
    "\n",
    "    return model\n",
    "\n",
    "simclr, dataug = build_simclr()\n",
    "sample = torch.rand(size=(5,1,100,90))\n",
    "simclr.forward(sample)\n",
    "summary(simclr,sample.shape[1:])\n",
    "# criterion, optimizer = setting_(model)\n",
    "# epochs = 500\n",
    "# simclr = train_unsupervised(model=simclr, \n",
    "#                            train_loader=train_loader, \n",
    "#                            criterion=criterion, \n",
    "#                            optimizer=optimizer, \n",
    "#                            epochs=epochs,\n",
    "#                            auto_save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN_baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "torch.set_deterministic(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
