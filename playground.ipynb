{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn  # for heatmaps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import loadmat\n",
    "\n",
    "# link = r\"D:\\external_data\\Experiment4\"\n",
    "# filename = r\"\\Dataset_PWR_WiFi.mat\"\n",
    "# directory = link + filename\n",
    "\n",
    "# mat = loadmat(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 files.\n",
      "input_user10_bendfwd.csv annotation_user10_bendfwd.csv user10\n",
      "input_user10_kneel.csv annotation_user10_kneel.csv user10\n",
      "input_user10_lie.csv annotation_user10_lie.csv user10\n",
      "input_user10_sit.csv annotation_user10_sit.csv user10\n",
      "input_user10_sitrotate.csv annotation_user10_sitrotate.csv user10\n",
      "input_user10_stand.csv annotation_user10_stand.csv user10\n",
      "input_user10_standrotate.csv annotation_user10_standrotate.csv user10\n",
      "input_user10_walking.csv annotation_user10_walking.csv user10\n",
      "input_user1_bendfwd.csv annotation_user1_bendfwd.csv user1\n",
      "input_user1_kneel.csv annotation_user1_kneel.csv user1\n",
      "input_user1_lie.csv annotation_user1_lie.csv user1\n",
      "input_user1_sit.csv annotation_user1_sit.csv user1\n",
      "input_user1_sitrotate.csv annotation_user1_sitrotate.csv user1\n",
      "input_user1_stand.csv annotation_user1_stand.csv user1\n",
      "input_user1_standrotate.csv annotation_user1_standrotate.csv user1\n",
      "input_user1_walking.csv annotation_user1_walking.csv user1\n",
      "input_user2_bendfwd.csv annotation_user2_bendfwd.csv user2\n",
      "input_user2_kneel.csv annotation_user2_kneel.csv user2\n",
      "input_user2_lie.csv annotation_user2_lie.csv user2\n",
      "input_user2_sit.csv annotation_user2_sit.csv user2\n",
      "input_user2_sitrotate.csv annotation_user2_sitrotate.csv user2\n",
      "input_user2_stand.csv annotation_user2_stand.csv user2\n",
      "input_user2_standrotate.csv annotation_user2_standrotate.csv user2\n",
      "input_user2_walking.csv annotation_user2_walking.csv user2\n",
      "input_user3_bendfwd.csv annotation_user3_bendfwd.csv user3\n",
      "input_user3_kneel.csv annotation_user3_kneel.csv user3\n",
      "input_user3_lie.csv annotation_user3_lie.csv user3\n",
      "input_user3_sit.csv annotation_user3_sit.csv user3\n",
      "input_user3_sitrotate.csv annotation_user3_sitrotate.csv user3\n",
      "input_user3_stand.csv annotation_user3_stand.csv user3\n",
      "input_user3_standrotate.csv annotation_user3_standrotate.csv user3\n",
      "input_user3_walking.csv annotation_user3_walking.csv user3\n",
      "input_user4_bendfwd.csv annotation_user4_bendfwd.csv user4\n",
      "input_user4_kneel.csv annotation_user4_kneel.csv user4\n",
      "input_user4_lie.csv annotation_user4_lie.csv user4\n",
      "input_user4_sit.csv annotation_user4_sit.csv user4\n",
      "input_user4_sitrotate.csv annotation_user4_sitrotate.csv user4\n",
      "input_user4_stand.csv annotation_user4_stand.csv user4\n",
      "input_user4_standrotate.csv annotation_user4_standrotate.csv user4\n",
      "input_user4_walking.csv annotation_user4_walking.csv user4\n",
      "input_user5_bendfwd.csv annotation_user5_bendfwd.csv user5\n",
      "input_user5_kneel.csv annotation_user5_kneel.csv user5\n",
      "input_user5_lie.csv annotation_user5_lie.csv user5\n",
      "input_user5_sit.csv annotation_user5_sit.csv user5\n",
      "input_user5_sitrotate.csv annotation_user5_sitrotate.csv user5\n",
      "input_user5_stand.csv annotation_user5_stand.csv user5\n",
      "input_user5_standrotate.csv annotation_user5_standrotate.csv user5\n",
      "input_user5_walking.csv annotation_user5_walking.csv user5\n",
      "input_user6_bendfwd.csv annotation_user6_bendfwd.csv user6\n",
      "input_user6_kneel.csv annotation_user6_kneel.csv user6\n",
      "input_user6_lie.csv annotation_user6_lie.csv user6\n",
      "input_user6_sit.csv annotation_user6_sit.csv user6\n",
      "input_user6_sitrotate.csv annotation_user6_sitrotate.csv user6\n",
      "input_user6_stand.csv annotation_user6_stand.csv user6\n",
      "input_user6_standrotate.csv annotation_user6_standrotate.csv user6\n",
      "input_user6_walking.csv annotation_user6_walking.csv user6\n",
      "input_user7_bendfwd.csv annotation_user7_bendfwd.csv user7\n",
      "input_user7_kneel.csv annotation_user7_kneel.csv user7\n",
      "input_user7_lie.csv annotation_user7_lie.csv user7\n",
      "input_user7_sit.csv annotation_user7_sit.csv user7\n",
      "input_user7_sitrotate.csv annotation_user7_sitrotate.csv user7\n",
      "input_user7_stand.csv annotation_user7_stand.csv user7\n",
      "input_user7_standrotate.csv annotation_user7_standrotate.csv user7\n",
      "input_user7_walking.csv annotation_user7_walking.csv user7\n",
      "input_user8_bendfwd.csv annotation_user8_bendfwd.csv user8\n",
      "input_user8_kneel.csv annotation_user8_kneel.csv user8\n",
      "input_user8_lie.csv annotation_user8_lie.csv user8\n",
      "input_user8_sit.csv annotation_user8_sit.csv user8\n",
      "input_user8_sitrotate.csv annotation_user8_sitrotate.csv user8\n",
      "input_user8_stand.csv annotation_user8_stand.csv user8\n",
      "input_user8_standrotate.csv annotation_user8_standrotate.csv user8\n",
      "input_user8_walking.csv annotation_user8_walking.csv user8\n",
      "input_user9_bendfwd.csv annotation_user9_bendfwd.csv user9\n",
      "input_user9_kneel.csv annotation_user9_kneel.csv user9\n",
      "input_user9_lie.csv annotation_user9_lie.csv user9\n",
      "input_user9_sit.csv annotation_user9_sit.csv user9\n",
      "input_user9_sitrotate.csv annotation_user9_sitrotate.csv user9\n",
      "input_user9_stand.csv annotation_user9_stand.csv user9\n",
      "input_user9_standrotate.csv annotation_user9_standrotate.csv user9\n",
      "input_user9_walking.csv annotation_user9_walking.csv user9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amp_1</th>\n",
       "      <th>amp_2</th>\n",
       "      <th>amp_3</th>\n",
       "      <th>amp_4</th>\n",
       "      <th>amp_5</th>\n",
       "      <th>amp_6</th>\n",
       "      <th>amp_7</th>\n",
       "      <th>amp_8</th>\n",
       "      <th>amp_9</th>\n",
       "      <th>amp_10</th>\n",
       "      <th>...</th>\n",
       "      <th>theta_83</th>\n",
       "      <th>theta_84</th>\n",
       "      <th>theta_85</th>\n",
       "      <th>theta_86</th>\n",
       "      <th>theta_87</th>\n",
       "      <th>theta_88</th>\n",
       "      <th>theta_89</th>\n",
       "      <th>theta_90</th>\n",
       "      <th>user</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31.675</td>\n",
       "      <td>43.080</td>\n",
       "      <td>53.718</td>\n",
       "      <td>60.338</td>\n",
       "      <td>63.286</td>\n",
       "      <td>63.721</td>\n",
       "      <td>64.516</td>\n",
       "      <td>65.070</td>\n",
       "      <td>66.817</td>\n",
       "      <td>67.594</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.519100</td>\n",
       "      <td>-2.568700</td>\n",
       "      <td>-2.577200</td>\n",
       "      <td>-2.551200</td>\n",
       "      <td>-2.551900</td>\n",
       "      <td>-2.517600</td>\n",
       "      <td>-2.498000</td>\n",
       "      <td>-2.10120</td>\n",
       "      <td>user10</td>\n",
       "      <td>noactivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.914</td>\n",
       "      <td>43.621</td>\n",
       "      <td>54.341</td>\n",
       "      <td>60.922</td>\n",
       "      <td>64.017</td>\n",
       "      <td>64.344</td>\n",
       "      <td>65.142</td>\n",
       "      <td>65.819</td>\n",
       "      <td>67.606</td>\n",
       "      <td>68.344</td>\n",
       "      <td>...</td>\n",
       "      <td>1.068600</td>\n",
       "      <td>1.048600</td>\n",
       "      <td>1.057300</td>\n",
       "      <td>1.053000</td>\n",
       "      <td>1.103500</td>\n",
       "      <td>1.075600</td>\n",
       "      <td>1.111100</td>\n",
       "      <td>1.44400</td>\n",
       "      <td>user10</td>\n",
       "      <td>noactivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32.180</td>\n",
       "      <td>44.219</td>\n",
       "      <td>55.030</td>\n",
       "      <td>61.563</td>\n",
       "      <td>64.823</td>\n",
       "      <td>65.029</td>\n",
       "      <td>65.835</td>\n",
       "      <td>66.646</td>\n",
       "      <td>68.476</td>\n",
       "      <td>69.172</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.970200</td>\n",
       "      <td>-4.005300</td>\n",
       "      <td>-4.022800</td>\n",
       "      <td>-3.961900</td>\n",
       "      <td>-3.968900</td>\n",
       "      <td>-3.926500</td>\n",
       "      <td>-3.887700</td>\n",
       "      <td>-3.54740</td>\n",
       "      <td>user10</td>\n",
       "      <td>noactivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.424</td>\n",
       "      <td>44.768</td>\n",
       "      <td>55.661</td>\n",
       "      <td>62.150</td>\n",
       "      <td>65.561</td>\n",
       "      <td>65.659</td>\n",
       "      <td>66.471</td>\n",
       "      <td>67.404</td>\n",
       "      <td>69.277</td>\n",
       "      <td>69.934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.070351</td>\n",
       "      <td>-0.001486</td>\n",
       "      <td>-0.033409</td>\n",
       "      <td>-0.012021</td>\n",
       "      <td>0.029555</td>\n",
       "      <td>0.061227</td>\n",
       "      <td>0.077287</td>\n",
       "      <td>0.40222</td>\n",
       "      <td>user10</td>\n",
       "      <td>noactivity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32.669</td>\n",
       "      <td>45.316</td>\n",
       "      <td>56.291</td>\n",
       "      <td>62.737</td>\n",
       "      <td>66.299</td>\n",
       "      <td>66.290</td>\n",
       "      <td>67.107</td>\n",
       "      <td>68.163</td>\n",
       "      <td>70.079</td>\n",
       "      <td>70.696</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.137010</td>\n",
       "      <td>-0.172940</td>\n",
       "      <td>-0.191950</td>\n",
       "      <td>-0.158630</td>\n",
       "      <td>-0.122420</td>\n",
       "      <td>-0.152340</td>\n",
       "      <td>-0.094355</td>\n",
       "      <td>0.26548</td>\n",
       "      <td>user10</td>\n",
       "      <td>noactivity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    amp_1   amp_2   amp_3   amp_4   amp_5   amp_6   amp_7   amp_8   amp_9  \\\n",
       "0  31.675  43.080  53.718  60.338  63.286  63.721  64.516  65.070  66.817   \n",
       "1  31.914  43.621  54.341  60.922  64.017  64.344  65.142  65.819  67.606   \n",
       "2  32.180  44.219  55.030  61.563  64.823  65.029  65.835  66.646  68.476   \n",
       "3  32.424  44.768  55.661  62.150  65.561  65.659  66.471  67.404  69.277   \n",
       "4  32.669  45.316  56.291  62.737  66.299  66.290  67.107  68.163  70.079   \n",
       "\n",
       "   amp_10  ...  theta_83  theta_84  theta_85  theta_86  theta_87  theta_88  \\\n",
       "0  67.594  ... -2.519100 -2.568700 -2.577200 -2.551200 -2.551900 -2.517600   \n",
       "1  68.344  ...  1.068600  1.048600  1.057300  1.053000  1.103500  1.075600   \n",
       "2  69.172  ... -3.970200 -4.005300 -4.022800 -3.961900 -3.968900 -3.926500   \n",
       "3  69.934  ...  0.070351 -0.001486 -0.033409 -0.012021  0.029555  0.061227   \n",
       "4  70.696  ... -0.137010 -0.172940 -0.191950 -0.158630 -0.122420 -0.152340   \n",
       "\n",
       "   theta_89  theta_90    user       label  \n",
       "0 -2.498000  -2.10120  user10  noactivity  \n",
       "1  1.111100   1.44400  user10  noactivity  \n",
       "2 -3.887700  -3.54740  user10  noactivity  \n",
       "3  0.077287   0.40222  user10  noactivity  \n",
       "4 -0.094355   0.26548  user10  noactivity  \n",
       "\n",
       "[5 rows x 182 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.import_data import import_experimental_data\n",
    "# from data.MyDataset import DatasetObject\n",
    "\n",
    "# import experimental dataset 1\n",
    "folderpath1 = \"D:/external_data/Experiment3/csv_files/exp_1\"  # CHANGE THIS IF THE PATH CHANGED\n",
    "df_exp1 = import_experimental_data(folderpath1) # import_clean_data('exp1',\n",
    "df_exp1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # import experimental dataset 2\n",
    "# folderpath2 = \"D:/external_data/Experiment3/csv_files/exp_2\"  # CHANGE THIS IF THE PATH CHANGED\n",
    "# df_exp2 = import_clean_data('exp2',folderpath2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_dataframes(df):\n",
    "    features_ls,labels_ls = [],[]\n",
    "    for user in df['user'].unique():\n",
    "        dataframe = df[df['user']==user]\n",
    "        features = dataframe[[f'amp_{i}' for i in range(1,91)]].to_numpy()\n",
    "#         features = MinMaxScaler().fit_transform(features)\n",
    "        features_ls.append(features)\n",
    "        label = dataframe[['label']].to_numpy()\n",
    "        labels_ls.append(label)\n",
    "    return features_ls,labels_ls\n",
    "\n",
    "X_ls, y_ls = seperate_dataframes(df_exp1)\n",
    "del df_exp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 arrays sizes ------ X:  (296, 800, 90)  Y:  (296,)  Z:  (296,)\n",
      "index 1 arrays sizes ------ X:  (305, 800, 90)  Y:  (305,)  Z:  (305,)\n",
      "index 2 arrays sizes ------ X:  (291, 800, 90)  Y:  (291,)  Z:  (291,)\n",
      "index 3 arrays sizes ------ X:  (282, 800, 90)  Y:  (282,)  Z:  (282,)\n",
      "index 4 arrays sizes ------ X:  (281, 800, 90)  Y:  (281,)  Z:  (281,)\n",
      "index 5 arrays sizes ------ X:  (289, 800, 90)  Y:  (289,)  Z:  (289,)\n",
      "index 6 arrays sizes ------ X:  (310, 800, 90)  Y:  (310,)  Z:  (310,)\n",
      "index 7 arrays sizes ------ X:  (298, 800, 90)  Y:  (298,)  Z:  (298,)\n",
      "index 8 arrays sizes ------ X:  (288, 800, 90)  Y:  (288,)  Z:  (288,)\n",
      "index 9 arrays sizes ------ X:  (307, 800, 90)  Y:  (307,)  Z:  (307,)\n",
      "size of DatasetObject ------ :  (10, 3)\n"
     ]
    }
   ],
   "source": [
    "from data.process_data import DatasetObject\n",
    "from data import process_data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "exp_1 = DatasetObject()\n",
    "exp_1.import_data(X_ls, y_ls,window_size=800,slide_size=100,skip_labels=['noactivity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index 0 arrays sizes ------ X:  (400, 800, 90, 1)  Y:  (400,)  Z:  (400,)\n",
      "index 1 arrays sizes ------ X:  (424, 800, 90, 1)  Y:  (424,)  Z:  (424,)\n",
      "index 2 arrays sizes ------ X:  (432, 800, 90, 1)  Y:  (432,)  Z:  (432,)\n",
      "index 3 arrays sizes ------ X:  (368, 800, 90, 1)  Y:  (368,)  Z:  (368,)\n",
      "index 4 arrays sizes ------ X:  (384, 800, 90, 1)  Y:  (384,)  Z:  (384,)\n",
      "index 5 arrays sizes ------ X:  (360, 800, 90, 1)  Y:  (360,)  Z:  (360,)\n",
      "index 6 arrays sizes ------ X:  (384, 800, 90, 1)  Y:  (384,)  Z:  (384,)\n",
      "index 7 arrays sizes ------ X:  (368, 800, 90, 1)  Y:  (368,)  Z:  (368,)\n",
      "index 8 arrays sizes ------ X:  (368, 800, 90, 1)  Y:  (368,)  Z:  (368,)\n",
      "index 9 arrays sizes ------ X:  (400, 800, 90, 1)  Y:  (400,)  Z:  (400,)\n",
      "size of DatasetObject ------ :  (10, 3)\n"
     ]
    }
   ],
   "source": [
    "exp_1.data_transform(lambda arr: arr.reshape(*arr.shape,1),axis=1,col=0)\n",
    "exp_1.data_transform(lambda x,y,z : process_data.resampling(x,y,z,True),axis=0,col=0)\n",
    "exp_1.__len__()\n",
    "# exp_1.label_encode(1,LabelEncoder())\n",
    "# exp_1.label_encode(2)\n",
    "# exp_1.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_ls,y_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set: [1, 2, 3, 4, 5, 6, 7, 8, 9] \ttest set: [0]\n"
     ]
    }
   ],
   "source": [
    "def transform_pipeline(X_train,y_train,X_test,y_test):\n",
    "    X_train = X_train.transpose(0,3,1,2)\n",
    "    X_test = X_test.transpose(0,3,1,2)\n",
    "    \n",
    "    lb = LabelEncoder()\n",
    "    y_train = lb.fit_transform(y_train)\n",
    "    y_test  = lb.transform(y_test)\n",
    "    return X_train,y_train,X_test,y_test\n",
    "\n",
    "(X_train,y_train,_),(X_test,y_test,_) = exp_1.__getitem__([0],return_sets=True)\n",
    "X_train,y_train,X_test,y_test = transform_pipeline(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X.shape)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traindataset = TensorDataset(Tensor(X_train),Tensor(y_train).long()) \n",
    "# testdataset = TensorDataset(Tensor(X_test), Tensor(y_test).long())\n",
    "# rand_x = torch.zeros_like(Tensor(X_train))\n",
    "# rand_y = torch.zeros_like(Tensor(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# rand_x = torch.rand((128*16, 1, 800, 90))\n",
    "# rand_y = torch.randint(0,8,(128*16,))\n",
    "# traindataset = TensorDataset(rand_x,rand_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(traindataset, batch_size=128,shuffle=True, num_workers=0)\n",
    "# test_loader = DataLoader(testdataset, batch_size=128,shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(X_train,y_train,X_test,y_test):\n",
    "    traindataset = TensorDataset(Tensor(X_train),Tensor(y_train).long()) \n",
    "    testdataset = TensorDataset(Tensor(X_test), Tensor(y_test).long())\n",
    "    train_loader = DataLoader(traindataset, batch_size=128,shuffle=True, num_workers=0)\n",
    "    test_loader = DataLoader(testdataset, batch_size=1024, shuffle=True, num_workers=0) \n",
    "    return train_loader, test_loader\n",
    "    \n",
    "train_loader, test_loader = create_dataloaders(X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.func(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        ### 1st ###\n",
    "        self.conv1 = nn.Conv2d(1,64,kernel_size=5,stride=5)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.norm1 = Lambda(lambda x:x)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,1))\n",
    "        ### 2nd ###\n",
    "        self.conv2 = nn.Conv2d(64,128,kernel_size=3,stride=3)\n",
    "        self.actv2 = nn.ReLU()\n",
    "        self.norm2 = Lambda(lambda x:x)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,1))\n",
    "        ### 3rd ###\n",
    "        self.conv3 = nn.Conv2d(128,256,kernel_size=2,stride=2)\n",
    "        self.actv3 = nn.ReLU()\n",
    "        self.norm3 = Lambda(lambda x:x)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,1))\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.pool1(self.norm1(self.actv1(self.conv1(X))))\n",
    "        X = self.pool2(self.norm2(self.actv2(self.conv2(X))))\n",
    "        X = self.pool3(self.norm3(self.actv3(self.conv3(X))))\n",
    "        X = torch.flatten(X, 1)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,input_shape):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_shape,128) \n",
    "        self.linear2 = nn.Linear(128,8)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = F.dropout(F.leaky_relu(self.linear1(X)))\n",
    "        X = self.linear2(X)\n",
    "        return F.log_softmax(X,dim=0)\n",
    "    \n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        ### 1st ###\n",
    "        self.conv1 = nn.Conv2d(1,64,kernel_size=5,stride=5)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.norm1 = Lambda(lambda x:x)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,1))\n",
    "        ### 2nd ###\n",
    "        self.conv2 = nn.Conv2d(64,128,kernel_size=3,stride=3)\n",
    "        self.actv2 = nn.ReLU()\n",
    "        self.norm2 = Lambda(lambda x:x)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,1))\n",
    "        ### 3rd ###\n",
    "        self.conv3 = nn.Conv2d(128,256,kernel_size=2,stride=2)\n",
    "        self.actv3 = nn.ReLU()\n",
    "        self.norm3 = Lambda(lambda x:x)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(2,2),stride=(2,1))\n",
    "        ### Classifier ####\n",
    "        self.linear1 = nn.Linear(768,128) \n",
    "        self.linear2 = nn.Linear(128,8)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        X = self.pool1(self.norm1(self.actv1(self.conv1(X))))\n",
    "        X = self.pool2(self.norm2(self.actv2(self.conv2(X))))\n",
    "        X = self.pool3(self.norm3(self.actv3(self.conv3(X))))\n",
    "        X = torch.flatten(X, 1)\n",
    "        X = F.dropout(F.leaky_relu(self.linear1(X)))\n",
    "        X = self.linear2(X)\n",
    "        return F.log_softmax(X,dim=0)\n",
    "    \n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = Encoder()\n",
    "# z  = encoder.forward(rand_x)  \n",
    "# z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = Classifier(z.shape[1])\n",
    "# out = classifier.forward(z)\n",
    "# out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "\n",
    "# summary(encoder,input_size=rand_x.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setting(model):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=0.001)\n",
    "    return criterion, optimizer\n",
    "\n",
    "criterion, optimizer = setting(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer_e = torch.optim.Adam(encoder.parameters(), lr=0.001)\n",
    "# optimizer_c = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
    "# optimizer   = torch.optim.Adam(list(encoder.parameters())+list(classifier.parameters()), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        \n",
    "        for b, (X_train, y_train) in enumerate(train_loader):\n",
    "            \n",
    "            model.zero_grad()\n",
    "            \n",
    "            y_pred = model(X_train)\n",
    "            loss   = criterion(y_pred, y_train)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "epochs = 100\n",
    "model  = train(model, train_loader, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "\n",
    "# epochs = 100\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "# train_correct = []\n",
    "# test_correct = []\n",
    "\n",
    "# for i in range(epochs):\n",
    "#     trn_corr = 0\n",
    "#     tst_corr = 0\n",
    "    \n",
    "#     # Run the training batches\n",
    "#     for b, (X_train, y_train) in enumerate(train_loader):\n",
    "#         b+=1\n",
    "        \n",
    "#         # Encoder\n",
    "#         encoder.zero_grad()\n",
    "#         z_train = encoder(X_train)\n",
    "#         # Classifier\n",
    "#         classifier.zero_grad()\n",
    "#         y_pred = classifier(z_train)\n",
    "#         # Calculate loss\n",
    "#         loss = criterion(y_pred, y_train)\n",
    " \n",
    "#         # Tally the number of correct predictions\n",
    "#         predicted = torch.max(y_pred.data, 1)[1]\n",
    "#         batch_corr = (predicted == y_train).sum()\n",
    "#         trn_corr += batch_corr\n",
    "        \n",
    "#         # Update parameters\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Print interim results\n",
    "#         print(f'epoch: {i:2}  batch: {b:4}  loss: {loss.item():10.8f} accuracy: {trn_corr.item()*100/(10*b):7.3f}%')\n",
    "        \n",
    "#     train_losses.append(loss)\n",
    "#     train_correct.append(trn_corr)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalaute(model, test_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for X_test, y_test in test_loader:\n",
    "            \n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val,1)[1] \n",
    "    \n",
    "    arr = confusion_matrix(y_test.view(-1), predicted.view(-1))\n",
    "    return arr\n",
    "\n",
    "arr = evalaute(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[31,  1,  9,  0,  0,  9,  0,  0],\n",
       "       [ 1, 46,  0,  0,  0,  3,  0,  0],\n",
       "       [12,  0, 19, 19,  0,  0,  0,  0],\n",
       "       [ 0,  0,  0, 25,  0, 16,  0,  9],\n",
       "       [ 0,  0,  0,  0, 32, 15,  0,  3],\n",
       "       [ 0,  0,  0,  0, 14, 19,  0, 17],\n",
       "       [ 1,  0,  0,  0,  0,  1, 48,  0],\n",
       "       [ 0, 17,  4,  0,  1,  4, 24,  0]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     for X_test, y_test in test_loader:\n",
    "#         y_val = classifier(encoder(X_test))\n",
    "#         predicted = torch.max(y_val,1)[1]\n",
    "#         correct += (predicted == y_test).sum()\n",
    "\n",
    "# arr = confusion_matrix(y_test.view(-1), predicted.view(-1))\n",
    "# arr\n",
    "# df_cm = pd.DataFrame(arr, class_names, class_names)\n",
    "# plt.figure(figsize = (9,6))\n",
    "# sn.heatmap(df_cm, annot=True, fmt=\"d\", cmap='BuGn')\n",
    "# plt.xlabel(\"prediction\")\n",
    "# plt.ylabel(\"label (ground truth)\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
