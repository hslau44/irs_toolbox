{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import glob\n",
    "import seaborn as sns  # for heatmaps\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_sepctrograms(path,columns=columns):\n",
    "    data = dict()\n",
    "    for label in os.listdir(path):\n",
    "        data[label] = dict()\n",
    "        for f in listdir(path+'/'+label):\n",
    "            user = f.split('.')[0].split('_')[1]\n",
    "            df = pd.read_csv(path+'/'+label+'/'+f, names=columns)\n",
    "            if user not in data[label]:\n",
    "                data[label][user] = []\n",
    "            data[label][user].append(df)\n",
    "        print(label)\n",
    "    return data\n",
    "\n",
    "# path = \"E:/external_data/Experiment2/spectrogram_data_by_activity_csv\"\n",
    "# data = import_sepctrograms(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lay\n",
      "picking\n",
      "sit\n",
      "walking\n",
      "waving\n",
      "stand_l\n",
      "stand_s\n"
     ]
    }
   ],
   "source": [
    "columns = [f\"col_{i+1}\" for i in range(501)]\n",
    "window_size=65\n",
    "slide_size=30\n",
    "dirc = 'E://external_data/Experiment4/Spectrogram_data_csv_files/CSI_data'\n",
    "\n",
    "X,y  = import_data(dirc,columns=columns,window_size=window_size,slide_size=slide_size) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cross_validation(item_func,X,y,n_splits):\n",
    "\n",
    "    acc_score = []\n",
    "\n",
    "    kf = KFold(n_splits)\n",
    "    \n",
    "    for train_index , test_index in kf.split(X):\n",
    "        X_train , X_test = X[train_index], X[test_index]\n",
    "        y_train , y_test = y[train_index], y[test_index]\n",
    "        train_loader, test_loader = create_dataloader(X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # create model \n",
    "        model,optimizer,criterion = item_func()\n",
    "        \n",
    "        # train model\n",
    "        model.fit(X_train,y_train)\n",
    "        pred_values = model.predict(X_test)\n",
    "        \n",
    "        # evaluate\n",
    "        acc = accuracy_score(pred_values , y_test)\n",
    "        acc_score.append(acc)\n",
    "\n",
    "    avg_acc_score = sum(acc_score)/k\n",
    "\n",
    "    print('accuracy of each fold - {}'.format(acc_score))\n",
    "    print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(p1,p2,t=1):\n",
    "    val = p1.dot(p2.transpose())/t\n",
    "    val = np.exp(val)\n",
    "    val = -np.log(np.diag(val)/np.sum(val,axis=1))\n",
    "    return val\n",
    "\n",
    "p1 = np.array([\n",
    " [ 0.70374682, -0.18682394, -0.68544673],\n",
    " [ 0.15465702,  0.32303224,  0.93366556],\n",
    " [ 0.53043332, -0.83523217, -0.14500935],\n",
    " [ 0.68285685, -0.73054075,  0.00409143],\n",
    " [ 0.76652431,  0.61500886,  0.18494479]])\n",
    "\n",
    "p2 = p1 + 0.1*np.random.random(p1.shape)\n",
    "\n",
    "# contrastive_loss(p1,p2,0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def contrastive_loss(z1,z2,t=1):\n",
    "    \"\"\"\n",
    "    One to N\n",
    "    \"\"\"\n",
    "    a = torch.mm(z1,z2.transpose(0,1))\n",
    "    b = torch.norm(z1,dim=1)*torch.norm(z2,dim=1)\n",
    "    sim_mat = torch.div(a,b)\n",
    "    mask = torch.ones_like(sim_mat,dtype=bool).fill_diagonal_(0)\n",
    "    pos = torch.diag(sim_mat).reshape(-1,1)\n",
    "    neg = sim_mat[mask].reshape(pos.shape[0],-1)\n",
    "    logits = torch.cat((pos, neg), dim=1)\n",
    "    labels = torch.zeros(pos.shape[0]).long()\n",
    "    loss = torch.nn.CrossEntropyLoss()(logits,labels)\n",
    "    return loss\n",
    "\n",
    "z1 = torch.tensor(p1)\n",
    "z2 = torch.tensor(p2)\n",
    "z3 = torch.zeros_like(z1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(d):\n",
    "    d = np.array(d)\n",
    "    ls = d[0]\n",
    "    for new in d[1:]:\n",
    "        ls = np.intersect1d(ls,new).tolist()\n",
    "    return ls\n",
    "\n",
    "\n",
    "def import_CsiPwr_data(directory):\n",
    "    \"\"\"\n",
    "    import all spectrogram (in pair) in the directory\n",
    "    \"\"\"\n",
    "    print(\"Importing Data \",end='')\n",
    "    data = {'X':{},'y':{}}\n",
    "    for label in os.listdir(directory):\n",
    "        pfiles = []\n",
    "        for modality in os.listdir(directory+'/'+label):\n",
    "            pfiles.append([f.split('.')[0] for f in os.listdir(directory+'/'+label+'/'+modality)])\n",
    "\n",
    "        print('>',end='')\n",
    "        common_files = intersect(pfiles)\n",
    "        \n",
    "        \n",
    "        for modality in os.listdir(directory+'/'+label):\n",
    "            files = [directory+'/'+label+'/'+modality+'/'+pfilename+'.csv' for pfilename in common_files]\n",
    "            X = import_spectrograms(files)\n",
    "            y = np.full(X.shape[0], label)\n",
    "            X[modality]\n",
    "        \n",
    "        # selcting available pairs\n",
    "        pfiles_csi = [f.split('.')[0] for f in os.listdir(directory+'/'+label+'/'+'csi')]\n",
    "        pfiles_pwr = [f.split('.')[0] for f in os.listdir(directory+'/'+label+'/'+'pwr')]\n",
    "        available_pairs = np.intersect1d(pfiles_csi,pfiles_pwr).tolist()\n",
    "        files_csi = [directory+'/'+label+'/'+'csi'+'/'+pfilename+'.csv' for pfilename in available_pairs]\n",
    "        files_pwr = [directory+'/'+label+'/'+'pwr'+'/'+pfilename+'.csv' for pfilename in available_pairs]\n",
    "        # importing\n",
    "        X1 = import_spectrograms(files_csi)\n",
    "        X2 = import_spectrograms(files_pwr)\n",
    "        y = np.full(X1.shape[0], label)\n",
    "        assert X1.shape[0] == X2.shape[0]\n",
    "        data['X1'].append(X1)\n",
    "        data['X2'].append(X2)\n",
    "        data['y'].append(y)\n",
    "    print(\" Complete\")\n",
    "    return np.concatenate(data['X1']), np.concatenate(data['X2']), np.concatenate(data['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUC pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def selections(*arg,**kwarg):\n",
    "    size = int(arg[0].shape[0]*kwarg['p'])\n",
    "    index = np.arange(0,arg[0].shape[0])\n",
    "    test_selection = np.random.choice(index,size,replace=False)\n",
    "    train_selection = np.array([i for i in index if i not in test_selection])\n",
    "    return [i[train_selection] for i in arg],[i[test_selection] for i in arg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloader_pairdata(dirc,mode,p=None,resample=None):\n",
    "    \"\"\"\n",
    "    Import pair data\n",
    "    \"\"\"\n",
    "    if mode == 1:\n",
    "        X1,X2,y = import_pair_data(dirc)\n",
    "    elif mode == 2:\n",
    "        X1,X2,y = import_CsiPwr_data(dirc)\n",
    "    else:\n",
    "        raise ValueError('Must be 1 or 2 for pair')\n",
    "    \n",
    "    X1 = X1.reshape(*X1.shape,1).transpose(0,3,1,2)\n",
    "    X2 = X2.reshape(*X2.shape,1).transpose(0,3,1,2)\n",
    "    y,lb = label_encode(y)\n",
    "    \n",
    "    ### train_test data\n",
    "    training,validation = selections(X1,X2,y,p=0.2)\n",
    "    X_test, y_test = validation[0],validation[2]\n",
    "    \n",
    "    ### train_test data\n",
    "    if p:\n",
    "        (X_train,y_train) , _ = selections(training[0],training[2],p=p)\n",
    "    else:\n",
    "        X_train,y_train = training[0],training[2]\n",
    "    \n",
    "    if resample:\n",
    "        X_train,y_train,_ = resampling(X_train,y_train,y_train,oversampling=True)\n",
    "        # X_test, y_test,_ = resampling(X_test, y_test,y_test,oversampling=False)\n",
    "    \n",
    "    ### Dataloader\n",
    "    print('X1: ',training[0].shape,' X2: ',training[1].shape)\n",
    "    print('X_train: ',X_train.shape,' y_train: ',y_train.shape,' X_test: ',X_test.shape,' y_test: ',y_test.shape)\n",
    "    pretraindataset = TensorDataset(Tensor(training[0]),Tensor(training[1]))\n",
    "    finetunedataset = TensorDataset(Tensor(X_train),Tensor(y_train).long())\n",
    "    validatndataset = TensorDataset(Tensor(X_test), Tensor(y_test).long())\n",
    "    pretrain_loader = DataLoader(pretraindataset, batch_size=bsz, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "    finetune_loader = DataLoader(finetunedataset, batch_size=bsz, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "    validatn_loader = DataLoader(validatndataset, batch_size=2000, shuffle=True, num_workers=num_workers)\n",
    "    return pretrain_loader, finetune_loader, validatn_loader, lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Data >>>>>> Complete\n",
      "X1:  (576, 1, 65, 501)  X2:  (576, 1, 65, 501)\n",
      "X_train:  (576, 1, 65, 501)  y_train:  (576,)  X_test:  (143, 1, 65, 501)  y_test:  (143,)\n"
     ]
    }
   ],
   "source": [
    "path = 'E:/external_data/Experiment4/Spectrogram_data_csv_files/CSI_data_pair'\n",
    "pretrain_loader, finetune_loader, validatn_loader, lb = prepare_dataloader_pairdata(path,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.baseline import LSTM\n",
    "from models.cnn import create_vgg16,create_vgg16_atn\n",
    "from models import Lambda\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,feature_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.linear = nn.Linear(feature_size,1) \n",
    "\n",
    "    def forward(self,X):\n",
    "        assert len(X.shape) == 3\n",
    "        a = self.linear(X)\n",
    "        a = torch.relu(a)\n",
    "        a = F.softmax(a,dim=1)\n",
    "        return a*X\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,seq_size,feature_size,out_feature_size,attention=False):\n",
    "        \"\"\"\n",
    "        2 layer LSTM model: feature_size --> 200 --> 3\n",
    "\n",
    "        attr:\n",
    "        seq_size: length of the sequence\n",
    "        feature_size: feature size of each interval in the sequence\n",
    "        \"\"\"\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(feature_size,200)\n",
    "        self.lstm2 = nn.LSTM(200,out_feature_size)\n",
    "        if attention == True:\n",
    "            self.attention = Attention(out_feature_size)\n",
    "        else: \n",
    "            self.attention = Lambda(lambda x:x)\n",
    "\n",
    "\n",
    "    def forward(self,X):\n",
    "        X, _ = self.lstm1(X)\n",
    "        X, _ = self.lstm2(X)\n",
    "        X = self.attention(X)\n",
    "        # X = torch.flatten(X,1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 33    \n",
    "feature = 45\n",
    "out = 7\n",
    "mdl = LSTM(seq,feature,out,True)\n",
    "attention = Attention(9)\n",
    "sample = torch.rand(5,seq,feature)\n",
    "attention = Attention(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_shape = (3,1)\n",
    "mdl = create_vgg16(out_shape)\n",
    "sample = torch.rand(3,1,100,41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "             Stack-1           [64, 3, 100, 41]               0\n",
      "            Conv2d-2          [64, 64, 100, 41]           1,792\n",
      "              ReLU-3          [64, 64, 100, 41]               0\n",
      "            Conv2d-4          [64, 64, 100, 41]          36,928\n",
      "              ReLU-5          [64, 64, 100, 41]               0\n",
      "         MaxPool2d-6           [64, 64, 50, 20]               0\n",
      "            Conv2d-7          [64, 128, 50, 20]          73,856\n",
      "              ReLU-8          [64, 128, 50, 20]               0\n",
      "            Conv2d-9          [64, 128, 50, 20]         147,584\n",
      "             ReLU-10          [64, 128, 50, 20]               0\n",
      "        MaxPool2d-11          [64, 128, 25, 10]               0\n",
      "           Conv2d-12          [64, 256, 25, 10]         295,168\n",
      "             ReLU-13          [64, 256, 25, 10]               0\n",
      "           Conv2d-14          [64, 256, 25, 10]         590,080\n",
      "             ReLU-15          [64, 256, 25, 10]               0\n",
      "           Conv2d-16          [64, 256, 25, 10]         590,080\n",
      "             ReLU-17          [64, 256, 25, 10]               0\n",
      "        MaxPool2d-18           [64, 256, 12, 5]               0\n",
      "           Conv2d-19           [64, 512, 12, 5]       1,180,160\n",
      "             ReLU-20           [64, 512, 12, 5]               0\n",
      "           Conv2d-21           [64, 512, 12, 5]       2,359,808\n",
      "             ReLU-22           [64, 512, 12, 5]               0\n",
      "           Conv2d-23           [64, 512, 12, 5]       2,359,808\n",
      "             ReLU-24           [64, 512, 12, 5]               0\n",
      "        MaxPool2d-25            [64, 512, 6, 2]               0\n",
      "           Conv2d-26            [64, 512, 6, 2]       2,359,808\n",
      "             ReLU-27            [64, 512, 6, 2]               0\n",
      "           Conv2d-28            [64, 512, 6, 2]       2,359,808\n",
      "             ReLU-29            [64, 512, 6, 2]               0\n",
      "           Conv2d-30            [64, 512, 6, 2]       2,359,808\n",
      "             ReLU-31            [64, 512, 6, 2]               0\n",
      "        MaxPool2d-32            [64, 512, 3, 1]               0\n",
      "AdaptiveAvgPool2d-33            [64, 512, 3, 1]               0\n",
      "          Flatten-34                 [64, 1536]               0\n",
      "================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.00\n",
      "Forward/backward pass size (MB): 1123.63\n",
      "Params size (MB): 56.13\n",
      "Estimated Total Size (MB): 1180.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(mdl,(1,100,41),64,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
