{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x220692ca2f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random seed\n",
    "np.random.seed(1024)\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setting\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "parallel = True\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data setting\n",
    "columns = [f\"col_{i+1}\" for i in range(501)] # 65*501\n",
    "window_size=501\n",
    "slide_size=501\n",
    "\n",
    "\n",
    "# Data main folder\n",
    "dirc = \"E:/external_data/Experiment4/Spectrogram_data_csv_files/CSI_data\"\n",
    "\n",
    "# Saving path\n",
    "PATH = 'C://Users/Creator/Script/Python/Project/irs_toolbox' # '.'\n",
    "\n",
    "# Training setting\n",
    "bsz = 64\n",
    "pre_train_epochs = 500\n",
    "fine_tune_epochs = 300\n",
    "\n",
    "exp_name = 'Encoder_64-128-512-64-7_pretrained_on_exp4csi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import Data\n"
     ]
    }
   ],
   "source": [
    "from data.spectrogram import import_data\n",
    "from data.process_data import label_encode, create_dataloaders\n",
    "\n",
    "def prepare_dataloader():\n",
    "    X,y  = import_data(dirc,columns=columns,window_size=window_size,slide_size=slide_size)\n",
    "    X = X.reshape(*X.shape,1).transpose(0,3,1,2)\n",
    "    y,lb = label_encode(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)\n",
    "    train_loader, test_loader = create_dataloaders(X_train, y_train, X_test, y_test, train_batch_sizes=bsz, test_batch_sizes=200, num_workers=num_workers)\n",
    "    return train_loader, test_loader,lb\n",
    "\n",
    "\n",
    "train_loader, test_loader, lb = prepare_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Lambda\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder for spectrogram (1,65,65), 3 layer\n",
    "    \"\"\"\n",
    "    def __init__(self,num_filters):\n",
    "        super(Encoder, self).__init__()\n",
    "        l1,l2,l3,l4 = num_filters\n",
    "        ### 1st ###\n",
    "        self.conv1 = nn.Conv2d(1,l1,kernel_size=(5,5),stride=(2,2))\n",
    "        self.norm1 = nn.BatchNorm2d(l1)\n",
    "        self.actv1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d((1,2))\n",
    "        ### 2nd ###\n",
    "        self.conv2 = nn.Conv2d(l1,l2,kernel_size=(4,4),stride=(2,2))\n",
    "        self.norm2 = nn.BatchNorm2d(l2)\n",
    "        self.actv2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d((1,2))\n",
    "        ### 3rd ###\n",
    "        self.conv3 = nn.Conv2d(l2,l3,kernel_size=(3,3),stride=(2,2))\n",
    "        self.norm3 = nn.BatchNorm2d(l3)\n",
    "        self.actv3 = nn.ReLU()\n",
    "        self.pool3 = nn.AdaptiveAvgPool2d((2,2))# nn.MaxPool2d((1,2)) \n",
    "        ### 4th ###\n",
    "#         self.conv4 = nn.Conv2d(l3,l4,kernel_size=(2,2),stride=(2,2))\n",
    "#         self.norm4 = Lambda(lambda x:x)\n",
    "#         self.actv4 = nn.Tanh()\n",
    "#         self.pool4 = nn.AdaptiveAvgPool2d((1,2))\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = self.pool1(self.actv1(self.norm1(self.conv1(X))))\n",
    "        X = self.pool2(self.actv2(self.norm2(self.conv2(X))))\n",
    "        X = self.pool3(self.actv3(self.norm3(self.conv3(X))))\n",
    "#         X = self.pool4(self.actv4(self.norm4(self.conv4(X))))\n",
    "        X = torch.flatten(X, 1)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "# summary(Encoder([64,128,256,512]),(1,65,501),batch_size=bsz,device='cpu')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Classifier (Double Layer)\n",
    "\n",
    "    Hint: user nn.Linear as Classifier (Single Layer)\n",
    "    \"\"\"\n",
    "    def __init__(self,input_shape,hidden_shape,output_shape,norm=False):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_shape,hidden_shape)\n",
    "        self.linear2 = nn.Linear(hidden_shape,output_shape)\n",
    "        if norm == True:\n",
    "            self.norm = Lambda(lambda x: F.normalize(X,dim=1))\n",
    "        else:\n",
    "            self.norm = Lambda(lambda x:x)\n",
    "\n",
    "    def forward(self,X):\n",
    "        X = F.leaky_relu(self.linear1(X),inplace=True)\n",
    "        X = F.dropout(X,0.2)\n",
    "        X = self.linear2(X)\n",
    "        X = self.norm(X)\n",
    "        return X\n",
    "\n",
    "# summary(Classifier(1024,128,7),(1024,),batch_size=bsz,device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ED_module\n",
    "\n",
    "def create_model(enc=None):\n",
    "    enc = Encoder([64,128,256,512])\n",
    "    clf = Classifier(1024,128,7)\n",
    "    model = ED_module(encoder=enc,decoder=clf)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from losses import SupConLoss\n",
    "\n",
    "\n",
    "# criterion = SupConLoss(temperature=0.1,base_temperature=0.1) # Contrastive loss\n",
    "criterion = nn.CrossEntropyLoss() # Cross entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(list(model.parameters()), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import train\n",
    "\n",
    "model, record = train(model,train_loader,criterion,optimizer,fine_tune_epochs,1,test_loader,parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import make_directory\n",
    "\n",
    "def record_log(mode,epochs,record,cmtx=None,cls=None):\n",
    "    if mode == 'pretrain':\n",
    "        path = make_directory(exp_name+'_pretrain',epoch=epochs,filepath=PATH+'/record/')\n",
    "        pd.DataFrame(record['train'],columns=['train_loss']).to_csv(path+'_loss.csv')\n",
    "    elif mode == 'finetuning':\n",
    "        path = make_directory(exp_name+'_finetuning',epoch=epochs,filepath=PATH+'/record/')\n",
    "        pd.DataFrame(record['train'],columns=['train_loss']).to_csv(path+'_loss.csv')\n",
    "        pd.DataFrame(record['validation'],columns=['validation_accuracy']).to_csv(path+'_accuracy.csv')\n",
    "        cls.to_csv(path+'_report.csv')\n",
    "        cmtx.to_csv(path+'_cmtx.csv')\n",
    "    return\n",
    "\n",
    "record_log('pretrain',pre_train_epochs,record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import evaluation\n",
    "\n",
    "cmtx,cls = evaluation(model,test_loader,label_encoder=lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_log(mode,epochs,record,cmtx=None,cls=None):\n",
    "    if mode == 'pretrain':\n",
    "        path = make_directory(exp_name+'_pretrain',epoch=epochs,filepath=PATH+'/record/')\n",
    "        pd.DataFrame(record['train'],columns=['train_loss']).to_csv(path+'_loss.csv')\n",
    "    elif mode == 'finetuning':\n",
    "        path = make_directory(exp_name+'_finetuning',epoch=epochs,filepath=PATH+'/record/')\n",
    "        pd.DataFrame(record['train'],columns=['train_loss']).to_csv(path+'_loss.csv')\n",
    "        pd.DataFrame(record['validation'],columns=['validation_accuracy']).to_csv(path+'_accuracy.csv')\n",
    "        cls.to_csv(path+'_report.csv')\n",
    "        cmtx.to_csv(path+'_cmtx.csv')\n",
    "    return\n",
    "\n",
    "record_log('finetuning',fine_tune_epochs,record,cmtx,cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save('finetuning',model,optimizer,fine_tune_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
